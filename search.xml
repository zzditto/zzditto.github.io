<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[mysql的数据备份恢复记录]]></title>
    <url>%2F2019%2F10%2F16%2FToWork%2Fmysql%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8D%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[描述数据使用Percona Xtrabackup进行备份。原mysql版本为5.6目标：拷贝数据，解压解包，使其能够在另一个台机器的mysql上恢复数据 解压解包数据原始数据123[root@localhost jcf1011]# ll总用量 291188028-rw-r--r-- 1 root root 298173185478 9月 6 16:40 01_gongshang_hins9129521_data_20190906122155_qp.xb 安装Percona XtraBackup注：根据阿里云文档说明。mysql5.6对应Percona XtraBackup2.3mysql5.7对应Percona XtraBackup2.4 但此次操作，mysql最终版本为5.6，Percona XtraBackup为2.4。仍然完成 Download Percona XtraBackup 2.4 选择对应的版本，进入在线安装命令 12345# 安装Percona存储库yum install https://repo.percona.com/yum/percona-release-latest.noarch.rpm# 安装软件包yum install percona-xtrabackup-24 解包1234xbstream -x -C /data/ &lt; ./test.xb/data 是解包文件目录 ./test.xb是xb格式备份文件# e.g.xbstream -x -C /d-2t/jcf1011/mysqldata/ &lt; 01_gongshang_hins9129521_data_20190906122155_qp.xb 解压1234#--remove-original解压完删除源文件innobackupex --decompress --remove-original /home/mysql/data#e.g.innobackupex --decompress --remove-original /d-2t/jcf1011/mysqldata 修改配置文件防止版本问题出错，修改解压文件下的配置文件，将部分注释掉 123456789101112131415161718192021[root@localhost mysqldata]# cat backup-my.cnf# This MySQL options file was generated by innobackupex.# The MySQL server[mysqld]innodb_checksum_algorithm=innodb#innodb_log_checksum_algorithm=innodbinnodb_data_file_path=ibdata1:200M:autoextendinnodb_log_files_in_group=2innodb_log_file_size=1572864000#innodb_fast_checksum=falseinnodb_page_size=16384#innodb_log_block_size=512innodb_undo_directory=.innodb_undo_tablespaces=0#rds_encrypt_data=false#innodb_encrypt_algorithm=aes_128_ecbskip-grant-tables 1234567891011121314## 阿里云说明#innodb_log_checksum_algorithm#innodb_fast_checksum#innodb_log_block_size#innodb_doublewrite_file#rds_encrypt_data#innodb_encrypt_algorithm#redo_log_version#master_key_id## 如果本地试用的是MylSAM引擎，和阿里云的innoDB不兼容，需要注释掉如下参数并增加skip-grant-tables参数：#innodb_log_checksum_algorithm=strict_crc32#redo_log_version=1skip-grant-tables 恢复解压好的备份文件12345678910111213141516171819202122232425262728293031323334353637383940innobackupex --defaults-file=/home/mysql/data/backup-my.cnf --apply-log /home/mysql/data#e.g.innobackupex --defaults-file=/d-2t/jcf1011/mysqldata/backup-my.cnf --apply-log /d-2t/jcf1011/mysqldata#mysql数据[root@localhost mysqldata]# pwd/d-2t/jcf1011/mysqldata[root@localhost mysqldata]# ll总用量 109431924-rw-rw---- 1 mysql mysql 56 10月 16 10:58 auto.cnf-rw-r--r-- 1 mysql mysql 474 10月 14 15:09 backup-my.cnf-rw-r--r-- 1 mysql mysql 7591690240 10月 16 11:57 ibdata1-rw-rw---- 1 mysql mysql 12582912 10月 16 11:52 ibdata1_bak2-rw-r----- 1 mysql mysql 12582912 10月 16 11:57 ibdata1_bak3-rw-rw---- 1 mysql mysql 50331648 10月 16 11:57 ib_logfile0-rw-rw---- 1 mysql mysql 50331648 10月 16 11:52 ib_logfile0_bak2-rw-r----- 1 mysql mysql 50331648 10月 16 11:57 ib_logfile0_bak3-rw-rw---- 1 mysql mysql 50331648 10月 16 11:57 ib_logfile1-rw-rw---- 1 mysql mysql 50331648 10月 16 10:58 ib_logfile1_bak2-rw-r----- 1 mysql mysql 50331648 10月 16 11:52 ib_logfile1_bak3-rw-r----- 1 root root 12582912 10月 15 19:31 ibtmp1-rw-r--r-- 1 mysql mysql 1435392 10月 14 13:46 log000000000002.tokulog29drwxr-x--- 2 mysql mysql 4096 10月 14 14:36 mysql-rw-r--r-- 1 root root 6140 11月 12 2015 mysql-community-release-el7-5.noarch.rpmdrwxr-x--- 2 mysql mysql 4096 10月 14 14:36 performance_schemadrwxr-x--- 2 mysql mysql 4096 10月 14 14:36 prism1-rw-r----- 1 mysql mysql 57242 10月 14 12:02 rds_table_info_json_123631.logdrwxr-x--- 2 mysql mysql 4096 10月 16 11:23 testdrwx------ 2 mysql mysql 4096 10月 16 11:48 test1-rw-r--r-- 1 mysql mysql 16384 10月 14 13:46 tokudb.directory-rw-r--r-- 1 mysql mysql 16384 10月 14 13:46 tokudb.environment-rw-r--r-- 1 mysql mysql 16384 10月 14 13:46 tokudb.rollback-rw-r--r-- 1 mysql mysql 74 10月 14 13:46 xtrabackup_binlog_info-rw-r--r-- 1 root root 27 10月 15 19:31 xtrabackup_binlog_pos_innodb-rw-r----- 1 mysql mysql 141 10月 15 19:31 xtrabackup_checkpoints-rw-r--r-- 1 mysql mysql 823 10月 14 13:46 xtrabackup_info-rw-r--r-- 1 mysql mysql 104125235200 10月 15 19:31 xtrabackup_logfile-rw-r--r-- 1 root root 1 10月 15 19:31 xtrabackup_master_key_id-rw-r--r-- 1 mysql mysql 81 10月 14 13:51 xtrabackup_slave_filename_info-rw-r--r-- 1 mysql mysql 114 10月 14 13:46 xtrabackup_slave_info 此时，mysql备份文件已经全部恢复。只需要安装好同版本【不同版本也可】的mysql，修改/my.cnf文件的datadir，重启mysql即可 问题记录修改datadir，无法启动mysql 首先安装的mysql版本是5.7，在修改datadir之后，mysql无法正常启动 修改回datadir，新建数据库和表，观察其目录结构如下。复制备份数据同样的文件，在数据库中出现表，但是表无法打开 1234567[root@localhost test1]# pwd/d-2t/jcf1011/mysqldata/test1[root@localhost test1]# ll总用量 112-rw-r----- 1 mysql mysql 61 10月 16 11:48 db.opt-rw-r----- 1 mysql mysql 8620 10月 16 11:48 test1.frm-rw-r----- 1 mysql mysql 98304 10月 16 11:48 test1.ibd 更换mysql5.7为mysql5.6版本使用在线安装 123[root@master ~]# wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm[root@master ~]# rpm -ivh mysql-community-release-el7-5.noarch.rpm[root@master ~]# yum install mysql-community-serve 【一般是没有此软件的，可以yum list | grep mysql 然后，安装mysql-community-serve........】 可通过修改/etc/yum.repo.d/中有关mysql的repo。将其中的5.6版本的enabled设置为1，其他版本的设置为0，yum中就会只显示5.6版本的mysql。此mysql在线安装可以通过下载官网最新文件，获取到最新版本以及历史版本 安装好5.6之后，仍旧无法启动，使用systemctl status mysqld命令查看 123456789101112131415[root@localhost mysqldata]# systemctl status mysql● mysqld.service - MySQL Community Server Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled) Active: active (running) since 三 2019-10-16 10:48:04 CST; 2min 19s ago Process: 18008 ExecStartPost=/usr/bin/mysql-systemd-start post (code=exited, status=0/SUCCESS) Process: 17993 ExecStartPre=/usr/bin/mysql-systemd-start pre (code=exited, status=0/SUCCESS) Main PID: 18007 (mysqld_safe) CGroup: /system.slice/mysqld.service ├─18007 /bin/sh /usr/bin/mysqld_safe --basedir=/usr └─18223 /usr/sbin/mysqld --basedir=/usr --datadir=/d-2t/mysql/data --plugin-dir=/usr/lib64/mysql/plugin --log-error=/var/log/mysqld.log --pid-file=/var...10月 16 10:48:03 localhost.localdomain systemd[1]: Starting MySQL Community Server...10月 16 10:48:03 localhost.localdomain mysqld_safe[18007]: 191016 10:48:03 mysqld_safe Logging to '/var/log/mysqld.log'.10月 16 10:48:03 localhost.localdomain mysqld_safe[18007]: 191016 10:48:03 mysqld_safe Starting mysqld daemon with databases from /d-2t/mysql/data10月 16 10:48:04 localhost.localdomain systemd[1]: Started MySQL Community Server. 从中发现，加载的目录为/d-2t/mysql/data,但是检查/etc/my.cnf，其中设置并没有此目录。说明mysql的启动读取了其他的配置参数，或者在其他地方设置了参数 使用查找命令： 123456789101112131415161718192021# 检查每个目录，发现没有设置[root@localhost data]# find / -name mysql/etc/logrotate.d/mysql/var/lib/mysql/var/lib/mysql/mysql/usr/bin/mysql/usr/lib/mysql/usr/lib64/perl5/vendor_perl/auto/DBD/mysql/usr/lib64/perl5/vendor_perl/DBD/mysql/usr/lib64/mysql/usr/share/mysql/home/mysql/d-2t/jcf1011/mysqldata/mysql/d-2t/mysql# 继续查找[root@localhost mysql]# find / -name my.ini[root@localhost mysql]# find / -name my.cnf/etc/my.cnf/usr/my.cnf#终于发现了`/usr/my.cnf`。并且确定，mysql优先读取`/usr/my.cnf`文件。删除此文件，重启mysql。但是mysql还是无法正常启动 /usr/my.cnf文件 1234567891011121314151617181920212223242526272829303132[root@localhost test1]# cat /usr/my.cnf_bak# For advice on how to change settings please see# http://dev.mysql.com/doc/refman/5.6/en/server-configuration-defaults.html[mysqld]# Remove leading # and set to the amount of RAM for the most important data# cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.# innodb_buffer_pool_size = 128M# Remove leading # to turn on a very important data integrity option: logging# changes to the binary log between backups.# log_bin# These are commonly set, remove the # and set as required.# basedir = ..... datadir = /d-2t/mysql/data# datadir = /d-2t/mysql2/mysql# port = .....# server_id = .....# socket = .....wait_timeout=1814400# Remove leading # to set options mainly useful for reporting servers.# The server defaults are faster for transactions and fast SELECTs.# Adjust sizes as needed, experiment to find the optimal values.# join_buffer_size = 128M# sort_buffer_size = 2M# read_rnd_buffer_size = 2Msql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLESskip-grant-tables 还是无法启动，查看状态 1234562019-10-16 10:56:48 18892 [Note] InnoDB: Completed initialization of buffer pool2019-10-16 10:56:48 18892 [ERROR] InnoDB: ./ib_logfile0 can't be opened in read-write mode2019-10-16 10:56:48 18892 [ERROR] Plugin 'InnoDB' init function returned error.2019-10-16 10:56:48 18892 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.2019-10-16 10:56:48 18892 [ERROR] Unknown/unsupported storage engine: InnoDB2019-10-16 10:56:48 18892 [ERROR] Aborting 从中可以看出，ib_logfile0无法打开，查看实际文件，发现是user:group设置为了root，将其改为mysql。正常启动。正常启动了mysql5.6，修改datadir，重启mysql，打开mysq，发现数据备份完成。 临时密码无法登陆的问题修改：/etc/my.cnf添加：skip-grant-tables 重启mysql登陆：mysql -u root #此时不需要密码，可以直接登录修改密码：update mysql.user set password=password(&#39;root&#39;) where user=&#39;root&#39;;增加远程登录：grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;123456&#39;;刷新：FLUSH PRIVILEGES;推出，修改my.cnf，去掉添加的内容，再重启mysql即可 参考阿里云 RDS for MySQL 物理备份文件恢复到自建数据库mysql 备份文件.xb 数据库备份还原我的MySQL数据库里面明明存在一个表，可是查看表的时候又显示不存在是怎么回事安装mysql后无法找到临时密码的解决方案]]></content>
      <categories>
        <category>ToWork</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ambari和ES，kibana集成-案例]]></title>
    <url>%2F2019%2F10%2F11%2FLearn%2FAmbari%E5%92%8CES%EF%BC%8Ckibana%E9%9B%86%E6%88%90-%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[简单介绍实现目的虽然上述两个生态应用软件在数据处理上都颇为重要，但是毕竟不是一家人。可是在打造工业产品的过程中，hadoop和elasticsearch经常是缺一不可的（一个用于高吞吐高延迟场景，一个用于低延迟搜索场景）。因此将二者合二为一的需求也是比较明显的。Ambari平台集成的是Apache自家的solr组件，但在实际应用中，ES对于大数据量的数据搜索来说，速度更加稳定。为了方便统一，以及大数据产品的易用操作性。将二者集成是有必要的。 集成实现过程详解ambari安装服务过程 Ambari Server 会读取 Stack 和 Service 的配置文件。当用 Ambari 创建服务的时候，Ambari Server 传送 Stack 和 Service 的配置文件以及 Service 生命周期的控制脚本到 Ambari Agent。Agent 拿到配置文件后，会下载安装公共源里软件包（Redhat，就是使用 yum 服务）。安装完成后，Ambari Server 会通知 Agent 去启动 Service。之后 Ambari Server 会定期发送命令到 Agent 检查 Service 的状态，Agent 上报给 Server，并呈现在 Ambari 的 GUI 上。 所以Ambari是支持服务扩展的，只需要，将我们的服务整理打包，在stack，service中设置好相应的配合文件。重启Ambari-server，就可以在ambari的增加服务页面找到我们的服务并且安装。 目录结构解析123456789101112131415|_ stacks |_ &lt;stack_name&gt; |_ &lt;stack_version&gt; metainfo.xml |_ hooks |_ repos repoinfo.xml #软件仓库的URL |_ services #服务目录 |_ &lt;service_name&gt; metainfo.xml metrics.json |_ configuration &#123;configuration files&#125; |_ package &#123;files, scripts, templates&#125; HDFS目录结构 1234567891011121314[root@vm01 HDFS]# pwd/var/lib/ambari-server/resources/stacks/HDP/2.6/services/HDFS[root@vm01 HDFS]# tree.├── configuration│ ├── core-site.xml│ ├── hadoop-env.xml│ ├── hadoop-metrics2.properties.xml│ ├── ranger-hdfs-audit.xml│ └── ranger-hdfs-plugin-properties.xml├── kerberos.json└── metainfo.xml1 directory, 7 files 安装好的ES的目录结构 1234567891011121314151617181920212223242526272829303132333435[root@vm01 ELASTICSEARCH-6.4.x]# pwd/var/lib/ambari-server/resources/stacks/HDP/2.6/services/ELASTICSEARCH-6.4.x[root@vm01 ELASTICSEARCH-6.4.x]# tree.├── alerts.json├── configuration│ ├── elasticsearch-config.xml│ ├── elasticsearch-env.xml│ ├── elasticsearch-log4j.xml│ └── original│ ├── elasticsearch.yml│ ├── jvm.options│ ├── log4j2.properties│ ├── role_mapping.yml│ ├── roles.yml│ ├── users│ └── users_roles├── metainfo.xml├── package│ ├── archive.zip│ ├── scripts│ │ ├── es_master.py│ │ ├── es_slave.py│ │ ├── __init__.py│ │ ├── params.py│ │ ├── service_check.py│ │ └── status_params.py│ └── templates│ ├── elasticsearch.jvm.options.j2│ ├── elasticsearch.master.yml.j2│ └── elasticsearch.slave.yml.j2└── quicklinks └── quicklinks.json6 directories, 23 files metainfo.xml - 服务整体描述首先，通过metainfo.xml文件对elasticsearch这个服务进行一个标准的描述。完整文件如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&lt;?xml version="1.0"?&gt;&lt;metainfo&gt; &lt;schemaVersion&gt;2.0&lt;/schemaVersion&gt; &lt;services&gt; &lt;service&gt; &lt;name&gt;ELASTICSEARCH&lt;/name&gt; &lt;displayName&gt;Elasticsearch&lt;/displayName&gt; &lt;comment&gt;A highly scalable open-source full-text search and analytics engine. Including storage, searching, and analyzing big volumes of data quickly and in near real time. &lt;/comment&gt; &lt;version&gt;6.4.2&lt;/version&gt; &lt;components&gt; &lt;component&gt; &lt;name&gt;ELASTICSEARCH_MASTER&lt;/name&gt; &lt;displayName&gt;Elasticsearch Master&lt;/displayName&gt; &lt;category&gt;MASTER&lt;/category&gt; &lt;cardinality&gt;1+&lt;/cardinality&gt; &lt;versionAdvertised&gt;true&lt;/versionAdvertised&gt; &lt;commandScript&gt; &lt;script&gt;scripts/es_master.py&lt;/script&gt; &lt;scriptType&gt;PYTHON&lt;/scriptType&gt; &lt;timeout&gt;1200&lt;/timeout&gt; &lt;/commandScript&gt; &lt;logs&gt; &lt;log&gt; &lt;logId&gt;elasticsearch_master&lt;/logId&gt; &lt;primary&gt;true&lt;/primary&gt; &lt;/log&gt; &lt;/logs&gt; &lt;/component&gt; &lt;component&gt; &lt;name&gt;ELASTICSEARCH_SLAVE&lt;/name&gt; &lt;displayName&gt;Elasticsearch Slave&lt;/displayName&gt; &lt;category&gt;SLAVE&lt;/category&gt; &lt;cardinality&gt;0+&lt;/cardinality&gt; &lt;versionAdvertised&gt;true&lt;/versionAdvertised&gt; &lt;commandScript&gt; &lt;script&gt;scripts/es_slave.py&lt;/script&gt; &lt;scriptType&gt;PYTHON&lt;/scriptType&gt; &lt;timeout&gt;1200&lt;/timeout&gt; &lt;/commandScript&gt; &lt;logs&gt; &lt;log&gt; &lt;logId&gt;elasticsearch_slave&lt;/logId&gt; &lt;primary&gt;true&lt;/primary&gt; &lt;/log&gt; &lt;/logs&gt; &lt;/component&gt; &lt;/components&gt; &lt;osSpecifics&gt; &lt;osSpecific&gt; &lt;osFamily&gt;any&lt;/osFamily&gt; &lt;packages&gt; &lt;package&gt; &lt;name&gt;elasticsearch-6.4.2&lt;/name&gt; &lt;!-- Not using. Please make sure the os already contains all the dependencies. --&gt; &lt;/package&gt; &lt;/packages&gt; &lt;/osSpecific&gt; &lt;/osSpecifics&gt; &lt;commandScript&gt; &lt;script&gt;scripts/service_check.py&lt;/script&gt; &lt;scriptType&gt;PYTHON&lt;/scriptType&gt; &lt;timeout&gt;300&lt;/timeout&gt; &lt;/commandScript&gt; &lt;configuration-dependencies&gt; &lt;config-type&gt;elasticsearch-config&lt;/config-type&gt; &lt;config-type&gt;elasticsearch-env&lt;/config-type&gt; &lt;config-type&gt;elasticsearch-log4j&lt;/config-type&gt; &lt;/configuration-dependencies&gt; &lt;restartRequiredAfterChange&gt;true&lt;/restartRequiredAfterChange&gt; &lt;quickLinksConfigurations&gt; &lt;quickLinksConfiguration&gt; &lt;fileName&gt;quicklinks.json&lt;/fileName&gt; &lt;default&gt;true&lt;/default&gt; &lt;/quickLinksConfiguration&gt; &lt;/quickLinksConfigurations&gt; &lt;/service&gt; &lt;/services&gt;&lt;/metainfo&gt; 主要参数###############################################################################定义服务的名称，显示名称，描述，版本号等核心信息 12345&lt;name&gt;ELASTICSEARCH&lt;/name&gt;&lt;displayName&gt;Elasticsearch&lt;/displayName&gt;&lt;comment&gt;A highly scalable open-source full-text search and analytics engine. Including storage, searching, and analyzing big volumes of data quickly and in near real time. &lt;/comment&gt;&lt;version&gt;6.4.2&lt;/version&gt; ###############################################################################定义服务的组件信息，这里针对elasticsearch的实现，分了两种组件：master和slave，这里以master为例： 123456789101112131415161718&lt;component&gt; &lt;name&gt;ELASTICSEARCH_MASTER&lt;/name&gt; &lt;displayName&gt;Elasticsearch Master&lt;/displayName&gt; &lt;category&gt;MASTER&lt;/category&gt; &lt;cardinality&gt;1+&lt;/cardinality&gt; &lt;versionAdvertised&gt;true&lt;/versionAdvertised&gt; &lt;commandScript&gt; &lt;script&gt;scripts/es_master.py&lt;/script&gt; &lt;scriptType&gt;PYTHON&lt;/scriptType&gt; &lt;timeout&gt;1200&lt;/timeout&gt; &lt;/commandScript&gt; &lt;logs&gt; &lt;log&gt; &lt;logId&gt;elasticsearch_master&lt;/logId&gt; &lt;primary&gt;true&lt;/primary&gt; &lt;/log&gt; &lt;/logs&gt;&lt;/component&gt; 其中基础信息包括组件名称(name),组件显示名称(displayname)。其他几个重要参数，包括： category：描述组件是主服务，节点从服务或是客户端 category类别 默认生命周期命令实现 MASTER install, start, stop, configure, status SLAVE install, start, stop, configure, status CLIENT install, configure, status cardinality：描述组件的数量限制 cardinality格式 格式说明举例 数字 1：表示只能有一个 数字区间 0-1：表示最少有零个，最多有一个 数字单增区间 1+：表示最少有1个 ALL ALL：表示所有节点都需要 commandScript：组件生命周期命令的执行脚本实现 脚本地址：/var/lib/ambari-server/resources/stacks/HDP/2.6/services/ELASTICSEARCH-6.4.x/package/scriptsAmbari支持Python脚本，按照Ambari官方文档的模式化脚本进行修改 ###############################################################################所支持的os系统定义，以及需要安装的软件包 1234567891011&lt;osSpecifics&gt; &lt;osSpecific&gt; &lt;osFamily&gt;any&lt;/osFamily&gt; &lt;packages&gt; &lt;package&gt; &lt;name&gt;elasticsearch-6.4.2&lt;/name&gt; &lt;!-- Not using. Please make sure the os already contains all the dependencies. --&gt; &lt;/package&gt; &lt;/packages&gt; &lt;/osSpecific&gt;&lt;/osSpecifics&gt; osFamily可定义如：Redhat6，Redhat7，ubuntu14，ubuntu16等。package-name，即为yum install或apt-get install的软件包名称 ###############################################################################服务检查的脚本实现【此处py脚本的命名要保持一致】 12345&lt;commandScript&gt; &lt;script&gt;scripts/service_check.py&lt;/script&gt; &lt;scriptType&gt;PYTHON&lt;/scriptType&gt; &lt;timeout&gt;300&lt;/timeout&gt;&lt;/commandScript&gt; ###############################################################################配置文件依赖，即服务安装前需要填写或者修改的配置文件： 12345&lt;configuration-dependencies&gt; &lt;config-type&gt;elasticsearch-config&lt;/config-type&gt; &lt;config-type&gt;elasticsearch-env&lt;/config-type&gt; &lt;config-type&gt;elasticsearch-log4j&lt;/config-type&gt;&lt;/configuration-dependencies&gt; ###############################################################################配置修改后是否重启【一般都为true】： 1&lt;restartRequiredAfterChange&gt;true&lt;/restartRequiredAfterChange&gt; ###############################################################################界面快速链接的json脚本实现【注意，此json脚本的命名要保持一致】： 123456&lt;quickLinksConfigurations&gt; &lt;quickLinksConfiguration&gt; &lt;fileName&gt;quicklinks.json&lt;/fileName&gt; &lt;default&gt;true&lt;/default&gt; &lt;/quickLinksConfiguration&gt;&lt;/quickLinksConfigurations&gt; ###############################################################################其他参数：ambari自定义服务（二）metainfo.xml说明官方文档 configuration - 服务配置项描述configuration的目录及内容如下图所示(不包括original，这是为了方便开发放入的原始配置文件) 12345678[root@vm01 configuration]# pwd/var/lib/ambari-server/resources/stacks/HDP/2.6/services/ELASTICSEARCH-6.4.x/configuration[root@vm01 configuration]# lltotal 24-rw-r--r-- 1 root root 4934 Oct 10 18:15 elasticsearch-config.xml-rw-r--r-- 1 root root 5406 Oct 10 18:15 elasticsearch-env.xml-rw-r--r-- 1 root root 4456 Oct 10 18:15 elasticsearch-log4j.xmldrwxr-xr-x 2 root root 148 Oct 10 18:15 original 配置类型这里可以简单的分为两种类型的配置文件，即-env和其他 -env.xml配置文件要求必须存在，主要描述安装包路径，用户，组，pid文件目录，根目录等。文件内容此处截取部分作为简单举例：【web页面上配置页面处的主要参数在此设置】 123456789101112131415161718192021222324252627282930313233343536&lt;property&gt; &lt;name&gt;elasticsearch.download.url&lt;/name&gt; &lt;value&gt;&lt;/value&gt; &lt;display-name&gt;Elasticsearch Download Url&lt;/display-name&gt; &lt;description&gt;Elasticsearch package download url. The official download url is: https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.4.2.tar.gz&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;elasticsearch.user&lt;/name&gt; &lt;value&gt;elasticsearch&lt;/value&gt; &lt;display-name&gt;Elasticsearch User&lt;/display-name&gt; &lt;property-type&gt;USER&lt;/property-type&gt; &lt;description&gt;Elasticsearch unix user.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;elasticsearch.group&lt;/name&gt; &lt;value&gt;elasticsearch&lt;/value&gt; &lt;display-name&gt;Elasticsearch Group&lt;/display-name&gt; &lt;property-type&gt;GROUP&lt;/property-type&gt; &lt;description&gt;Elasticsearch unix group.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;elasticsearch.base.dir&lt;/name&gt; &lt;value&gt;/opt/elasticsearch&lt;/value&gt; &lt;display-name&gt;Elasticsearch Base Directory&lt;/display-name&gt; &lt;description&gt;Elasticsearch base directory.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;elasticsearch.pid.dir&lt;/name&gt; &lt;value&gt;/var/run/elasticsearch&lt;/value&gt; &lt;display-name&gt;Elasticsearch Pid Directory&lt;/display-name&gt; &lt;description&gt;Elasticsearch pid file directory.&lt;/description&gt; &lt;/property&gt; 其他应用配置文件 其他应用配置配置文件即值应用本身所需的配置文件，例如elasticsearch的核心配置文件：elasticsearch.yml。但是在实现的时候，不用严格按照原始配置文件的格式。可以将elasticsearch.yml分为多个部分。文件名就是配置参数的上的块名 此处的配置文件设置，具体的实现就在web页面，如下图：【对比看上面configuration目录设置】 配置格式默认使用最简单的key-value格式，如下： 1234&lt;property&gt; &lt;name&gt;elasticsearch.base.dir&lt;/name&gt; &lt;value&gt;/opt/elasticsearch&lt;/value&gt;&lt;/property&gt; 但是此种格式，缺少说明，且web页面显示不太友好。可以添加来替换的显示；添加,来显示说明和描述 1234567&lt;property&gt; &lt;name&gt;elasticsearch.base.dir&lt;/name&gt; &lt;value&gt;/opt/elasticsearch&lt;/value&gt; &lt;display-name&gt;Elasticsearch Base Directory&lt;/display-name&gt; &lt;description&gt;Elasticsearch base directory.&lt;/description&gt;&lt;/property&gt; 效果图： 默认情况下，值不能为空，如果想允许值为空，可在下添加为true： 12345678&lt;property&gt; &lt;name&gt;server.basePath&lt;/name&gt; &lt;value&gt;none&lt;/value&gt; &lt;value-attributes&gt; &lt;empty-value-valid&gt;true&lt;/empty-value-valid&gt; &lt;/value-attributes&gt; &lt;display-name&gt;Server Base Path&lt;/display-name&gt;&lt;/property&gt; 还支持一些常见的格式，例如boolean：此处要注意，xml文件中填写值的时候需要小写（false，true）。但后续代码在获取参数时，获取的是boolean类型的False和True 12345678910&lt;property&gt; &lt;name&gt;server.rewriteBasePath&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;value-attributes&gt; &lt;type&gt;boolean&lt;/type&gt; &lt;overridable&gt;false&lt;/overridable&gt; &lt;/value-attributes&gt; &lt;display-name&gt;Server Rewrite Base Path&lt;/display-name&gt; &lt;description&gt;xxxx&lt;/description&gt;&lt;/property&gt; 针对密码类型的支持：将值显示为*号 123456789&lt;property&gt; &lt;name&gt;zeppelin.ssl.keystore.password&lt;/name&gt; &lt;value&gt;change me&lt;/value&gt; &lt;value-attributes&gt; &lt;type&gt;password&lt;/type&gt; &lt;/value-attributes&gt; &lt;property-type&gt;PASSWORD&lt;/property-type&gt; &lt;description&gt;Keystore password. Can be obfuscated by the Jetty Password tool&lt;/description&gt;&lt;/property&gt; 针对数字类型的支持，其中type可以为int,long等，minimum和maximum为可选参数用于给定范围，unit为单位显示等，例如： 123456789101112131415&lt;property&gt; &lt;name&gt;ops.interval&lt;/name&gt; &lt;value&gt;5000&lt;/value&gt; &lt;value-attributes&gt; &lt;type&gt;int&lt;/type&gt; &lt;minimum&gt;100&lt;/minimum&gt; &lt;maximum&gt;50000&lt;/maximum&gt; &lt;unit&gt;Milliseconds&lt;/unit&gt; &lt;increment-step&gt;100&lt;/increment-step&gt; &lt;overridable&gt;false&lt;/overridable&gt; &lt;/value-attributes&gt; &lt;display-name&gt;Ops Interval&lt;/display-name&gt; &lt;description&gt;Set the interval in milliseconds to sample system and process performance metrics. Minimum is 100ms. Defaults to 5000.&lt;/description&gt;&lt;/property&gt; 具体其他参数可参考Ambari平台的参数和对应的页面路径/var/lib/ambari-server/resources/common-services package-服务交互代码实现123456789101112131415161718192021[root@vm01 package]# pwd/var/lib/ambari-server/resources/stacks/HDP/2.6/services/ELASTICSEARCH-6.4.x/package[root@vm01 package]# lltotal 24-rwxr-xr-x 1 root root 23777 Oct 10 18:15 archive.zipdrwxr-xr-x 2 root root 129 Oct 10 18:15 scriptsdrwxr-xr-x 2 root root 111 Oct 10 18:15 templates[root@vm01 package]# tree.├── archive.zip├── scripts│ ├── es_master.py│ ├── es_slave.py│ ├── __init__.py│ ├── params.py│ ├── service_check.py│ └── status_params.py└── templates ├── elasticsearch.jvm.options.j2 ├── elasticsearch.master.yml.j2 └── elasticsearch.slave.yml.j2 scripts - 脚本实现 其中，params、service_check和status_params为必实现的脚本，分别用于参数的获取和加工、服务检查、为服务检查提供参数；es_master和es_slave是根据组件（component）区分的声明周期实现脚本，如下所示： 脚本 具体实现 要求及说明 params.py 通过Script类的方法读取之前configuration文件夹中.xml文件定义的各个配置变量，有需要的话进行进一步加工,例如单位的添加，列表的解析，布尔值的转换等 脚本名称不可修改 service_check.py 可直接使用内置方法实现，具体请见源码 脚本名称要与metainfo.xml中定义的保持一致 status_params.py 引用pid文件目录及文件参数即可 内容参数可以与params中的对应项保持一致（之所以独立，是受ambari的检查机制读所限，其在做检查时是不会走params的） es_master.py/es_slave.py 组件的install，start，stop等命令的实现 脚本名称要与metainfo.xml中定义的保持一致 templates - 配置模板【不懂，测试，可将此设置为变量】 通过.xml对配置的定义，以及params.py对配置项的处理，以jinjia2为基础的templates模板将最终完成配置映射到原始应用配置文件的过程。例如：在elasticsearch.master.yml.j2文件中引用params中定义好的用于master节点的配置，如： 123456789# ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):#path.data: &#123;&#123;master_path_data&#125;&#125;## Path to log files:#path.logs: &#123;&#123;master_path_logs&#125;&#125; 在configure方法的实现中，实现master模板和对应master配置文件的替换： 1234567configurations = params.config['configurations']['elasticsearch-config']File(format("&#123;es_master_conf_dir&#125;/elasticsearch.yml"), content=Template("elasticsearch.master.yml.j2", configurations=configurations), owner=params.es_user, group=params.es_group) 其他资源文件 quicklinks基础格式如下，只需改写显示名称，组件名称，对应的端口号即可。property请填写.xml中定义的端口参数名称（此处作者比较偷懒，直接走默认端口）： 12345678910111213141516171819202122232425262728&#123; "name": "default", "description": "default quick links configuration", "configuration": &#123; "protocol": &#123; "type":"http", "checks":[ ] &#125;, "links": [ &#123; "name": "elasticsearch_ui", "label": "Elasticsearch UI", "requires_user_name": "false", "component_name": "ELASTICSEARCH_MASTER", "url":"%@://%@:%@", "port":&#123; "http_property": "http_port", "http_default_port": "9200", "https_property": "http_port", "https_default_port": "9200", "regex": "\\w*:(\\d+)", "site": "elastic-config" &#125; &#125; ] &#125;&#125; 其它更为复杂的alerts.json配置同样请参考Ambari（HDP）下内置Hadoop组件的配置：/var/lib/ambari-server/resources/common-services。 参考从Elasticsearch详解Ambari与第三方软件的集成（一）从Elasticsearch详解Ambari与第三方软件的集成（二）从Elasticsearch详解Ambari与第三方软件的集成（三）Elasticsearch与Kibana的集成Redis的集成ambari 二次开发之增加ElasticSearch服务ElasticSearch和solr的差别自定义service服务官方文档]]></content>
      <categories>
        <category>Learn</category>
      </categories>
      <tags>
        <tag>HDP</tag>
        <tag>Ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala[2]-基本类型及操作，程序控制结构]]></title>
    <url>%2F2019%2F10%2F10%2FLearn%2Fscala%2Fscala-2-%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B%E5%8F%8A%E6%93%8D%E4%BD%9C%EF%BC%8C%E7%A8%8B%E5%BA%8F%E6%8E%A7%E5%88%B6%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[参考Scala入门到精通—— 第二节Scala基本类型及操作、程序控制结构]]></content>
      <categories>
        <category>Learn</category>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala[1]-简介，变量，函数]]></title>
    <url>%2F2019%2F10%2F10%2FLearn%2Fscala%2Fscala-1-%E7%AE%80%E4%BB%8B%EF%BC%8C%E5%8F%98%E9%87%8F%EF%BC%8C%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[scala简介 scala变量定义123456789101112131415161718192021#不可变变量scala&gt; val helloString="hello String"helloString: String = hello Stringscala&gt; helloString="hello"&lt;console&gt;:12: error: reassignment to val helloString="hello"#可变变量 ^scala&gt; var helloString2="hello String2"helloString2: String = hello String2scala&gt; helloString2="hello"helloString2: String = hello# 延迟加载变量scala&gt; lazy val helloString3="hello Stirng3"helloString3: String = &lt;lazy&gt;## 在使用时才会给变量赋值，scala&gt; helloString3res0: String = hello Stirng3 val,var用于声明变量val：不可变var：可变 scala函数1234567891011121314#定义一个函数，使用return返回结果【java是 `类型 值`;scala是`值:类型`】scala&gt; def add(a:Int,b:Int):Int=&#123;return a+b&#125;add: (a: Int, b: Int)Intscala&gt; add(25,36)res1: Int = 61#可以省略return，scala会将最后一句执行语句，作为返回结果#可以省略返回值类型，scala会自动进行类型推断scala&gt; def add(a:Int,b:Int)=&#123;a+b&#125;add: (a: Int, b: Int)Intscala&gt; add(25,36)res2: Int = 61 helloworld12345object helloworld &#123; def main(args: Array[String]): Unit = &#123; println("helloworld") &#125;&#125; 参考Scala入门到精通——第一节 Scala语言初步Scala惰性赋值：lazy的使用]]></content>
      <categories>
        <category>Learn</category>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sparkStreamin+kafka实操]]></title>
    <url>%2F2019%2F10%2F10%2FLearn%2FsparkStreamin-kafka%E5%AE%9E%E6%93%8D%2F</url>
    <content type="text"><![CDATA[随机产生单词,写入kafka123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/*** @program: ztgx4-&gt;Scala_SetData* @description: 产生实时流数据* @author: lwz* @create: 2019-09-30 11:40* * while true实时产生数据，用于实时处理word-count * * 数据写入kafka * * 26个英文字母和1-9的数字随机组合 * * 设置不同时间段的间隔，用于不同时间的随机产生**/object Scala_SetData &#123; def main(args: Array[String]): Unit = &#123; /*生产数据*/// 定义字典表 val DicData=Array("a","b","c","d","e","f","g","h","i","j","k","l","m","n","o","p","q","r","s","t","u","v","w","x","y","z","0","1","2","3","4","5","6","7","8","9")// 定义随机时间 单位ms val DicDate=Array(1500,2000,3000,100,500,4000,10000)// 时间格式 val DateFramt : SimpleDateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:SS") while (true)&#123; var word=""// 获取单词,4字母单词,拼接成一行，一行有10个单词 for (i &lt;- 1 to 4) &#123; val num = Random.nextInt(36) word += DicData(num) &#125; println(word) /*数据写入kafka*/// 设置参数 val prop : Properties = new Properties() prop.setProperty("bootstrap.servers", "vm110:6667,vm111:6667,vm112:6667") prop.setProperty("key.serializer", classOf[StringSerializer].getName) prop.setProperty("value.serializer", classOf[StringSerializer].getName)// 生产者 val kafkaPro : KafkaProducer[String, String] = new KafkaProducer[String,String](prop)// 定义topic val topic="test"// val words=String.valueOf(word) val mes : ProducerRecord[String, String] = new ProducerRecord[String,String](topic,2,"",word) kafkaPro.send(mes) /*设置信息产生间隔*/// 随机数，获取随机时间角标 val DicDate_num = Random.nextInt(7)// 睡眠，按照随机时间 Thread.sleep(DicDate(DicDate_num).toLong) &#125; &#125;&#125; sparkstreaming从kafka读取数据，wordcount123456789101112131415161718192021222324252627282930313233343536373839404142434445464748object StreaminWordCount &#123; /*log4j*/ Logger.getLogger("org").setLevel(Level.ERROR) def main(args: Array[String]): Unit = &#123; /*ssc*/ val conf : SparkConf = new SparkConf() conf.setAppName(this.getClass.getSimpleName) conf.setMaster("local[*]") val ssc : StreamingContext = new StreamingContext(conf, Seconds(10)) /*kafkaparams*/ val kafkaPrams : mutable.HashMap[String, Object] = new mutable.HashMap[String,Object]() kafkaPrams += ("bootstrap.servers" -&gt; "vm110:6667,vm111:6667,vm112:6667") kafkaPrams += ("key.deserializer" -&gt; classOf[StringDeserializer].getName) kafkaPrams += ("value.deserializer" -&gt; classOf[StringDeserializer].getName) kafkaPrams += ("group.id" -&gt; "test7") kafkaPrams += ("auto.offset.reset" -&gt; "earliest") /*kafkaUtils*/ val dsStream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream(ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe(Array("test"), kafkaPrams: collection.Map[String, Object]) ) /*wordcount*/// map中for循环的，将for循环得到的数据放入集合中，使用flatmap压平，在使用map变成元组 dsStream.foreachRDD( rdds =&gt; &#123; val resWord= rdds.flatMap(values =&gt; &#123; val word = values.value().toString val arrs : Array[String] = word.split("") arrs &#125;) val resWc : RDD[(String, Int)] = resWord.map((_,1)).reduceByKey(_+_).filter(f =&gt; !f._1.isEmpty) resWc.foreach(println) &#125;) ssc.start() ssc.awaitTermination() &#125;&#125;]]></content>
      <categories>
        <category>Learn</category>
      </categories>
      <tags>
        <tag>sparkStreaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的家庭菜单]]></title>
    <url>%2F2019%2F10%2F07%2FEssays%2F%E6%88%91%E7%9A%84%E5%AE%B6%E5%BA%AD%E8%8F%9C%E5%8D%95%2F</url>
    <content type="text"><![CDATA[Please enter the password to read the blog. Incorrect Password! No content to display! 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]]></content>
      <categories>
        <category>Essays</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[自我观察]]></title>
    <url>%2F2019%2F10%2F07%2FEssays%2F%E8%87%AA%E6%88%91%E8%A7%82%E5%AF%9F%2F</url>
    <content type="text"><![CDATA[Please enter the password to read the blog. Incorrect Password! No content to display! U2FsdGVkX19I4yxRr4tzcsZkcvmiJ1ORrohK1yxNxCW91vmxZ+7sSNUkbTfabRNArzDTiUeLG+h4DpjpaRlE4m90jnmMTzFta0vgWJXxNkyiBMkhL1CkVkZolUnovdCwT2qjaOkA0zEKVi4ef1kYFayZ5kven75cRR7aXWkOR3NuDCu4vob1qAr4r0uCecEJTviGz0vHdkCUALFMA2Lzfo+3eFZ6nMp8MdcU8qmteAursh7s6MNSEfKO0KAl27ypKZ9l3KJBvZQr4eK9t2sgPvTSNwyul2nv0JRNkvqe55hEy7YBBYZYJyzyjWI9JBst7CW9KNlyormOgGQj95rHfpdDee2TlenHgzJLWX7oJHCsId8kfJIQeISJx2JiL9iheTLYi/sgE+ngkdtJLfBwnqcH5B5mfCN0ESutvyDRJb427kFcX+Q22o0CHTW7n8vq8QebM+OFlSTb7PfbXY3B1TTAwgaBhMh2eN4Gd6pQtAJ3McDqqCPe1tJpc+9m/p1BN/9zFu6V5Jx2ylQ48D6tZzjqvYRDEC0572pIdvqNQlzH0JoQLQ4ENZJql6UxmQMHRcnAxSRxDhHQmkZs1MCYGtCg2JbYvK8hqdJZYzOIz5IztBSRMhM11qv/heMf7znjpQOWMluWatL192y31SBwLehxv06HSpCGz5JNRb9IomVzl5f7lmaeKh8O1zhnUVuOXD6gG93d7B7fry09RlegzRH9TVWTdP1MDxo+H1AdBmdLxgu+twYLqVebkKsGDNsKcrFuMawqfZ8Wdl/kYBB/Rh7lPmPzy8twMDz9EiWrUnWD+j7goNw6+D0elLS8iPb8W2eGwMODPQa/oE6SVmMonVxTVFk1IA8qr2PqpeKKxwl6xv8mg/N2cFgKlqTmvAXCqNdn/8JeaSJnkSaQt7yhPDI+BEcZ3bQuCokhXNBbBzfhd2w1Anjt6ShngWZXLaFdxolS4tgUnWBGQ9GBr2ii37gfUdFhQdpCoSo7M+9FaWdY1QVyH0f2Ip/MtV+1es6gDECZFjAc/bRm0ouLUjD3/QHHFlAdNFl15nY+T/EzxPk2HWhPCSIphqI/aQibQSVqvJzJtSkqghQdhRosQwZGBE4ffFkTiKH+c7jSomn4AJ+eRZnhHeLAllo7A8CrH2sqea/oWc8+j7/kBQKs/2d2Ac5tj9J8vhhO42OllnlElTG8KiGgDGOnsKXZcKWxxdMYojRd8BPIMlxQffu+1TyD8SCCMpfuECabgHpcmYnmj30=]]></content>
      <categories>
        <category>Essays</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hbase压缩]]></title>
    <url>%2F2019%2F09%2F29%2FBigData%2FHbase%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[Hbase压缩压缩算法的比较 算法 压缩比 压缩 解压 LZO 20.5% 135MB/s 410MB/s GZIP 13.4% 21MB/S 118MB/S Snappy/Zippy 22.2% 172MB/s 409MB/s 总结： GZIP的压缩率最高，但它是CPU密集型的，对CPU的消耗较多，压缩和解压速度也慢； LZO的压缩率居中，比GZIP要低一些，但是压缩和解压速度明显要比GZIP快很多，其中解压速度快的更多； Zippy/Snappy的压缩率最低，而压缩和解压速度要稍微比LZO要快一些 所以，一般情况下，推荐使用snappy和zippy压缩算法 hbase表启动压缩的步骤这里分两种情况：一是在创建表指定压缩算法，二是在创建表后指定压缩算法或者修改压缩算法 创建表时指定压缩算法123create 'test', &#123;NAME =&gt; 'info', VERSIONS =&gt; 1, COMPRESSION =&gt; 'snappy'&#125;# 表创建之后，使用describe命令查看表信息describe 'test' 创建表后指定或者修改压缩算法 disable需要修改的表 disable &#39;test&#39; 使用alter命令进行修改 alter &#39;test&#39;, NAME =&gt; &#39;info&#39;, COMPRESSION =&gt; &#39;snappy&#39; NAME即column family，列族。HBase修改压缩格式，需要一个列族一个列族的修改，注意大小写，不要弄错了。如果修改错了，将会创建一个新的列族，且压缩格式为snappy。当然，假如你还是不小心创建了一个新列族的话，可以通过以下方式删除： alter &#39;test&#39;, {NAME=&gt;&#39;info&#39;, METHOD=&gt;&#39;delete&#39;} 重新enable表 enable &#39;test&#39; 对表进行major_compact操作，使压缩生效 major_compact &#39;test&#39; 注意，如果表的数据较多，该操作需要较长时间，所以尽量选择一个不忙的时间，避免对服务造成影响。 修改完成后，可使用describe命令查看表信息]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据导入Hbase的四种方法]]></title>
    <url>%2F2019%2F09%2F27%2FBigData%2F%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5Hbase%E7%9A%84%E5%9B%9B%E7%A7%8D%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[put方法写入12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** *classname:PutToHbase *auther:LWZ *date:2019/9/26 9:59 * put方法*/object PutToHbase &#123; def main(args: Array[String]): Unit = &#123; /*sparkcontext*/ /*sparkcontext可以makerdd*/ val conf : SparkConf = new SparkConf().setAppName(this.getClass.getSimpleName).setMaster("local[*]") val context : SparkContext = new SparkContext(conf) /*make RDD*/ val rdd : RDD[Int] = context.makeRDD(1 to 100000,4) println("count is:"+rdd.count()) rdd.take(20).foreach(println) /*family*/ val family : Array[Byte] = Bytes.toBytes("info") /*column*/ val column : Array[Byte] = Bytes.toBytes("column_name") rdd.foreachPartition(fore =&gt; &#123; val getTable : Table = gettable() fore.foreach( value =&gt; &#123; val put : Put = new Put(Bytes.toBytes(value)) put.addImmutable(family,column,Bytes.toBytes(value)) getTable.put(put) &#125;) getTable.close() &#125;) context.stop() &#125; def gettable():Table=&#123; /*获取hbase连接*/ val configuration : Configuration = HBaseConfiguration.create() configuration.set("hbase.zookeeper.quorum", "vm110,vm111,vm112") configuration.set("hbase.zookeeper.property.clientPort", "2181") configuration.set("hbase.defaults.for.version.skip", "true") configuration.set("zookeeper.znode.parent", "/hbase-unsecure") val conn : Connection = ConnectionFactory.createConnection(configuration) /*表名*/ val table="test01" /*gettable*/ val getTable : Table = conn.getTable(TableName.valueOf(table)) getTable &#125;&#125; 速度： put list方法写入123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** *classname:PutListToHbase *auther:LWZ *date:2019/9/26 10:49 * */object PutListToHbase &#123; def main(args: Array[String]): Unit = &#123; /*sparkcontext*/ val conf: SparkConf = new SparkConf() .setMaster("local[*]") .setAppName(this.getClass.getSimpleName) val context : SparkContext = new SparkContext(conf) /*make rdd*/ val rdd : RDD[Int] = context.makeRDD(1 to 100000,4) /*family*/ val family : Array[Byte] = Bytes.toBytes("info") /*column*/ val column : Array[Byte] = Bytes.toBytes("column_age") rdd.foreachPartition(fore =&gt; &#123; val table : Table = gettable() val lst : util.LinkedList[Put] = new util.LinkedList[Put]() fore.foreach(value =&gt; &#123; /*row_key*/ val put : Put = new Put(Bytes.toBytes(value)) put.addImmutable(family,column,Bytes.toBytes(value)) lst.add(put) &#125;) gettable().put(lst) gettable().close() &#125;) context.stop() &#125; /*gettable*/ def gettable():Table=&#123; /*获取hbase连接*/ val configuration : Configuration = HBaseConfiguration.create() configuration.set("hbase.zookeeper.quorum", "vm110,vm111,vm112") configuration.set("hbase.zookeeper.property.clientPort", "2181") configuration.set("hbase.defaults.for.version.skip", "true") configuration.set("zookeeper.znode.parent", "/hbase-unsecure") val conn : Connection = ConnectionFactory.createConnection(configuration) /*表名*/ val table="test01" /*gettable*/ val getTable : Table = conn.getTable(TableName.valueOf(table)) getTable &#125;&#125; 速度：比put方法快很多 使用saveAsNewAPIHadoopDataset API12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** *classname:BulkLoadToHbase3 *auther:LWZ *date:2019/9/27 11:42 * saveAsNewAPIHadoopDataset api的使用 * 此api用于使用put，对多字段数据，批量导入hbase * 注：put和bulkload不能同时使用*/object BulkLoadToHbase3 &#123; def main(args: Array[String]): Unit = &#123; /*sparkcontext*/ val conf: SparkConf = new SparkConf() .setMaster("local[*]") .setAppName(this.getClass.getSimpleName) val context: SparkContext = new SparkContext(conf) /*makerdd*/ val rdd: RDD[Int] = context.makeRDD(1 to 100000, 4) /*组合成多字段的tuple*/ val rdd2 = rdd.map(line =&gt; &#123; (line, line + "name", line + "age") &#125;) /*基本参数*/ val tablename = "test01" val family = Bytes.toBytes("info") /*获取Hbase连接*/ val configuration: Configuration = HBaseConfiguration.create() configuration.set("hbase.zookeeper.quorum", "vm110,vm111,vm112") configuration.set("hbase.zookeeper.property.clientPort", "2181") configuration.set("hbase.defaults.for.version.skip", "true") configuration.set("zookeeper.znode.parent", "/hbase-unsecure") configuration.set(TableOutputFormat.OUTPUT_TABLE, tablename) /*data清洗+排序+生成kv_list*/ val rdd3 : RDD[(Int, String, String)] = rdd2.sortBy(so=&gt; so._1) val data=rdd3.map(value =&gt; &#123; val put : Put = new Put(Bytes.toBytes(value._1)) put.addColumn(family,Bytes.toBytes("name"),Bytes.toBytes(value._2)) put.addColumn(family,Bytes.toBytes("age"),Bytes.toBytes(value._3)) (new ImmutableBytesWritable(),put) &#125;) /*save Hfiles on hdfs*/ val job : Job = Job.getInstance(configuration) job.setOutputKeyClass(classOf[ImmutableBytesWritable]) job.setOutputValueClass(classOf[Put]) job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]]) data.saveAsNewAPIHadoopDataset(job.getConfiguration) context.stop() &#125;&#125; 速度：比putlist快 bulkload方法[kv]12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** *classname:BulkLoadToHbase2 *auther:LWZ *date:2019/9/27 10:36 * bulkload无法对put进行操作，只能操作keyvalue。但是keyvalue只能操作一个字段 * 现在将多个keyvalue放入集合，再遍历，达到解决多个字段的效果 * * 测试之后，发现不可以 * * 对于多个字段还是得使用put * 判断是否为bulkload，观察hbase master页面，是否有数据插入的情况 * 此方法表现状况为：页面无插入详情，条数也不显示，但是count hbase表，有数据*/object BulkLoadToHbase2 &#123; def main(args: Array[String]): Unit = &#123; /*sparkcontext*/ val conf : SparkConf = new SparkConf().setMaster("local[*]").setAppName(this.getClass.getSimpleName) val context : SparkContext = new SparkContext(conf) /*makerdd*/ val rdd : RDD[Int] = context.makeRDD(1 to 100000,4) /*组合成多字段的tuple*/ val rdd2=rdd.map(line =&gt; &#123; (line,line+"name",line+"age") &#125;) /*基本参数*/ val tablename="test01" val HfilePath = "/tmp/test01" val family=Bytes.toBytes("info") /*获取Hbase连接*/ val configuration: Configuration = HBaseConfiguration.create() configuration.set("hbase.zookeeper.quorum", "vm110,vm111,vm112") configuration.set("hbase.zookeeper.property.clientPort", "2181") configuration.set("hbase.defaults.for.version.skip", "true") configuration.set("zookeeper.znode.parent", "/hbase-unsecure") configuration.set(TableOutputFormat.OUTPUT_TABLE, tablename) /*get table*/ val conn: Connection = ConnectionFactory.createConnection(configuration) val admin : Admin = conn.getAdmin val table : Table = conn.getTable(TableName.valueOf(tablename)) val locator : RegionLocator = conn.getRegionLocator(TableName.valueOf(tablename)) /*FileSystem*/ val filesystem : FileSystem = FileSystem.get(configuration) if(filesystem.exists(new Path(HfilePath)))&#123; filesystem.delete(new Path(HfilePath)) &#125; /*data清洗+排序+生成kv_list*/ val rdd3 : RDD[(Int, String, String)] = rdd2.sortBy(so=&gt; so._1) val data=rdd3.map(value =&gt; &#123; val kv : KeyValue = new KeyValue(Bytes.toBytes(value._1),family,Bytes.toBytes("name"),Bytes.toBytes(value._2+"+++++"+value._3)) (new ImmutableBytesWritable(),kv) &#125;) /*save Hfiles on HDFS*/ val job : Job = Job.getInstance(configuration) job.setMapOutputKeyClass(classOf[ImmutableBytesWritable]) job.setMapOutputValueClass(classOf[KeyValue]) HFileOutputFormat2.configureIncrementalLoadMap(job,table) data.saveAsNewAPIHadoopFile(HfilePath,classOf[ImmutableBytesWritable],classOf[KeyValue],classOf[HFileOutputFormat2],configuration) /*Bulk load Hfils to Hbase*/ val bulkloader : LoadIncrementalHFiles = new LoadIncrementalHFiles(configuration) bulkloader.doBulkLoad(new Path(HfilePath),admin,table,locator) conn.close() filesystem.close() context.stop() &#125;&#125; 注生成HFile的过程比较慢，生成HFile后写入hbase非常快，基本上就是hdfs上的mv过程.对于生成HFile方式入库的时候有一个改进的方案，就是先对数据排序，然后生成HFile。 HFile方式在所有的加载方案里面是最快的，不过有个前提——数据是第一次导入，表是空的。如果表中已经有了数据。HFile再导入到hbase的表中会触发split操作，最慢的时候这种操作会耗时1小时。 在生产中，需要导入hbase的数据分为up和add两部分，即更新数据和新增数据。所以会出现部分很快，部分会慢一些的情况。【只是理论上来讲，应该先慢后快，实际上先快后慢】 缓存块方法123456789101112131415161718192021222324252627282930313233343536373839404142/** *classname:BulkLoadToHbase4 *auther:LWZ *date:2019/9/27 17:11 * 缓存块方式写数据到Hbase*/object BulkLoadToHbase4 &#123; def main(args: Array[String]): Unit = &#123; /*sparksession*/ val session=SparkSession.builder() .master("local[*]") .appName(this.getClass.getSimpleName) .getOrCreate() /*makerdd*/ val rdd : RDD[Int] = session.sparkContext.makeRDD(1 to 1000000,4) rdd.foreachPartition(ma =&gt; &#123; /*获取Hbase连接*/ val configuration: Configuration = HBaseConfiguration.create() configuration.set("hbase.zookeeper.quorum", "vm110,vm111,vm112") configuration.set("hbase.zookeeper.property.clientPort", "2181") configuration.set("hbase.defaults.for.version.skip", "true") configuration.set("zookeeper.znode.parent", "/hbase-unsecure") /*htable*/ val htable : HTable = new HTable(configuration, TableName.valueOf("test01")) htable.setAutoFlush(false,false) //关键点1 htable.setWriteBufferSize(512*1024*1024) //关键点2 /*data*/ ma.foreach(value =&gt; &#123; val put : Put = new Put(Bytes.toBytes(value)) put.addColumn(Bytes.toBytes("info"),Bytes.toBytes("name"),Bytes.toBytes(value)) htable.put(put) &#125;) htable.flushCommits() //关键点3 &#125;) session.stop() &#125;&#125; 参考addcolumn和addImmutable有何区别spark写数据到hbase的三种方法HBase总结（1）– 数据插入与Put对象注：缓存块方法无法使用，已经被淘汰spark将数据写入hbase以及从hbase读取数据HBase 写优化之 BulkLoad 实现数据快速入库Hbase几种数据入库（load）方式比较Spark读写HBase之使用Spark自带的API以及使用Bulk Load将大量数据导入HBaseBulk Load——Spark 批量导入多列数据到HBase（scala/Java）]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测试集群增加节点以及删除节点]]></title>
    <url>%2F2019%2F09%2F17%2FBigData%2F%E6%B5%8B%E8%AF%95%E9%9B%86%E7%BE%A4%E5%A2%9E%E5%8A%A0%E8%8A%82%E7%82%B9%E4%BB%A5%E5%8F%8A%E5%88%A0%E9%99%A4%E8%8A%82%E7%82%B9%2F</url>
    <content type="text"><![CDATA[起因使用测试集群增加个人电脑的节点到集群中 步骤设置个人电脑虚拟机为桥接模式新机器设置ip1234567891011121314151617[root@test ~]# cat /etc/sysconfig/network-scripts/ifcfg-ens33TYPE=EthernetBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=ens33DEVICE=ens33ONBOOT=yesIPADDR=192.168.2.250NETMASK=255.255.255.0GATEWAY=192.168.2.1DNS1=8.8.8.8 注：设置为桥接模式，需处于和主机相同网段 修改主机名和主机映射12345678910111213[root@test ~]# vi /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.200.110 vm110192.168.200.111 vm111192.168.200.112 vm112192.168.200.200 bs00192.168.200.201 bs01192.168.200.202 bs02192.168.200.203 bs03192.168.2.250 test 修改/etc/sysconfig/network12345[root@test ~]# cat /etc/sysconfig/network# Created by anacondaNETWORKING=yes[root@test ~]# systemctl restart network 配置jdk配置免密123456789101112131415161718[root@vm110 home]# ssh-copy-id testThe authenticity of host 'test (192.168.2.250)' can't be established.ECDSA key fingerprint is 7f:0c:e8:7f:87:29:ed:e9:66:4c:1b:50:f8:26:ac:a5.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@test's password:Number of key(s) added: 1Now try logging into the machine, with: "ssh 'test'"and check to make sure that only the key(s) you wanted were added.[root@vm110 home]# ssh vm110 date; ssh vm111 date; ssh vm112 date ; ssh test date;Tue Sep 17 17:39:48 CST 2019Tue Sep 17 17:39:48 CST 2019Tue Sep 17 17:39:48 CST 2019Tue Sep 17 17:39:47 CST 2019 修改文件限制1234567[root@test jdk]# vi /etc/security/limits.conf# End of file* soft nofile 65536* hard nofile 131072* soft nproc 2048* hard nproc 4096 关闭防火墙设置selinux123456789101112[root@test jdk]# vi /etc/selinux/config# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of three values:# targeted - Targeted processes are protected,# minimum - Modification of targeted policy. Only selected processes are protected.# mls - Multi Level Security protection.SELINUXTYPE=targeted 设置ntp服务，同步时钟12345678910111213[root@test jdk]# vi /etc/ntp.conf#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver 192.168.200.110 prefer[root@test jdk]# ntpq -p remote refid st t when poll reach delay offset jitter============================================================================== vm110 120.25.115.20 3 u 13 64 1 2.422 49.073 0.000 修改yum源1234[root@vm110 yum.repos.d]# scp ambari.repo test:/etc/yum.repos.d/ambari.repo 100% 271 0.3KB/s 00:00[root@vm110 yum.repos.d]# scp hdp.repo test:/etc/yum.repos.d/hdp.repo 页面增加节点注意：在test节点安装hadoop组件时，出现了无法使用163镜像和阿里云镜像，虽然网络是可以ping通百度，但也是十分不稳定，所以采用了离线安装的方式。即：将已经准备好的local_yum上传至/var/www/html(需要提前安装httpd服务),然后配置本地yum源，将base的yum源移除，此时，节点上可使用的yum源为ambari，hdp，local_yum。遂安装成功 安装节点后测试wordcount测试wordcount数据生成123456789101112131415161718192021222324252627282930313233public class wddata &#123; public static void main(String[] args) &#123; //数据存储路径 String output= args[0]; String strs[]=&#123;"hello", "hi", "word", "hahah", "nihao", "spark", "html", "css"&#125;; String restr=""; Random random = new Random(); int flog=0; try &#123; OutputStreamWriter out = new OutputStreamWriter(new FileOutputStream(output)); for(int i=0;i&lt;10000000;i++)&#123; int num = random.nextInt(8); String str = strs[num]; out.write(str); out.write(","); flog+=1; if(flog==10)&#123; out.write("\r\n"); flog=0; &#125; &#125; out.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 打包上传至集群中运行，数据存储在节点本地 12345678910[root@vm110 testdata]# pwd/usr/local/src/liu/testdata[root@vm110 testdata]# lltotal 32-rw-r--r-- 1 root root 32262 Sep 18 10:55 test2-1.0-SNAPSHOT.jar[root@vm110 testdata]# java -cp test2-1.0-SNAPSHOT.jar wddata /usr/local/src/liu/testdata/data.data[root@vm110 testdata]# du -sh *51M data.data 将data.data上传至hdfs 1[hdfs@vm110 testdata]$ hadoop fs -put data.data /test/wd.txt 1[hdfs@vm110 testdata]$ spark-submit --master yarn --class wdcount test2-1.0-SNAPSHOT.jar wordcount123456789101112131415161718192021222324252627282930/** *classname:wdcount *auther:LWZ *date:2019/8/23 14:17 * */object wdcount &#123; /*saprksession*/ def main(args: Array[String]): Unit = &#123; val sessioin=SparkSession.builder()// .master("local[*]") .appName(this.getClass.getSimpleName) .getOrCreate() /*input*/ val input="hdfs://vm110:8020/test/wd.txt" /*src*/ val src : Dataset[String] = sessioin.read.textFile(input) val res: RDD[(String, Int)] =src.rdd.flatMap(line =&gt; &#123; val arrs : Array[String] = line.split(",") arrs &#125;).map(tp =&gt; (tp,1)).reduceByKey((re1,re2)=&gt;re1+re2) println(res.collect().toList) &#125;&#125; 结果： 1List((word,1250381), (hello,1250540), (hahah,1251383), (spark,1248842), (hi,1249971), (css,1247879), (html,1250719), (nihao,1250285)) 文件复制测试准备1.1G大文件在节点机器上，上传至hdfs 1234[hdfs@vm110 data]$ hadoop fs -mkdir /test[hdfs@vm110 data]$ hadoop fs -put testdata.data /test/data.data[hdfs@vm110 data]$ du -sh *1.2G testdata.data 由图可见，test节点使用量为675.74Mb 在要删除的节点设置datanode为Decommission 查看datanode页面，等待完成 block块对比before after 一个block块有三个副本，现在有四个，说明test上面的block块已经被成功复制 停止节点上的所有组件删除节点停止ambari-agent 1234567[root@test yum.repos.d]# ambari-agent stopVerifying Python version compatibility...Using python /usr/bin/pythonFound ambari-agent PID: 13784Stopping ambari-agentRemoving PID file at /run/ambari-agent/ambari-agent.pidambari-agent successfully stopped 删除 重启HDFS]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>HDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bigdata零零碎碎]]></title>
    <url>%2F2019%2F09%2F17%2FBigData%2Fbigdata%E9%9B%B6%E9%9B%B6%E7%A2%8E%E7%A2%8E%2F</url>
    <content type="text"><![CDATA[个人电脑虚拟机作为测试集群节点桥接模式是将主机当成为一个交换机，桥接模式的虚拟机和主机都可以看成通过这个交换机连接到外部网络，这两个机器不会互相影响，完全独立。但是得设置虚拟机的ip和网关和主机的ip和网关在同一网段 使用测试集群中节点ping主机，能够ping通，所以猜想，如果可以弄一个虚拟机处于和主机相同的网络环境，就可以将此虚拟机增加到集群之中 参考文章VMWare桥接方式设置及问题解决VMware的三种网络类型 hive的explode函数源数据如下： 1234[root@vm110 hivetest]# cat test.txtzs 1,2,3,5 25li 6,2 30hh 3 20 hive建表： 1234567891011121314151617hive&gt; create table explodeTest (name string,score array&lt;string&gt;,age string) &gt; ROW FORMAT DELIMITED &gt; FIELDS TERMINATED BY '\t' &gt; COLLECTION ITEMS TERMINATED BY ',';OKTime taken: 0.33 secondshive&gt; load data local inpath '/usr/local/src/liu/hivetest/test.txt' into table explodetest;Loading data to table liu.explodetestTable liu.explodetest stats: [numFiles=1, numRows=0, totalSize=32, rawDataSize=0]OKTime taken: 0.224 secondshive&gt; select * from explodeTest;OKzs ["1","2","3","5"] 25li ["6","2"] 30hh ["3"] 20Time taken: 0.156 seconds, Fetched: 3 row(s) explode演示： 1234567891011#行转列hive&gt; select explode(score) from explodeTest;OK1235623Time taken: 0.105 seconds, Fetched: 7 row(s) 使用lateral view 12345678910hive&gt; select name,age,score1 from explodeTest lateral view explode(score) score2 as score1;OKzs 25 1zs 25 2zs 25 3zs 25 5li 30 6li 30 2hh 20 3Time taken: 0.103 seconds, Fetched: 7 row(s) 参考Hive-explode[行转列]关键字使用 spark-sql中使用explode函数需要导入两个包，如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344# import org.apache.spark.sql.functions.explode# import session.implicits._package cn.sic_credit.bigdata.tycdata.com_bus_risks_industrialimport org.apache.spark.sql.functions.explodeimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;/** *classname:parseJsonETL *auther:LWZ *date:2019/9/23 11:41*/object parseJsonETL &#123; def main(args: Array[String]): Unit = &#123; /*sparksession*/ val session=SparkSession.builder() .master("local[*]") .appName(this.getClass.getSimpleName) .getOrCreate() import session.implicits._ /*args*/ val tableName="com_bus_risks-industrial" /*input*/ val input="hdfs://vm50:8020/flume/logs/tyc_data/2019-09-21/*" val output="" /*src*/ val src : DataFrame = session.read.json(input) src.show(20) src.printSchema() /**/ val df1 = src.select($"com_name",$"com_credit_code",(explode($"com_bus_risks-industrial") as "com_bus_risks-industrial")) df1.show(20) df1.printSchema() session.stop() &#125;&#125; sparkSQL并行度参数设置在代码中设置123val spark = SparkSession.builder() .config("spark.sql.shuffle.partitions",100)//设置并行度100 .getOrCreate() 提交任务时设置注：代码优先级高于提交时的优先级 123456789./bin/spark-submit \--class com.imooc.log.TopNStatJobYARN \--name TopNStatJobYARN \--master yarn \--executor-memory 1G \--num-executors 1 \--conf spark.sql.shuffle.partitions=100 \/home/hadoop/lib/sql-1.0-jar-with-dependencies.jar \hdfs://hadoop001:8020/imooc/clean 20170511 参考SparkSQL并行度参数设置方法 自定义yum，或者修改yum源的设置修改yum源： 修改 yum clean all #清除系统所有yum缓存 yum makecache #重新生成yum缓存 yum repolist #查看所有yum源 在自定义yum源，修改了yum源之后，推荐上述三个步骤全部弄一遍 参考CentOS7系统配置国内yum源和epel源 yum 指定安装某个源下的软件的方法一条命令一个参数搞定代码如下: 1[root@aikaiyuan ~]# yum install nginx --enablerepo=epel 稍微解释一下：代码如下:yum install XXX –enablerepo=YYYXXX是要安装的软件，YYY是repo源的名字。 安装mysql后无法找到临时密码的解决方案安装mysql后无法找到临时密码的解决方案]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Other</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux的压缩和解压缩]]></title>
    <url>%2F2019%2F09%2F12%2FLearn%2FLinux%E7%9A%84%E5%8E%8B%E7%BC%A9%E5%92%8C%E8%A7%A3%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[文件后缀的含义12345678910*.Z // compress程序压缩产生的文件(现在很少使用) *.gz // gzip程序压缩产生的文件 *.bz2 // bzip2程序压缩产生的文件 *.zip // zip压缩文件 *.rar // rar压缩文件 *.7z // 7-zip压缩文件 *.tar // tar程序打包产生的文件 *.tar.gz // 由tar程序打包并由gzip程序压缩产生的文件 *.tar.bz2 // 由tar程序打包并由bzip2程序压缩产生的文件 压缩命令–gzip，bzip2tar打包命令gzip 或 bzip2 带有多个文件作为参数时，执行的操作是将各个文件独立压缩，而不是将其放在一起进行压缩。这样就无法产生类似于Windows环境下的文件夹打包压缩的效果。(gzip与bzip2也可以使用文件夹作为参数，使用 -f 选项，但也是将其中的每个文件独立压缩)。为了实现打包压缩的效果，可以使用命令 tar 进行文件的打包操作(archive)，再进行压缩。 tar 指令可以将文件打包成文件档案(archive)存储在磁盘/磁带中，打包操作一般伴随压缩操作，也可以使用 tar 指令对打包压缩后的文件解压。 tar 常用命令参数如下： 123456789101112131415161718192021基本格式：tar [Options] file_archive //注意tar的第一参数必须为命令选项，即不能直接接待处理文件 常用命令参数： //指定tar进行的操作，以下三个选项不能出现在同一条命令中 -c //创建一个新的打包文件(archive) -x //对打包文件(archive)进行解压操作 -t //查看打包文件(archive)的内容,主要是构成打包文件(archive)的文件名 //指定支持的压缩/解压方式，操作取决于前面的参数，若为创建(-c),则进行压缩，若为解压(-x),则进行解压，不加下列参数时，则为单纯的打包操作(而不进行压缩)，产生的后缀文件为.tar -z //使用gzip进行压缩/解压，一般使用.tar.gz后缀 -j //使用bzip2进行压缩/解压，一般使用.tar.bz2后缀 //指定tar指令使用的文件，若没有压缩操作，则以.tar作为后缀 -f filename //-f后面接操作使用的文件，用空格隔开，且中间不能有其他参数，推荐放在参数集最后或单独作为参数 //文件作用取决于前面的参数，若为创建(-c),则-f后为创建的文件的名字(路径)，若为(-x/t),则-f后为待解压/查看的打包压缩文件名 //其他辅助选项 -v //详细显示正在处理的文件名 -C Dir //将解压文件放置在 -C 指定的目录下 -p(小写) //保留文件的权限和属性，在备份文件时较有用 -P(大写) //保留原文件的绝对路径，即不会拿掉文件路径开始的根目录，则在还原时会覆盖对应路径上的内容 --exclude=file //排除不进行打包的文件 示例： 123456789101112[root@test opt]# tar -czvf ambaritest.tar.gz ./ambari-web压缩：tar -cvjpf etc.tar.bz2 /etc //-c为创建一个打包文件，相应的-f后面接创建的文件的名称，使用了.tar.bz2后缀，-j标志使用bzip2压缩，最后面为具体的操作对象/etc目录查看：tar -tvjf etc.tar.bz2 //-t为查看操作，则-f对应所查看的文件的名称，文件后缀显示使用bzip2进行压缩，所以加入-j选项，-v会显示详细的权限信息解压tar -xvjf etc.tar.bz2 //-x为解压操作，则-f指定的是解压使用的文件，文件后缀显示使用bzip2进行压缩，所以加入-j选项，即使用bzip2解压 //若只解压指定打包文件中的一个文件，在上述指令的最后加上待解压文件名作为参数即可解压tar -xvf etc.tar -C ~ //将直接打包的.tar文件解压,并放置在用户主目录下 unzipunzip 命令与之前的 tar 指令类似，具有对 zip 文件进行查看、测试和解压的功能 123456789101112131415基本格式：unzip [Options] file[.zip] //不接任何Options时，默认将指定的file文件解压至当前文件夹，可同时接受多个文件参数 常用命令参数： //压缩文件内容查看 -Z //以形如 ls -l 的格式显示目标文件内容，实际原理是命令第一个参数为-Z时，其余参数会被视为 zipinfo 的参数，并产生对应效果 -Z1 //仅显示压缩文件内容的文件名，更多显示可查看 zipinfo 命令的 man 帮助 -l //显示压缩文件中包括时间、占用空间和文件名等信息，内容上较 -Z 更简单 //文件测试 -t //在内存中解压文件并进行文件的完整性校验(CRC校验) //解压缩参数，注意unzip默认即为解压操作 -f //注意与 tar 命令不同，unzip指定 -f 参数时，则将磁盘上已经存在且内容新于对应磁盘文件的压缩内容解压出来 -n //解压缩时不覆盖已存在的文件(而是跳过) -q //安静模式，仅解压缩而不输出详细信息 -d dir //将文件解压至dir指定的文件夹中 可以使用unzip命令对zip文件进行相关的操作。 (1) 查看压缩文件的所有文件名(注意 -Z 选项表示之后所有的参数被视为 zipinfo 的参数并输出相应结果) -&gt; unzip -Z1 file.zip(2) 测试文件的完整性 -&gt; unzip -t file.zip(3) 将文件解压至当前用户的主目录 -&gt; unzip -q file.zip -d ~ 参考Linux下文件的打包、解压缩指令——tar，gzip，bzip2，unzip，rar]]></content>
      <categories>
        <category>Learn</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ambari汉化]]></title>
    <url>%2F2019%2F09%2F09%2FBigData%2FAmbari%E6%B1%89%E5%8C%96%2F</url>
    <content type="text"><![CDATA[准备工作确认已经安装的HDP平台的ambari版本 下载对应的源码包 npm依赖包下载 上传解压源码包1234567891011121314#下载安装unzip工具[root@test zip]# yum install --downloadonly --downloaddir=/opt/apps/zip unzip[root@test zip]# yum -y install unzip-6.0-19.el7.x86_64.rpm#上传源码包，解压[root@test opt]# pwd/opt[root@test opt]# ll总用量 140632-rw-r--r--. 1 root root 144004645 9月 6 07:57 ambari-release-2.6.2.zipdrwxr-xr-x. 11 root root 129 9月 6 07:38 apps[root@test opt]# unzip ambari-release-2.6.2.zipArchive: ambari-release-2.6.2.zip631319b00937a8d04667d93714241d2a0cb17275 安装npm依赖包【用于实时编译js】123456[root@test npm]# pwd/opt/apps/npm[root@test npm]# ll总用量 12736-rw-r--r--. 1 root root 13040896 9月 6 07:33 node-v10.16.3-linux-x64.tar.xz[root@test npm]# tar -xvf node-v10.16.3-linux-x64.tar.xz 编辑环境变量 12345[root@test opt]# cat /etc/profileexport JAVA_HOME=/opt/apps/jdk/jdk1.8.0_144export NODE_HOME=/opt/apps/npm/node-v10.16.3-linux-x64export PATH=$PATH:$JAVA_HOME/bin:$NODE_HOME/bin 测试是否安装成功 12345[root@mm01 img]# source /etc/profile[root@mm01 img]# npm -v6.9.0[root@mm01 img]# node -vv10.16.0 到ambari源码包的web目录 12[root@vm01 ambari-web]# pwd/opt/ambari-release-2.6.2/ambari-web 安装brunch使用一台可以联网环境的linux 安装npmboxnpmbox原名叫npmzip，用于压缩npm安装包。npmbox工具会直接在npm服务器上自动查找并下载好我们指定的安装包，并将安装包压缩成一个后缀为.npmbox的压缩文件。 1234567[root@test opt]# npm install npmbox -gnpm ERR! code CERT_NOT_YET_VALIDnpm ERR! errno CERT_NOT_YET_VALIDnpm ERR! request to https://registry.npmjs.org/npmbox failed, reason: certificate is not yet validnpm ERR! A complete log of this run can be found in:npm ERR! /root/.npm/_logs/2019-09-06T01_44_51_956Z-debug.log 提示报错，这是由于时间与本地时间不一致 123#当前时间：2019-9-10 11:59:00[root@test opt]# date2019年 09月 06日 星期五 09:46:11 CST 安装ntpdate服务，设置时间 12345678[root@test opt]# yum -y install ntpdate[root@test opt]# systemctl start ntpdate#若启动失败，查看是否开启了ntp服务，关闭ntp，启动ntpdate即可[root@test opt]# ntpdate 'ntp3.aliyun.com'10 Sep 12:03:18 ntpdate[17115]: adjust time server 203.107.6.88 offset 0.021964 sec[root@test opt]# date2019年 09月 10日 星期二 12:03:21 CST 安装npmbox，并且下载npmbox包 1234567891011121314[root@test npmbox]# npm install npmbox -gnpm WARN deprecated jade@0.26.3: Jade has been renamed to pug, please install the latest version of pug instead of jadenpm WARN deprecated minimatch@0.3.0: Please update to minimatch 3.0.2 or higher to avoid a RegExp DoS issuenpm WARN deprecated tar.gz@1.0.2: ⚠️ WARNING ⚠️ tar.gz module has been deprecated and your application is vulnerable. Please use tar module instead: https://npmjs.com/tarnpm WARN deprecated to-iso-string@0.0.2: to-iso-string has been deprecated, use @segment/to-iso-string instead.npm WARN deprecated minimatch@3.0.0: Please update to minimatch 3.0.2 or higher to avoid a RegExp DoS issue/opt/apps/npm/node-v10.16.3-linux-x64/bin/npmbox -&gt; /opt/apps/npm/node-v10.16.3-linux-x64/lib/node_modules/npmbox/bin/npmbox/opt/apps/npm/node-v10.16.3-linux-x64/bin/npmunbox -&gt; /opt/apps/npm/node-v10.16.3-linux-x64/lib/node_modules/npmbox/bin/npmunbox+ npmbox@4.2.1added 340 packages from 807 contributors in 108.399s[root@test npmbox]# ll总用量 11012-rw-r--r--. 1 root root 11272311 9月 10 14:12 npmbox.npmbox 下载brunch包 1234[root@test brunch]# npmbox brunch[root@test brunch]# ll总用量 6016-rw-r--r--. 1 root root 6156380 9月 10 14:15 brunch.npmbox 此时，转换到无法联网的正式服务器上，使用npmbox命令解压安装brunch 上传npmbox包和brunch包 123456789101112131415161718192021222324252627282930[root@vm01 npmbox]# pwd/opt/apps/npmbox[root@vm01 npmbox]# lltotal 11012-rw-r--r-- 1 root root 11272311 Sep 10 14:22 npmbox.npmbox#解压[root@vm01 npmbox]# tar --no-same-owner --no-same-permissions -xvzf npmbox.npmbox[root@vm01 npmbox]# ll -atotal 11024drwxr-xr-x 4 root root 70 Sep 10 14:24 .drwxr-xr-x. 16 root root 211 Sep 10 14:20 ..drwxr-xr-x 3 root root 19 Sep 10 14:24 ..npmbox.cachedrwxr-xr-x 253 root root 8192 Sep 10 14:12 .npmbox.cache-rw-r--r-- 1 root root 11272311 Sep 10 14:22 npmbox.npmbox#安装[root@vm01 npmbox]# npm install --global --cache .\.npmbox.cache --cache-min 99999 --shrinkwrap false npmboxnpm ERR! code ENOTFOUNDnpm ERR! errno ENOTFOUNDnpm ERR! network request to https://registry.npmjs.org/npmbox failed, reason: getaddrinfo ENOTFOUND registry.npmjs.org registry.npmjs.org:443npm ERR! network This is a problem related to network connectivity.npm ERR! network In most cases you are behind a proxy or have bad network settings.npm ERR! networknpm ERR! network If you are behind a proxy, please make sure that thenpm ERR! network 'proxy' config is set properly. See: 'npm help config'npm ERR! A complete log of this run can be found in:npm ERR! /opt/apps/npmbox/..npmbox.cache/_logs/2019-09-10T06_29_35_160Z-debug.log 报错 node模块的安装步骤如下：发出npm install命令npm 向 registry 查询模块压缩包的网址下载压缩包，存放在~/.npm目录解压压缩包到当前项目的node_modules目录 所以一个模块安装后，在服务器上实际上是保存了两份，一份在~/.npm，一份在node_modules。但是，运行npm install时，只会检查node_modules目录。所以如果此目录没有文件，npm仍旧会联网进行下载 因此，这里将联网服务器安装模块的node_modules下的所有文件都拷贝到离线服务器的目录中注：这里的npm是tar安装包离线安装不是在线命令安装，所以目录在安装包的解压目录下 123456789101112131415#离线服务器目录before[root@vm01 node_modules]# pwd/opt/node-v10.16.3-linux-x64/lib/node_modules[root@vm01 node_modules]# lltotal 4drwxrwxr-x 10 500 500 4096 Aug 16 02:51 npm#联网服务器目录before[root@test node_modules]# pwd/opt/apps/npm/node-v10.16.3-linux-x64/lib/node_modules[root@test node_modules]# ll总用量 4drwxr-xr-x. 6 root root 226 9月 10 12:47 brunchdrwxrwxr-x. 10 500 500 4096 8月 16 02:51 npmdrwxr-xr-x. 4 root root 246 9月 10 14:09 npmbox 拷贝 12345678910[root@test node_modules]# scp -r brunch/ npmbox/ 192.168.133.3:/opt/node-v10.16.3-linux-x64/lib/node_modules#离线服务器目录after[root@vm01 node_modules]# pwd/opt/node-v10.16.3-linux-x64/lib/node_modules[root@vm01 node_modules]# lltotal 4drwxr-xr-x 6 root root 226 Sep 10 14:38 brunchdrwxrwxr-x 10 500 500 4096 Aug 16 02:51 npmdrwxr-xr-x 4 root root 246 Sep 10 14:38 npmbox 安装npmbox 123456789101112[root@vm01 npmbox]# npm install --global --cache .\.npmbox.cache --optional --cache-min 99999 --shrinkwrap false npmboxnpm ERR! code ENOTFOUNDnpm ERR! errno ENOTFOUNDnpm ERR! network request to https://registry.npmjs.org/npmbox failed, reason: getaddrinfo ENOTFOUND registry.npmjs.org registry.npmjs.org:443npm ERR! network This is a problem related to network connectivity.npm ERR! network In most cases you are behind a proxy or have bad network settings.npm ERR! networknpm ERR! network If you are behind a proxy, please make sure that thenpm ERR! network 'proxy' config is set properly. See: 'npm help config'npm ERR! A complete log of this run can be found in:npm ERR! /opt/apps/npmbox/..npmbox.cache/_logs/2019-09-10T06_59_41_645Z-debug.log 仍然报错 突然意识到在联网服务器上，只是使用npmbox下载包，并没有安装。所以使用npm命令安装之后，将node_models下的文件拷贝但仍然报错失败 后面又意识到，我只是需要一个汉化文件，用来做软连接，只是这个汉化文件需要使用brunch工具，所以决定在联网机器上先编译好文件，然后再复制过去 联网机器上的编译步骤： 下载源码包文件，对里面内容进行更改（汉化，logo，字体等） 打包上传到联网服务器上，解压 进入解压的文件夹的ambari-web下，安装npmnpm install npm安装过程十分缓慢，可以在打包时将node_model文件夹复制到ambari-web中，后续就不用使用npm install，可以直接brunch，前提是机器上已经安装过npm和brunch 安装brunch，npm -g install brunch 使用brunch build命令进行编译 将编译好的整个文件夹拷贝到离线服务器上，建立ambari中的web文件夹的软连接 重启ambari-server 使用brunch编译使用的是联网的服务器 1234567891011121314151617181920212223242526[root@test ambari-web]# pwd/opt/ambari-release-2.6.2/ambari-web[root@test ambari-web]# ll总用量 216drwxr-xr-x. 13 root root 4096 4月 27 2018 app-rw-r--r--. 1 root root 4142 4月 27 2018 config.coffee-rw-r--r--. 1 root root 1662 4月 27 2018 copy-pluggable-stack-resources.sh-rw-r--r--. 1 root root 862 4月 27 2018 gzip-content.cmd-rw-r--r--. 1 root root 2349 4月 27 2018 gzip-content.ps1-rw-r--r--. 1 root root 5437 4月 27 2018 karma.conf.jsdrwxr-xr-x. 387 root root 12288 9月 10 12:35 node_modules-rw-r--r--. 1 root root 1326 4月 27 2018 package.json-rw-r--r--. 1 root root 14106 4月 27 2018 pom.xmldrwxr-xr-x. 9 root root 125 9月 10 12:47 public-rw-r--r--. 1 root root 868 4月 27 2018 set-ambari-version.cmd-rw-r--r--. 1 root root 1144 4月 27 2018 set-ambari-version.ps1-rw-r--r--. 1 root root 1042 4月 27 2018 set-ambari-version.sh-rw-r--r--. 1 root root 875 4月 27 2018 set-default-stack-version.cmd-rw-r--r--. 1 root root 1800 4月 27 2018 set-default-stack-version.ps1-rw-r--r--. 1 root root 1369 4月 27 2018 set-default-stack-version.shdrwxr-xr-x. 12 root root 4096 4月 27 2018 test-rw-r--r--. 1 root root 869 4月 27 2018 toggle-experimental.cmd-rw-r--r--. 1 root root 1138 4月 27 2018 toggle-experimental.ps1-rw-r--r--. 1 root root 1056 4月 27 2018 toggle-experimental.shdrwxr-xr-x. 4 root root 35 4月 27 2018 vendor-rw-r--r--. 1 root root 107873 4月 27 2018 yarn.lock 上传下载的汉化包，解压获取messages.js文件下载地址 123456789101112131415161718192021222324252627[root@test app]# pwd/opt/Ambari-Web-Modify-master/ambari-web/app[root@test app]# ll总用量 332-rw-r--r--. 1 root root 13923 8月 12 2016 app.jsdrwxr-xr-x. 7 root root 87 8月 12 2016 assets-rw-r--r--. 1 root root 3928 8月 12 2016 config.jsdrwxr-xr-x. 5 root root 170 8月 12 2016 controllers-rw-r--r--. 1 root root 9508 8月 12 2016 controllers.jsdrwxr-xr-x. 7 root root 234 8月 12 2016 data-rw-r--r--. 1 root root 829 8月 12 2016 ember.js-rw-r--r--. 1 root root 1900 8月 12 2016 initialize.jsdrwxr-xr-x. 3 root root 4096 8月 12 2016 mappers-rw-r--r--. 1 root root 1960 8月 12 2016 mappers.js-rw-r--r--. 1 root root 223487 8月 12 2016 messages.jsdrwxr-xr-x. 7 root root 81 8月 12 2016 mixins-rw-r--r--. 1 root root 2713 8月 12 2016 mixins.jsdrwxr-xr-x. 6 root root 4096 8月 12 2016 models-rw-r--r--. 1 root root 3013 8月 12 2016 models.js-rw-r--r--. 1 root root 18585 8月 12 2016 router.jsdrwxr-xr-x. 2 root root 4096 8月 12 2016 routesdrwxr-xr-x. 2 root root 211 8月 12 2016 stylesdrwxr-xr-x. 6 root root 160 8月 12 2016 templates-rw-r--r--. 1 root root 1227 8月 12 2016 templates.jsdrwxr-xr-x. 5 root root 4096 8月 12 2016 utilsdrwxr-xr-x. 5 root root 160 8月 12 2016 views-rw-r--r--. 1 root root 17744 8月 12 2016 views.js 备份源码包里面的messages.js，并将汉化包中的messages.js复制到源码包中 123[root@test app]# mv messages.js messages.js_bak[root@test app]# pwd/opt/ambari-release-2.6.2/ambari-web/app 1234567891011121314151617181920212223242526272829303132[root@test app]# pwd/opt/Ambari-Web-Modify-master/ambari-web/app[root@test app]# cp messages.js /opt/ambari-release-2.6.2/ambari-web/app#查看源码包[root@test app]# ll总用量 604-rw-r--r--. 1 root root 14809 4月 27 2018 app.jsdrwxr-xr-x. 7 root root 87 4月 27 2018 assets-rw-r--r--. 1 root root 4505 4月 27 2018 config.jsdrwxr-xr-x. 5 root root 170 4月 27 2018 controllers-rw-r--r--. 1 root root 10117 4月 27 2018 controllers.jsdrwxr-xr-x. 7 root root 234 4月 27 2018 data-rw-r--r--. 1 root root 829 4月 27 2018 ember.js-rw-r--r--. 1 root root 1859 4月 27 2018 initialize.jsdrwxr-xr-x. 3 root root 4096 4月 27 2018 mappers-rw-r--r--. 1 root root 2004 4月 27 2018 mappers.js-rw-r--r--. 1 root root 223487 9月 10 17:09 messages.js-rw-r--r--. 1 root root 256344 4月 27 2018 messages.js_bakdrwxr-xr-x. 7 root root 81 4月 27 2018 mixins-rw-r--r--. 1 root root 3353 4月 27 2018 mixins.jsdrwxr-xr-x. 7 root root 4096 4月 27 2018 models-rw-r--r--. 1 root root 3244 4月 27 2018 models.js-rw-r--r--. 1 root root 29313 4月 27 2018 router.jsdrwxr-xr-x. 2 root root 4096 4月 27 2018 routesdrwxr-xr-x. 2 root root 239 4月 27 2018 stylesdrwxr-xr-x. 6 root root 160 4月 27 2018 templates-rw-r--r--. 1 root root 1283 4月 27 2018 templates.jsdrwxr-xr-x. 6 root root 4096 4月 27 2018 utils-rw-r--r--. 1 root root 1477 4月 27 2018 utils.jsdrwxr-xr-x. 5 root root 160 4月 27 2018 views-rw-r--r--. 1 root root 20029 4月 27 2018 views.js 编译 1234[root@test ambari-web]# pwd/opt/ambari-release-2.6.2/ambari-web[root@test ambari-web]# brunch build10 Sep 17:10:22 - info: compiled 1265 files into 5 files, copied 318 in 6073ms 将编译好的web文件夹整个拷贝到离线服务器中，并建立软连接替代原本的ambari的web文件夹 12345678910111213141516171819202122232425#ambari服务自带的web目录[root@vm01 web]# pwd/lib/ambari-server/web[root@vm01 web]# lltotal 12drwxr-xr-x 23 root root 4096 Aug 19 10:38 datadrwxr-xr-x 2 root root 131 Aug 19 10:38 fontdrwxr-xr-x 3 root root 4096 Aug 19 10:38 img-rw-r--r-- 1 root root 2012 May 30 2018 index.htmldrwxr-xr-x 2 root root 37 Aug 19 10:38 javascriptsdrwxr-xr-x 2 root root 24 Aug 19 10:38 licensesdrwxr-xr-x 2 root root 39 Aug 19 10:38 stylesheetsdrwxr-xr-x 3 root root 80 Aug 19 10:38 test[root@vm01 web]# cd ..[root@vm01 ambari-server]# mv web/ web_bak#建立软连接[root@vm01 ambari-server]# ln -s /opt/ambari-web/public web-rw-r--r-- 1 root root 63777 May 30 2018 validation-api-1.1.0.Final.jar-rw-r--r-- 1 root root 449505 May 30 2018 velocity-1.7.jarlrwxrwxrwx 1 root root 22 Sep 10 17:16 web -&gt; /opt/ambari-web/publicdrwxr-xr-x 9 root root 125 Aug 19 10:38 web_bak-rw-r--r-- 1 root root 15010 May 30 2018 xmlenc-0.52.jar-rw-r--r-- 1 root root 94816 May 30 2018 xz-1.2.jar 重启ambari-server服务。完成 图标ambari-web/app/assets/img/logo.pngambari-web/app/assets/img/logo-white.png 此路径下的logo.png文件为页面浏览器标题上的图标文件logo-white.png为大数据集成平台的图标文件 设置app/assets/img下的路径是原始包，更改里面的图标。后面进行编译，自动在public中有此更改的图标 也可设置public中的img图片，不过重新编译之后会丢失。可用于测试 记录 更改logo，logo_white图片 将hdp字符取消 修改了字体为显示中文 参考基于已部署的Ambari 2.6.2 集群 对Ambari-web 前端源码二次开发实时编译Ambari安装和汉化（转）Ambari2.0.0汉化npm 模块安装机制简介npm离线安装包]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>HDP</tag>
        <tag>Ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql的主从复制和读写分离]]></title>
    <url>%2F2019%2F09%2F05%2FLearn%2Fmysql%E7%9A%84%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E5%92%8C%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[主从复制以及主从复制的作用在实际的生产环境中，对数据库的读和写都在一个数据库服务器中，是不能满足生产需要的。一般通过主从复制的方式来同步数据，再设置读写分离以提高数据库服务器的并发负载能力 具体设置安装mysql节点 123vm02vm03vm04 vm02vm02已经安装mysql，密码为Mysql@123456修改mysql密码为 !QAZ2wsx3edc 1set password for root@localhost = password('!QAZ2wsx3edc'); vm03,vm04安装mysql 123456789101112131415[root@vm03 ~]# yum -y install mysql-community-server-5.7.27-1.el7.x86_64 --nogpgcheck[root@vm03 ~]# systemctl start mysqld[root@vm03 ~]# systemctl enable mysqld[root@vm03 ~]# systemctl status mysqld● mysqld.service - MySQL Server Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2019-09-05 14:46:14 CST; 11s ago Docs: man:mysqld(8) http://dev.mysql.com/doc/refman/en/using-systemd.html Main PID: 80936 (mysqld) CGroup: /system.slice/mysqld.service └─80936 /usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pidSep 05 14:46:10 vm03 systemd[1]: Starting MySQL Server...Sep 05 14:46:14 vm03 systemd[1]: Started MySQL Server. 修改密码，设置远程登录 1234567891011121314#获取临时密码[root@vm03 ~]# grep 'temporary password' /var/log/mysqld.log2019-09-05T06:46:11.198794Z 1 [Note] A temporary password is generated for root@localhost: k=euwnEoC7,u#登录之后修改密码mysql&gt; alter user 'root'@'localhost' identified by '!QAZ2wsx3edc';Query OK, 0 rows affected (0.01 sec)#设置远程登录mysql&gt; grant all privileges on *.* to 'root'@'%' identified by '!QAZ2wsx3edc';Query OK, 0 rows affected, 1 warning (0.01 sec)mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.00 sec) 设置主从复制主节点修改/etc/my.cnf文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@vm02 ~]# cat /etc/my.cnf# For advice on how to change settings please see# http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html[mysqld]## Remove leading # and set to the amount of RAM for the most important data# cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.# innodb_buffer_pool_size = 128M## Remove leading # to turn on a very important data integrity option: logging# changes to the binary log between backups.# log_bin## Remove leading # to set options mainly useful for reporting servers.# The server defaults are faster for transactions and fast SELECTs.# Adjust sizes as needed, experiment to find the optimal values.# join_buffer_size = 128M# sort_buffer_size = 2M# read_rnd_buffer_size = 2Mdatadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sock# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pidserver-id= 11# mysql数据的唯一标示（不能重复）log-slave-updates=true# 允许连级复制(增加)log-bin=master-bin#二进制文件名（修改）#需要备份的数据库binlog_do_db=test#备注：#server-id 服务器唯一标识。#log_bin 启动MySQL二进制日志，即数据同步语句，从数据库会一条一条的执行这些语句。#binlog_do_db 指定记录二进制日志的数据库，即需要复制的数据库名，如果复制多个数据库，重复设置这个选项即可。#binlog_ignore_db 指定不记录二进制日志的数据库，即不需要复制的数据库名，如果有多个数据库，重复设置这个选项即可。#其中需要注意的是，binlog_do_db和binlog_ignore_db为互斥选项，一般只需要一个即可。 设置从服务器的权限 12345678mysql&gt; grant replication slave on *.* to masterbackup@'172.18.4.13' identified by '!QAZ2wsx3edc';Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; grant replication slave on *.* to masterbackup@'172.18.4.14' identified by '!QAZ2wsx3edc';Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.00 sec) 重启 12[root@vm02 ~]# systemctl restart mysqld[root@vm02 ~]# systemctl status mysqld 登录mysql 1234567891011#创建test数据库mysql&gt; create database test;Query OK, 1 row affected (0.01 sec)mysql&gt; show master status;+-------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+-------------------+----------+--------------+------------------+-------------------+| master-bin.000004 | 154 | test | | |+-------------------+----------+--------------+------------------+-------------------+1 row in set (0.00 sec) 从节点修改/etc/my.cnf文件 123456789101112131415161718192021222324252627282930313233343536373839404142[root@vm03 ~]# cat /etc/my.cnf# For advice on how to change settings please see# http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html[mysqld]## Remove leading # and set to the amount of RAM for the most important data# cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.# innodb_buffer_pool_size = 128M## Remove leading # to turn on a very important data integrity option: logging# changes to the binary log between backups.# log_bin## Remove leading # to set options mainly useful for reporting servers.# The server defaults are faster for transactions and fast SELECTs.# Adjust sizes as needed, experiment to find the optimal values.# join_buffer_size = 128M# sort_buffer_size = 2M# read_rnd_buffer_size = 2Mdatadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sock# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pidserver-id=2log-bin=mysql-binrelay-log=slave-relay-binrelay-log-index=slave-relay-bin.index#replicate-do-db=test#备注：#server-id 服务器唯一标识，如果有多个从服务器，每个服务器的server-id不能重复，跟IP一样是唯一标识，如果你没设置server-id或者设置为0，则从服务器不会连接到主服务器。#relay-log 启动MySQL二进制日志，可以用来做数据备份和崩溃恢复，或主服务器挂掉了，将此从服务器作为其他从服务器的主服务器。#replicate-do-db 指定同步的数据库，如果复制多个数据库，重复设置这个选项即可。若在master端不指定binlog-do-db，则在slave端可用replication-do-db来过滤。#replicate-ignore-db 不需要同步的数据库，如果有多个数据库，重复设置这个选项即可。#其中需要注意的是，replicate-do-db和replicate-ignore-db为互斥选项，一般只需要一个即可。 重启mysql 1[root@vm03 ~]# systemctl restart mysqld 连接master主服务器 12345678910#登录进mysql之后#连接master主服务器mysql&gt; change master to master_host='172.18.4.12',master_port=3306,master_user='masterbackup',master_password='!QAZ2wsx3edc',master_log_file='master-bin.000004',master_log_pos=154;Query OK, 0 rows affected, 2 warnings (0.01 sec)#备注：#master_host对应主服务器的IP地址。#master_port对应主服务器的端口。#master_log_file对应show master status显示的File列：master-bin.000001。#master_log_pos对应show master status显示的Position列：154。 启动slave数据同步 123456#启动mysql&gt; start slave;Query OK, 0 rows affected (0.00 sec)#停止mysql&gt; stop slave; 查看slave状态 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263mysql&gt; show slave status\G;*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 172.18.4.12 Master_User: masterbackup Master_Port: 3306 Connect_Retry: 60 Master_Log_File: master-bin.000004 Read_Master_Log_Pos: 154 Relay_Log_File: slave-relay-bin.000002 Relay_Log_Pos: 321 Relay_Master_Log_File: master-bin.000004 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 154 Relay_Log_Space: 528 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: b84fb42f-c004-11e9-9449-005056be31d6 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version:1 row in set (0.00 sec)ERROR:No query specified Slave_IO_Running和Slave_SQL_Running都为yes，则表示同步成功. 测试主服务器建表添加数据 123456789101112131415161718192021mysql&gt; create table tb_test(id int,name varchar(20),age int);Query OK, 0 rows affected (0.01 sec)mysql&gt; insert into tb_test values (19,'aa',40);Query OK, 1 row affected (0.01 sec)mysql&gt; show tables;+----------------+| Tables_in_test |+----------------+| tb_test |+----------------+1 row in set (0.00 sec)mysql&gt; select * from tb_test;+------+------+------+| id | name | age |+------+------+------+| 19 | aa | 40 |+------+------+------+1 row in set (0.00 sec) 查看从服务器 12345678910mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys |+--------------------+4 rows in set (0.00 sec) 再看slave的状态 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263mysql&gt; show slave status\G;*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 172.18.4.12 Master_User: masterbackup Master_Port: 3306 Connect_Retry: 60 Master_Log_File: master-bin.000004 Read_Master_Log_Pos: 615 Relay_Log_File: slave-relay-bin.000002 Relay_Log_Pos: 321 Relay_Master_Log_File: master-bin.000004 Slave_IO_Running: Yes Slave_SQL_Running: No Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 1049 Last_Error: Error 'Unknown database 'test'' on query. Default database: 'test'. Query: 'create table tb_test(id int,name varchar(20),age int)' Skip_Counter: 0 Exec_Master_Log_Pos: 154 Relay_Log_Space: 1540 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: NULLMaster_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 1049 Last_SQL_Error: Error 'Unknown database 'test'' on query. Default database: 'test'. Query: 'create table tb_test(id int,name varchar(20),age int)' Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: b84fb42f-c004-11e9-9449-005056be31d6 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: 190905 15:42:00 Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version:1 row in set (0.00 sec)ERROR:No query specified 主mysql创建了test数据库，同步的是test数据库。所以要求从数据库也自己创建test数据库。创建数据库之后，同步不成功，需要重启mysql服务 重启之后查看同步情况,成功同步 1234567891011121314151617181920mysql&gt; use test;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables;+----------------+| Tables_in_test |+----------------+| tb_test |+----------------+1 row in set (0.00 sec)mysql&gt; select * from tb_test;+------+------+------+| id | name | age |+------+------+------+| 19 | aa | 40 |+------+------+------+1 row in set (0.00 sec) vm04节点做相同配置，修改server-id即可 Atlas实现mysql的读写分离安装atlas 123456[root@vm02 atlas]# lltotal 4848-rw-r--r--. 1 root root 4963681 Sep 5 16:38 Atlas-2.2.1.el6.x86_64.rpm[root@vm02 atlas]# pwd/opt/apps/atlas[root@vm02 atlas]# rpm -ivh Atlas-2.2.1.el6.x86_64.rpm 默认安装位置为/usr/local/mysql-proxy/ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273[root@vm02 atlas]# cd /usr/local/mysql-proxy/[root@vm02 mysql-proxy]# lltotal 4drwxr-xr-x. 2 root root 75 Sep 5 16:38 bindrwxr-xr-x. 2 root root 42 Sep 5 17:00 confdrwxr-xr-x. 3 root root 4096 Sep 5 16:38 libdrwxr-xr-x. 2 root root 6 Dec 17 2014 log[root@vm02 mysql-proxy]# cd conf/[root@vm02 conf]# lltotal 8-rw-r--r--. 1 root root 2860 Sep 5 17:00 test.cnf-rw-r--r--. 1 root root 2810 Dec 17 2014 test.cnf_bak[root@vm02 conf]# vi test.cnf[mysql-proxy]#带#号的为非必需的配置项目#管理接口的用户名admin-username = masterbackup#管理接口的密码admin-password = !QAZ2wsx3edc#Atlas后端连接的MySQL主库的IP和端口，可设置多项，用逗号分隔proxy-backend-addresses = 172.18.4.12:3306#Atlas后端连接的MySQL从库的IP和端口，@后面的数字代表权重，用来作负载均衡，若省略则默认为1，可设置多项，用逗号分隔proxy-read-only-backend-addresses = 172.18.4.13:3306@1,172.18.4.14:3306@1#用户名与其对应的加密过的MySQL密码，密码使用PREFIX/bin目录下的加密程序encrypt加密，下行的user1和user2为示例，将其替换为你的MySQL的用户名和加密密码！pwds = masterbackup:+jKsgB3YAG8=, masterbackup:GS+tr4TPgqc=#设置Atlas的运行方式，设为true时为守护进程方式，设为false时为前台方式，一般开发调试时设为false，线上运行时设为true,true后面不能有空格。daemon = true#设置Atlas的运行方式，设为true时Atlas会启动两个进程，一个为monitor，一个为worker，monitor在worker意外退出后会自动将其重启，设为false时只有worker，没有monitor，一般开发调试时设为false，线上运行时设为true,true后面不能有空格。keepalive = true#工作线程数，对Atlas的性能有很大影响，可根据情况适当设置event-threads = 8#日志级别，分为message、warning、critical、error、debug五个级别log-level = message#日志存放的路径log-path = /usr/local/mysql-proxy/log#SQL日志的开关，可设置为OFF、ON、REALTIME，OFF代表不记录SQL日志，ON代表记录SQL日志，REALTIME代表记录SQL日志且实时写入磁盘，默认为OFFsql-log = OFF#慢日志输出设置。当设置了该参数时，则日志只输出执行时间超过sql-log-slow（单位：ms)的日志记录。不设置该参数则输出全部日志。sql-log-slow = 10#实例名称，用于同一台机器上多个Atlas实例间的区分#instance = test#Atlas监听的工作接口IP和端口proxy-address = 0.0.0.0:1234#Atlas监听的管理接口IP和端口admin-address = 0.0.0.0:2345#分表设置，此例中person为库名，mt为表名，id为分表字段，3为子表数量，可设置多项，以逗号分隔，若不分表则不需要设置该项#tables = person.mt.id.3#默认字符集，设置该项后客户端不再需要执行SET NAMES语句charset = utf8#允许连接Atlas的客户端的IP，可以是精确IP，也可以是IP段，以逗号分隔，若不设置该项则允许所有IP连接，否则只允许列表中的IP连接#client-ips = 127.0.0.1, 192.168.1#Atlas前面挂接的LVS的物理网卡的IP(注意不是虚IP)，若有LVS且设置了client-ips则此项必须设置，否则可以不设置#lvs-ips = 192.168.1.1 启动和关闭atlas 123456#启动[root@vm02 bin]# ./mysql-proxyd test startOK: MySQL-Proxy of test is started#关闭[root@vm02 bin]# ./mysql-proxyd test stop 启动服务之后，查看读写分离状态 1234567891011121314151617181920212223242526272829303132333435363738[root@vm02 bin]# mysql -h127.0.0.1 -P2345 -umasterbackup -pEnter password:mysql&gt; SELECT * FROM help;+----------------------------+---------------------------------------------------------+| command | description |+----------------------------+---------------------------------------------------------+| SELECT * FROM help | shows this help || SELECT * FROM backends | lists the backends and their state || SET OFFLINE $backend_id | offline backend server, $backend_id is backend_ndx's id || SET ONLINE $backend_id | online backend server, ... || ADD MASTER $backend | example: "add master 127.0.0.1:3306", ... || ADD SLAVE $backend | example: "add slave 127.0.0.1:3306", ... || REMOVE BACKEND $backend_id | example: "remove backend 1", ... || SELECT * FROM clients | lists the clients || ADD CLIENT $client | example: "add client 192.168.1.2", ... || REMOVE CLIENT $client | example: "remove client 192.168.1.2", ... || SELECT * FROM pwds | lists the pwds || ADD PWD $pwd | example: "add pwd user:raw_password", ... || ADD ENPWD $pwd | example: "add enpwd user:encrypted_password", ... || REMOVE PWD $pwd | example: "remove pwd user", ... || SAVE CONFIG | save the backends to config file || SELECT VERSION | display the version of Atlas |+----------------------------+---------------------------------------------------------+16 rows in set (0.00 sec)mysql&gt; SELECT * FROM backends;+-------------+------------------+-------+------+| backend_ndx | address | state | type |+-------------+------------------+-------+------+| 1 | 172.18.4.12:3306 | up | rw || 2 | 172.18.4.13:3306 | up | ro || 3 | 172.18.4.14:3306 | up | ro |+-------------+------------------+-------+------+3 rows in set (0.01 sec)#up表示连接成功#rw代表写#ro代表读 参考mysql数据库的主从同步，实现读写分离mysql主从复制与读写分离基于Atlas实现mysql读写分离（1）–主从同步Atlas安装配置Mysql 实现读写分离-Atlas中间件]]></content>
      <categories>
        <category>Learn</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kibana的安装使用]]></title>
    <url>%2F2019%2F09%2F04%2FLearn%2Fkibana%E7%9A%84%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[kibana是什么 kibana的安装上传解压 12345678910111213[root@test ELK]# ll总用量 678400drwxr-xr-x. 11 es es 182 9月 4 00:31 elasticsearch-7.3.1-rw-r--r--. 1 root root 285082863 9月 3 18:57 elasticsearch-7.3.1-linux-x86_64.tar.gzdrwxr-xr-x. 14 root root 271 9月 4 01:03 kibana-7.3.1-linux-x86_64-rw-r--r--. 1 root root 236770201 9月 3 20:30 kibana-7.3.1-linux-x86_64.tar.gzdrwxr-xr-x. 14 root root 281 9月 4 00:13 logstash-7.3.1-rw-r--r--. 1 root root 171784034 9月 3 20:30 logstash-7.3.1.tar.gzdrwxr-xr-x. 9 1001 1001 186 9月 3 23:08 nginx-1.17.3-rw-r--r--. 1 root root 1034586 9月 3 22:50 nginx-1.17.3.tar.gz[root@test ELK]# pwd/opt/apps/ELK[root@test ELK]# tar -zxvf kibana-7.3.1-linux-x86_64.tar.gz 修改配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118[root@test config]# pwd/opt/apps/ELK/kibana-7.3.1-linux-x86_64/config[root@test config]# vi kibana.yml[root@test config]# cat kibana.yml# Kibana is served by a back end server. This setting specifies the port to use.server.port: 5601# Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values.# The default is 'localhost', which usually means remote machines will not be able to connect.# To allow connections from remote users, set this parameter to a non-loopback address.server.host: "192.168.133.11"# Enables you to specify a path to mount Kibana at if you are running behind a proxy.# Use the `server.rewriteBasePath` setting to tell Kibana if it should remove the basePath# from requests it receives, and to prevent a deprecation warning at startup.# This setting cannot end in a slash.#server.basePath: ""# Specifies whether Kibana should rewrite requests that are prefixed with# `server.basePath` or require that they are rewritten by your reverse proxy.# This setting was effectively always `false` before Kibana 6.3 and will# default to `true` starting in Kibana 7.0.#server.rewriteBasePath: false# The maximum payload size in bytes for incoming server requests.#server.maxPayloadBytes: 1048576# The Kibana server's name. This is used for display purposes.#server.name: "your-hostname"# The URLs of the Elasticsearch instances to use for all your queries.elasticsearch.hosts: ["http://192.168.133.11:9200"]# When this setting's value is true Kibana uses the hostname specified in the server.host# setting. When the value of this setting is false, Kibana uses the hostname of the host# that connects to this Kibana instance.#elasticsearch.preserveHost: true# Kibana uses an index in Elasticsearch to store saved searches, visualizations and# dashboards. Kibana creates a new index if the index doesn't already exist.kibana.index: ".kibana"# The default application to load.#kibana.defaultAppId: "home"# If your Elasticsearch is protected with basic authentication, these settings provide# the username and password that the Kibana server uses to perform maintenance on the Kibana# index at startup. Your Kibana users still need to authenticate with Elasticsearch, which# is proxied through the Kibana server.#elasticsearch.username: "kibana"#elasticsearch.password: "pass"# Enables SSL and paths to the PEM-format SSL certificate and SSL key files, respectively.# These settings enable SSL for outgoing requests from the Kibana server to the browser.#server.ssl.enabled: false#server.ssl.certificate: /path/to/your/server.crt#server.ssl.key: /path/to/your/server.key# Optional settings that provide the paths to the PEM-format SSL certificate and key files.# These files validate that your Elasticsearch backend uses the same key files.#elasticsearch.ssl.certificate: /path/to/your/client.crt#elasticsearch.ssl.key: /path/to/your/client.key# Optional setting that enables you to specify a path to the PEM file for the certificate# authority for your Elasticsearch instance.#elasticsearch.ssl.certificateAuthorities: [ "/path/to/your/CA.pem" ]# To disregard the validity of SSL certificates, change this setting's value to 'none'.#elasticsearch.ssl.verificationMode: full# Time in milliseconds to wait for Elasticsearch to respond to pings. Defaults to the value of# the elasticsearch.requestTimeout setting.#elasticsearch.pingTimeout: 1500# Time in milliseconds to wait for responses from the back end or Elasticsearch. This value# must be a positive integer.#elasticsearch.requestTimeout: 30000# List of Kibana client-side headers to send to Elasticsearch. To send *no* client-side# headers, set this value to [] (an empty list).#elasticsearch.requestHeadersWhitelist: [ authorization ]# Header names and values that are sent to Elasticsearch. Any custom headers cannot be overwritten# by client-side headers, regardless of the elasticsearch.requestHeadersWhitelist configuration.#elasticsearch.customHeaders: &#123;&#125;# Time in milliseconds for Elasticsearch to wait for responses from shards. Set to 0 to disable.#elasticsearch.shardTimeout: 30000# Time in milliseconds to wait for Elasticsearch at Kibana startup before retrying.#elasticsearch.startupTimeout: 5000# Logs queries sent to Elasticsearch. Requires logging.verbose set to true.#elasticsearch.logQueries: false# Specifies the path where Kibana creates the process ID file.#pid.file: /var/run/kibana.pid# Enables you specify a file where Kibana stores log output.#logging.dest: stdout# Set the value of this setting to true to suppress all logging output.#logging.silent: false# Set the value of this setting to true to suppress all logging output other than error messages.#logging.quiet: false# Set the value of this setting to true to log all events, including system usage information# and all requests.#logging.verbose: false# Set the interval in milliseconds to sample system and process performance# metrics. Minimum is 100ms. Defaults to 5000.#ops.interval: 5000# Specifies locale to be used for all localizable strings, dates and number formats.# Supported languages are the following: English - en , by default , Chinese - zh-CN .#i18n.locale: "en" 将文件夹的组改为es用户 123[root@test config]# chown -R es:es /opt/apps/ELK/kibana-7.3.1-linux-x86_64[root@test config]# su es[es@test config]$ cd /opt/apps/ELK/kibana-7.3.1-linux-x86_64/bin/ 设置kiban为中文 1234567891011[es@test config]$ pwd/opt/apps/ELK/kibana-7.3.1-linux-x86_64/config[es@test config]$ vi kibana.yml#在配置文件的最后#ops.interval: 5000# Specifies locale to be used for all localizable strings, dates and number formats.# Supported languages are the following: English - en , by default , Chinese - zh-CN .#i18n.locale: "en"i18n.locale: "zh-CN" 启动，并访问页面 1234567[es@test config]$ cd ../bin/[es@test bin]$ ll总用量 12-rwxr-xr-x. 1 es es 639 8月 20 04:49 kibana-rwxr-xr-x. 1 es es 566 8月 20 04:49 kibana-keystore-rwxr-xr-x. 1 es es 617 8月 20 04:49 kibana-plugin[es@test bin]$ ./kibana 在安装好es，logstach，kibana，Nginx之后，可以通过刷新Nginx更新日志，logstach抽取日志到es，kibana又进行展示 参考企业级实战ELK之kibana-7.2.0版本安装实战]]></content>
      <categories>
        <category>Learn</category>
      </categories>
      <tags>
        <tag>Kibana</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch的安装使用]]></title>
    <url>%2F2019%2F09%2F04%2FLearn%2Felasticsearch%E7%9A%84%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[elasticearch是什么 elasticsearch的安装上传解压,修改参数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293[root@test ELK]# tar -zxvf elasticsearch-7.3.1-linux-x86_64.tar.gz[root@test config]# pwd/opt/apps/ELK/elasticsearch-7.3.1/config[root@test config]# vi elasticsearch.yml[root@test config]# cat elasticsearch.yml# ======================== Elasticsearch Configuration =========================## NOTE: Elasticsearch comes with reasonable defaults for most settings.# Before you set out to tweak and tune the configuration, make sure you# understand what are you trying to accomplish and the consequences.## The primary way of configuring a node is via this file. This template lists# the most important settings you may want to configure for a production cluster.## Please consult the documentation for further information on configuration options:# https://www.elastic.co/guide/en/elasticsearch/reference/index.html## ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:#cluster.name: my-es## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:#node.name: es-1## Add custom attributes to the node:##node.attr.rack: r1## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):#path.data: /opt/apps/ELK/elasticsearch-7.3.1/ESdate## Path to log files:#path.logs: /opt/apps/ELK/elasticsearch-7.3.1/ESlogs## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:##bootstrap.memory_lock: true## Make sure that the heap size is set to about half the memory available# on the system and that the owner of the process is allowed to use this# limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):#network.host: 192.168.133.11## Set a custom port for HTTP:#http.port: 9200## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when this node is started:# The default list of hosts is ["127.0.0.1", "[::1]"]##discovery.seed_hosts: ["host1", "host2"]## Bootstrap the cluster using an initial set of master-eligible nodes:##cluster.initial_master_nodes: ["node-1", "node-2"]## For more information, consult the discovery and cluster formation module documentation.## ---------------------------------- Gateway -----------------------------------## Block initial recovery after a full cluster restart until N nodes are started:##gateway.recover_after_nodes: 3## For more information, consult the gateway module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: true 增加es用户，并设置密码 1234567[root@test elasticsearch-7.3.1]# adduser es[root@test elasticsearch-7.3.1]# passwd es更改用户 es 的密码 。新的 密码：重新输入新的 密码：passwd：所有的身份验证令牌已经成功更新。[root@test config]# chown -R es:es /opt/apps/ELK/elasticsearch-7.3.1 启动 1234567891011121314[es@test bin]$ ./elasticsearchfuture versions of Elasticsearch will require Java 11; your Java version from [/opt/apps/jdk/jdk1.8.0_144/jre] does not meet this requirement[2019-09-04T00:47:34,034][INFO ][o.e.e.NodeEnvironment ] [es-1] using [1] data paths, mounts [[/ (rootfs)]], net usable_space [13.3gb], net total_space [16.9gb], types [rootfs][2019-09-04T00:47:34,034][INFO ][o.e.e.NodeEnvironment ] [es-1] heap size [1007.3mb], compressed ordinary object pointers [true][2019-09-04T00:47:34,036][INFO ][o.e.n.Node ] [es-1] node name [es-1], node ID [vS7rK74YQ4WxBd1Mu_bDNQ], cluster name [my-es][2019-09-04T00:47:34,036][INFO ][o.e.n.Node ] [es-1] version[7.3.1], pid[12929], build[default/tar/4749ba6/2019-08-19T20:19:25.651794Z], OS[Linux/3.10.0-957.el7.x86_64/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_144/25.144-b01][2019-09-04T00:47:34,037][INFO ][o.e.n.Node ] [es-1] JVM home [/opt/apps/jdk/jdk1.8.0_144/jre][2019-09-04T00:47:34,037][INFO ][o.e.n.Node ] [es-1] JVM arguments [-Xms1g, -Xmx1g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Djava.io.tmpdir=/tmp/elasticsearch-3737861652460694710, -XX:+HeapDumpOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -XX:+PrintGCDetails, -XX:+PrintGCDateStamps, -XX:+PrintTenuringDistribution, -XX:+PrintGCApplicationStoppedTime, -Xloggc:logs/gc.log, -XX:+UseGCLogFileRotation, -XX:NumberOfGCLogFiles=32, -XX:GCLogFileSize=64m, -Dio.netty.allocator.type=unpooled, -XX:MaxDirectMemorySize=536870912, -Des.path.home=/opt/apps/ELK/elasticsearch-7.3.1, -Des.path.conf=/opt/apps/ELK/elasticsearch-7.3.1/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true][2019-09-04T00:47:36,184][INFO ][o.e.p.PluginsService ] [es-1] loaded module [aggs-matrix-stats][2019-09-04T00:47:36,184][INFO ][o.e.p.PluginsService ] [es-1] loaded module [analysis-common][2019-09-04T00:47:36,185][INFO ][o.e.p.PluginsService ] [es-1] loaded module [data-frame][2019-09-04T00:47:36,185][INFO ][o.e.p.PluginsService ] [es-1] loaded module [flattened][2019-09-04T00:47:36,185][INFO ][o.e.p.PluginsService ] [es-1] loaded module [ingest-common][2019-09-04T00:47:36,185][INFO ][o.e.p.PluginsService ] [es-1] loaded module [ingest-geoip] 报错，信息如下 1234ERROR: [3] bootstrap checks failed[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535][2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144][3]: the default discovery settings are unsuitable for production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configured 安装报错信息，做相应修改 12345678910111213141516171819202122232425262728293031323334353637383940414243[es@test bin]$ exitexit[root@test bin]# vi /etc/sysctl.conf[root@test bin]# cat /etc/sysctl.conf# sysctl settings are defined through files in# /usr/lib/sysctl.d/, /run/sysctl.d/, and /etc/sysctl.d/.## Vendors settings live in /usr/lib/sysctl.d/.# To override a whole file, create a new file with the same in# /etc/sysctl.d/ and put new settings there. To override# only specific settings, add a file with a lexically later# name in /etc/sysctl.d/ and put new settings there.## For more information, see sysctl.conf(5) and sysctl.d(5).vm.max_map_count=655360[root@test bin]# sysctl -pvm.max_map_count = 655360[root@test bin]# vi /etc/security/limits.conf# End of file#增加如下内容* soft nofile 65536* hard nofile 131072* soft nproc 2048* hard nproc 4096[root@test config]# pwd/opt/apps/ELK/elasticsearch-7.3.1/config[root@test config]# vi elasticsearch.yml# --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when this node is started:# The default list of hosts is ["127.0.0.1", "[::1]"]#discovery.seed_hosts: ["test"]## Bootstrap the cluster using an initial set of master-eligible nodes:#cluster.initial_master_nodes: ["es-1"]## For more information, consult the discovery and cluster formation module documentation. 重新启动 12[root@test bin]# su es[es@test bin]$ ./elasticsearch 启动好es，安装好Nginx用于产生日志，然后安装logstach收集日志并解析，然后发送到es中，再安装kibana进行图形化展示]]></content>
      <categories>
        <category>Learn</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logstach的安装使用]]></title>
    <url>%2F2019%2F09%2F04%2FLearn%2Flogstach%E7%9A%84%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[logstach是什么 logstach安装上传解压 123456789101112131415161718192021[root@test logstash-7.3.1]# pwd/opt/apps/ELK/logstash-7.3.1[root@test logstash-7.3.1]# ll总用量 848drwxr-xr-x. 2 root root 4096 9月 3 23:34 bindrwxr-xr-x. 2 root root 142 9月 3 23:34 config-rw-r--r--. 1 root root 2276 8月 20 05:47 CONTRIBUTORSdrwxr-xr-x. 5 root root 84 9月 4 00:13 data-rw-r--r--. 1 root root 4144 8月 20 05:48 Gemfile-rw-r--r--. 1 root root 23109 8月 20 05:49 Gemfile.lockdrwxr-xr-x. 6 root root 84 9月 3 23:34 lib-rw-r--r--. 1 root root 13675 8月 20 05:47 LICENSE.txtdrwxr-xr-x. 2 root root 66 9月 4 00:13 logsdrwxr-xr-x. 4 root root 90 9月 3 23:34 logstash-coredrwxr-xr-x. 3 root root 86 9月 3 23:34 logstash-core-plugin-apidrwxr-xr-x. 4 root root 55 9月 3 23:34 modulesdrwxr-xr-x. 2 root root 31 9月 4 00:12 Myconf-rw-r--r--. 1 root root 808305 8月 20 05:47 NOTICE.TXTdrwxr-xr-x. 3 root root 30 9月 3 23:34 toolsdrwxr-xr-x. 4 root root 33 9月 3 23:34 vendordrwxr-xr-x. 9 root root 193 9月 3 23:34 x-pack 建立抽取日志的conf文件夹 123456789101112131415161718192021222324252627282930313233343536[root@test logstash-7.3.1]# mkdir Myconf[root@test logstash-7.3.1]# cd Myconf/[root@test Myconf]# ll总用量 4-rw-r--r--. 1 root root 553 9月 4 00:12 logstachTest.conf[root@test Myconf]# vi logstachTest.conf#指定抽取的日志的目录input &#123; file &#123; path =&gt; "/usr/local/nginx/logs/*.log" &#125;&#125;#指定过滤标准filter &#123; if [path] =~ "access" &#123; mutate &#123; replace =&gt; &#123; type =&gt; "apache_access" &#125; &#125; grok &#123; match =&gt; &#123; "message" =&gt; "%&#123;COMBINEDAPACHELOG&#125;" &#125; &#125; date &#123; match =&gt; [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ] &#125; &#125; else if [path] =~ "error" &#123; mutate &#123; replace =&gt; &#123; type =&gt; "apache_error" &#125; &#125; &#125; else &#123; mutate &#123; replace =&gt; &#123; type =&gt; "random_logs" &#125; &#125; &#125;&#125;#输出到es中output &#123; elasticsearch &#123; hosts =&gt; ["192.168.133.11:9200"] &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125; 启动 1234567891011[root@test logstash-7.3.1]# bin/logstash -f Myconf/logstachTest.confThread.exclusive is deprecated, use Thread::MutexSending Logstash logs to /opt/apps/ELK/logstash-7.3.1/logs which is now configured via log4j2.properties[2019-09-04T00:13:15,542][INFO ][logstash.setting.writabledirectory] Creating directory &#123;:setting=&gt;"path.queue", :path=&gt;"/opt/apps/ELK/logstash-7.3.1/data/queue"&#125;[2019-09-04T00:13:15,562][INFO ][logstash.setting.writabledirectory] Creating directory &#123;:setting=&gt;"path.dead_letter_queue", :path=&gt;"/opt/apps/ELK/logstash-7.3.1/data/dead_letter_queue"&#125;[2019-09-04T00:13:16,025][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified[2019-09-04T00:13:16,032][INFO ][logstash.runner ] Starting Logstash &#123;"logstash.version"=&gt;"7.3.1"&#125;[2019-09-04T00:13:16,054][INFO ][logstash.agent ] No persistent UUID file found. Generating new UUID &#123;:uuid=&gt;"610b90dd-f455-49dc-80ad-ba886b0c45f6", :path=&gt;"/opt/apps/ELK/logstash-7.3.1/data/uuid"&#125;[2019-09-04T00:13:18,007][INFO ][org.reflections.Reflections] Reflections took 43 ms to scan 1 urls, producing 19 keys and 39 values[2019-09-04T00:13:19,735][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated &#123;:changes=&gt;&#123;:removed=&gt;[], :added=&gt;[http://192.168.133.11:9200/]&#125;&#125;[2019-09-04T00:13:20,010][WARN ][logstash.outputs.elasticsearch] Attempted to resurrect connection to dead ES instance, but got an error. &#123;:url=&gt;"http://192.168.133.11:9200/", :error_type=&gt;LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=&gt;"Elasticsearch Unreachable: [http://192.168.133.11:9200/][Manticore::SocketException] 拒绝连接 (Connection refused)"&#125; 启动需要指定conf文件。类似于flume此处表示es还未启动 参考文档logstach配置conf文件官方示例]]></content>
      <categories>
        <category>Learn</category>
      </categories>
      <tags>
        <tag>Logstach</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ntp错误记录]]></title>
    <url>%2F2019%2F09%2F04%2FBigData%2Fntp%E9%94%99%E8%AF%AF%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[错误描述新安装的Ambari集群经常出现Hbase的两个节点挂掉的情况，查看系统时间，发现是时间不一致，但是重启，或者重装ntp服务，在一天之后，还是会发生两个节点时间不同步的情况 正确节点的情况 1234567891011121314151617181920212223242526272829[root@vm03 ~]# systemctl status ntpd● ntpd.service - Network Time Service Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2019-09-02 10:19:34 CST; 2 days ago Process: 125593 ExecStart=/usr/sbin/ntpd -u ntp:ntp $OPTIONS (code=exited, status=0/SUCCESS) Main PID: 125594 (ntpd) CGroup: /system.slice/ntpd.service └─125594 /usr/sbin/ntpd -u ntp:ntp -gSep 02 10:19:34 vm03 ntpd[125594]: Listen normally on 2 lo 127.0.0.1 UDP 123Sep 02 10:19:34 vm03 ntpd[125594]: Listen normally on 3 ens192 172.18.4.13 UDP 123Sep 02 10:19:34 vm03 ntpd[125594]: Listen normally on 4 lo ::1 UDP 123Sep 02 10:19:34 vm03 ntpd[125594]: Listen normally on 5 ens192 fe80::7d45:f43c:d65:1879 UDP 123Sep 02 10:19:34 vm03 ntpd[125594]: Listening on routing socket on fd #22 for interface updatesSep 02 10:19:34 vm03 ntpd[125594]: 0.0.0.0 c016 06 restartSep 02 10:19:34 vm03 ntpd[125594]: 0.0.0.0 c012 02 freq_set kernel 3.969 PPMSep 02 10:22:47 vm03 ntpd[125594]: 0.0.0.0 c615 05 clock_syncSep 04 11:29:09 vm03 ntpd[125594]: 0.0.0.0 0618 08 no_sys_peerSep 04 13:13:38 vm03 ntpd[125594]: 0.0.0.0 0613 03 spike_detect -3.416143 s[root@vm03 ~]# timedatectl Local time: Wed 2019-09-04 13:19:11 CST Universal time: Wed 2019-09-04 05:19:11 UTC RTC time: Wed 2019-09-04 05:19:11 Time zone: Asia/Shanghai (CST, +0800) NTP enabled: yesNTP synchronized: yes RTC in local TZ: no DST active: n/a 时间错误节点 1234567891011121314151617181920212223242526272829[root@vm02 ~]# systemctl status ntpd● ntpd.service - Network Time Service Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled) Active: active (running) since Wed 2019-09-04 21:11:43 CST; 7h left Process: 95340 ExecStart=/usr/sbin/ntpd -u ntp:ntp $OPTIONS (code=exited, status=0/SUCCESS) Main PID: 95341 (ntpd) CGroup: /system.slice/ntpd.service └─95341 /usr/sbin/ntpd -u ntp:ntp -gSep 04 21:11:43 vm02 ntpd[95341]: Listen normally on 2 lo 127.0.0.1 UDP 123Sep 04 21:11:43 vm02 ntpd[95341]: Listen normally on 3 ens192 172.18.4.12 UDP 123Sep 04 21:11:43 vm02 ntpd[95341]: Listen normally on 4 lo ::1 UDP 123Sep 04 21:11:43 vm02 ntpd[95341]: Listen normally on 5 ens192 fe80::40a0:33d4:f356:1e42 UDP 123Sep 04 21:11:43 vm02 ntpd[95341]: Listening on routing socket on fd #22 for interface updatesSep 04 21:11:43 vm02 ntpd[95341]: 0.0.0.0 c016 06 restartSep 04 21:11:43 vm02 ntpd[95341]: 0.0.0.0 c012 02 freq_set kernel 9.198 PPMSep 04 21:14:58 vm02 ntpd[95341]: 0.0.0.0 c61c 0c clock_step -28752.284818 sSep 04 13:15:46 vm02 ntpd[95341]: 0.0.0.0 c614 04 freq_modeSep 04 13:15:47 vm02 ntpd[95341]: 0.0.0.0 c618 08 no_sys_peer[root@vm02 ~]# timedatectl Local time: Wed 2019-09-04 13:20:25 CST Universal time: Wed 2019-09-04 05:20:25 UTC RTC time: Wed 2019-09-04 13:19:37 Time zone: Asia/Shanghai (CST, +0800) NTP enabled: yesNTP synchronized: no RTC in local TZ: no DST active: n/a 可以观察到，唯一不同的是NTP synchronized: no 尝试重启timedatectl服务 12345678910[root@vm05 360safe]# systemctl restart systemd-timedated.service[root@vm05 360safe]# timedatectl Local time: Wed 2019-09-04 13:21:52 CST Universal time: Wed 2019-09-04 05:21:52 UTC RTC time: Wed 2019-09-04 05:21:52 Time zone: Asia/Shanghai (CST, +0800) NTP enabled: yesNTP synchronized: yes RTC in local TZ: no DST active: n/a 12345678910[root@vm02 ~]# systemctl restart systemd-timedated.service[root@vm02 ~]# timedatectl Local time: Wed 2019-09-04 13:23:29 CST Universal time: Wed 2019-09-04 05:23:29 UTC RTC time: Wed 2019-09-04 13:22:42 Time zone: Asia/Shanghai (CST, +0800) NTP enabled: yesNTP synchronized: no RTC in local TZ: no DST active: n/a 发现错误的两个节点，其中一个恢复到了正常，另一个仍旧如此两台节点唯一的不同是一个装了360，恢复正常的一个将360卸载了，遂尝试卸载360 123456789101112131415161718192021[root@vm02 ~]# cd /opt/360safe/[root@vm02 360safe]# lltotal 2164-r-xr--r--. 1 root root 316048 Sep 13 2017 360entclientdrwx------. 2 root root 24 Aug 12 17:47 backup-r-xr-xr-x. 1 root root 1869992 Sep 13 2017 clearfirewalldrwxr-xr-x. 2 root root 257 Sep 4 13:24 confdrwxr-xr-x. 2 root root 169 Sep 4 13:24 Datadrwxr-xr-x. 3 root root 113 Aug 30 08:36 enginedrwxr-xr-x. 2 root root 4096 Aug 19 13:19 Frameworks-rw-rw-r--. 1 root root 3736 Sep 13 2017 install.shdrwxr-xr-x. 2 root root 39 Aug 12 17:47 lib7zdrwxr-xr-x. 4 root root 4096 Sep 4 00:46 logdrwxr-xr-x. 2 root root 4096 Aug 19 13:19 modularizedrwxr-xr-x. 7 root root 89 Aug 19 13:19 ThirdPath-rw-rw-r--. 1 root root 2154 Sep 13 2017 uninstall.sh-rw-r--r--. 1 root root 1368 Sep 4 01:04 updatecfg.ini[root@vm02 360safe]# bash ./uninstall.shstop service service360saferm init.d/service360saferm /opt/360safe 再重启timedatectl服务 发现仍旧没有成功，然后再等待，等待，就好了]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Ntp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx的学习使用]]></title>
    <url>%2F2019%2F09%2F04%2FLearn%2FNginx%E7%9A%84%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[nginx是什么Nginx是一个轻量级，高性能的web服务器/反向代理服务器/电子邮件代理服务器。具有占用内存小，并发性高的优点。 名词解释代理服务器代理，或者说网络代理，是指一种特殊的网络服务。允许一个网络终端（一般指客户端）和另一个网络终端（一般指服务器端）进行非直连的连接。一些网关，路由器设备就具有这种网络代理功能。一般来说，网络代理有利于网络终端的隐私和安全，防止攻击。 代理服务器，指提供网络代理功能的计算机系统或者其他类型的网络终端。 正向代理和反向代理不涉及代理： 用户A直接向服务器A发送请求，服务器A再将请求的内容返回给用户A 反向代理： 代理的是服务器端客户端向代理服务器发送请求，代理服务器向原始服务器转交请求，返回内容给代理服务器A，再返回给用户 正向代理： 代理的是客户端允许客户端通过代理服务器Z访问任意网络，并且隐藏客户端自身代理服务器需要采取安全措施，确保为经过授权的客户端提供服务 Nginx的安装Nginx是由c开发的，所以需要安装c的编译环境 1[root@test ELK]# yum install gcc-c++ Nginx的http模块需要pcre-devel解析正则表达式 1[root@test ELK]# yum install pcre-devel Nginx使用zlib对httpd的内容进行gzip 1[root@test ELK]# yum install -y zlib zlib-devel OpenSSL是一个强大的安全套接字层密码库，囊括主要的密码算法、常见的密钥、证书封装管理功能及SSL协议。这是为nginx的https服务提供支持的 1[root@test ELK]# yum install -y openssl openssl-devel 解压,安装 123456789101112[root@test ELK]# tar -zxvf nginx-1.17.3.tar.gz[root@test nginx]# cd nginx-1.17.3[root@test nginx]# ./configure &amp;&amp; make &amp;&amp; make install[root@test nginx]# whereis nginxnginx: /usr/local/nginx[root@test nginx]# cd /usr/local/nginx/[root@test nginx]# ll总用量 4drwxr-xr-x. 2 root root 4096 9月 3 23:08 confdrwxr-xr-x. 2 root root 40 9月 3 23:08 htmldrwxr-xr-x. 2 root root 6 9月 3 23:08 logsdrwxr-xr-x. 2 root root 19 9月 3 23:08 sbin 启动 1234567891011[root@test nginx]# cd sbin/[root@test sbin]# ll总用量 3744-rwxr-xr-x. 1 root root 3830200 9月 3 23:08 nginx[root@test sbin]# ./nginxnginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)nginx: [emerg] still could not bind() 发现Nginx默认的80端口已经被使用，修改默认端口和ip 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138[root@test sbin]# cd ..[root@test nginx]# cd conf/[root@test conf]# ll总用量 68-rw-r--r--. 1 root root 1077 9月 3 23:08 fastcgi.conf-rw-r--r--. 1 root root 1077 9月 3 23:08 fastcgi.conf.default-rw-r--r--. 1 root root 1007 9月 3 23:08 fastcgi_params-rw-r--r--. 1 root root 1007 9月 3 23:08 fastcgi_params.default-rw-r--r--. 1 root root 2837 9月 3 23:08 koi-utf-rw-r--r--. 1 root root 2223 9月 3 23:08 koi-win-rw-r--r--. 1 root root 5231 9月 3 23:08 mime.types-rw-r--r--. 1 root root 5231 9月 3 23:08 mime.types.default-rw-r--r--. 1 root root 2656 9月 3 23:08 nginx.conf-rw-r--r--. 1 root root 2656 9月 3 23:08 nginx.conf.default-rw-r--r--. 1 root root 636 9月 3 23:08 scgi_params-rw-r--r--. 1 root root 636 9月 3 23:08 scgi_params.default-rw-r--r--. 1 root root 664 9月 3 23:08 uwsgi_params-rw-r--r--. 1 root root 664 9月 3 23:08 uwsgi_params.default-rw-r--r--. 1 root root 3610 9月 3 23:08 win-utf[root@test conf]# vi nginx.conf[root@test sbin]# cat ../conf/nginx.conf#user nobody;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #log_format main '$remote_addr - $remote_user [$time_local] "$request" ' # '$status $body_bytes_sent "$http_referer" ' # '"$http_user_agent" "$http_x_forwarded_for"'; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 8088; server_name 192.168.133.11; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; root html; index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\.ht &#123; # deny all; #&#125; &#125; # another virtual host using mix of IP-, name-, and port-based configuration # #server &#123; # listen 8000; # listen somename:8080; # server_name somename alias another.alias; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125; # HTTPS server # #server &#123; # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125;&#125; 修改端口为8088，ip为本机ip：192.168.133.11启动 12345[root@test nginx]# cd sbin/[root@test sbin]# ll总用量 3744-rwxr-xr-x. 1 root root 3830200 9月 3 23:08 nginx[root@test sbin]# ./nginx web页面访问： 查看Nginx状态 12345[root@test sbin]# ./nginx -tnginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful[root@test sbin]# pwd/usr/local/nginx/sbin 停止 1./nginx -s stop 如果修改了配置文件，不需要重启Nginx，可以使用命令重新加载配置文件 1./nginx -s reload 参考8分钟带你深入浅出搞懂NginxNginx系列（一）–nginx是什么？Nginx——nginx安装及使用（一）]]></content>
      <categories>
        <category>Learn</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群基本配置]]></title>
    <url>%2F2019%2F09%2F04%2FToWork%2F%E5%85%AC%E5%8F%B8%E9%9B%86%E7%BE%A4%E5%9F%BA%E6%9C%AC%E6%83%85%E5%86%B5%2F</url>
    <content type="text"><![CDATA[Please enter the password to read the blog. Incorrect Password! No content to display! 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]]></content>
      <categories>
        <category>ToWork</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[东北攻略]]></title>
    <url>%2F2019%2F09%2F02%2FEssays%2F%E4%B8%9C%E5%8C%97%E6%94%BB%E7%95%A5%2F</url>
    <content type="text"><![CDATA[起因国庆东北游 经过路线规划大连线 路线和车费北京—-&gt;大连(261~400)推荐：D45车次 &emsp; 北京-大连北 &emsp;时间：6:55-13:08 &emsp;耗时：6h13分 &emsp;车费：261 大连线景点推荐 大连滨海路延绵数公里的海景公路，徒步骑行胜地 金石滩国家旅游度假区汇集娱乐休闲于一体，是个海滨度假好去处 星海公园历史悠久的开放式海滨公园，沙滩浴场，是大连市民休憩消暑的好去处 俄罗斯风情街中国第一条俄式建筑风格街道，可以淘到一些东欧特色小商品 东方威尼斯水城漫步于欧式建筑与水城风光中，感受威尼斯水城般的异域风情。 老虎滩海洋公园欣赏各种极地动物的逗趣表演，乘坐跨海索道俯瞰海滨风光。 棒棰岛大连市区环境最好的海滩，孤立于海上的棒棰岛是著名景观。 大连圣亚海洋世界在全景海底通道中感受海底世界，欣赏精彩的海豚、海象等表演。 老铁山 日落下的山顶灯塔，和泾渭分明的黄渤海交界线，是老铁山的经典去处。 大鹿岛大鹿岛到庄河的王家岛、石城岛，长海的大小长山岛、獐子岛等周边海岛。 如果都去，预计花费时间3-4天，此处按照4天计算 沈阳： 沈阳热门景点如图，想去的欲望并不大。所以从路线上看，推荐从北京直达大连，在大连逛4-5天左右，做慢火车慢慢回北京。 吉林线 吉林线景点推荐 长白山天池位于长白山主峰山巅的圣洁之湖，是长白山的必赏之处。 长白山北坡观长白瀑布、漫步谷底林海，坐倒站车上主峰赏天池美景。 长白山西坡景区赏锦江大峡谷奇景和草甸山花遍野，观视野开阔的天池全貌。 长白瀑布长白山瀑布夏季轰鸣如雷，冬季一挂银川，相当壮观。 鸭绿江 望天鹅风景区 大兴安岭 推荐冬季游玩 伊春 小兴安岭—黑龙江伊春尝试过坐着火车穿越森林吗？去伊春感受一下吧，伊春每个区都有林场，很多是半原始的森林，天然氧吧。其实大兴安岭景点没有小兴安岭集中，去牙克石、博克图附近转转吧。 秋天，白桦和栎树的叶子变黄了，枫树的叶子火一样红，松柏显得更苍翠了。秋风吹来，落叶在林间飞舞。这时候，森林向人们献出了酸甜可口的山葡萄，又香又脆的榛子，鲜嫩的蘑菇和木耳，还有人参等名贵药材。 ——《美丽的小兴安岭》 伊春景点推荐 毛衫沟国家森林公园原生态的自然风光，大峡谷的风景让人惊艳。 汤旺河国家公园 黑龙江汤旺河国家公园地处小兴安岭南麓，植被覆盖率99.8%以上，以红松为主的针阔叶混交林是亚洲最完整、最具代表性的原始红松林生长地，素有“红松故乡”之美誉。 兴安森林公园 金山鹿苑 五营国家森林公园 与刘少奇主席坐过的小火车合影，登上观光塔环望浩瀚的原始林海。 茅山瀑布 小东沟原始森林 参考去哪儿网：https://travel.qunar.com/p-cs300134-dalian-jingdian马蜂窝：http://www.mafengwo.cn/jd/10301/gonglve.html知乎问题：作为一个南方人，我想去东北旅行，但是东北这么大，哪些地方比较好玩呀？知乎问题：国内有哪些冷门但有特色的旅游地点？ st=>start: 北京 e=>end: 北京 op1=>operation: 大连 st->op1->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);st=>start: 北京 e=>end: 北京 op1=>operation: 吉林长白山 st->op1->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-1-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-1-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-1", options);st=>start: 北京 e=>end: 北京 op1=>operation: 哈尔滨 op2=>operation: 伊春 st->op1->op2->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-2-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-2-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-2", options);]]></content>
      <categories>
        <category>Essays</category>
      </categories>
      <tags>
        <tag>旅行</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK学习]]></title>
    <url>%2F2019%2F09%2F02%2FLearn%2FELK%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[ELK是什么ELK是一个日志收集处理分析系统 业务场景 当公司的系统发生了故障，工程师需要登录到各个服务器去查看日志，看看到底是出了什么问题。 但是，如果你的机器很多，并且每台机器上有个组件，那么当寻找发生了什么情况的时候，就会特别不方便。 如果可以将所有服务器的所有组件的不同日志都集中分类进行处理分析，我们只需要在一个终端上查看寻找定位，便可以找到出问题的机器或者组件，这样就很方便。 ELK便是这样一个集中式的日志收集处理分析系统。 日志： app win：ERP,OA,CRM等等 服务器日志 tomcat，Nginx，http 接口 当然除了定位日志报错信息，还可以通过分析日志信息，得到我们想要的数据。ELK是三个工具的集合：elasticsearch + logstach + kibana，三套工具形成了一套实用，易用的监控架构，很多公司利用他来搭建可视化的海量日志分析平台 ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。 Logstash是一个用于管理日志和事件的工具，你可以用它去收集日志、转换日志、解析日志并将他们作为数据提供给其它模块调用，例如搜索、存储等。 Kibana是一个优秀的前端日志展示框架，它可以非常详细的将日志转化为各种图表，为用户提供强大的数据可视化支持。 Beats是ELK的轻量型数据采集器，从不同的服务器和系统收集不同类型的各种数据，发送给logstach或者es 杂记 es除了做一个搜索引擎，还可以用来做存储。但是做存储有一个弊端，就是数据的增量不能波动太大，不能说一次就来大级别，当然大数据量的数据也做不了。需要数据的增量是一个稳定的，小量的。 ELK除了分析日志，还可以将日志换成数据，用来做数据处理，除了说ELK对处理的数据量，可能不能像大数据那样大，还有什么区别呢？数据的稳定性是否可以由kafka来保证？ 某些公司处理数据？日志？使用的如下组件 hbase+impala+solr hbase+Phoenix 某些专门处理日志的公司，如果说了解ELK，应该就知道这几个公司 elastic 日志易 splunk 问题 ERP,OA,CRM等系统是什么？ B/S,C/S是什么？ tomcat是什么，有什么用处？ nginx是什么？有什么用处？ 公司的系统的接口具体是什么？ hbase+impala+solr的集成 hbase+Phoenix的集成使用 Nginx是什么？有什么用处？ kibane和grafana MYSQL的监控工具zabbix ELK做对kafka的监控，需要使用beats 参考 什么是ELK？]]></content>
      <categories>
        <category>Learn</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ntp自定义日志文件]]></title>
    <url>%2F2019%2F09%2F02%2FBigData%2Fntp%E8%87%AA%E5%AE%9A%E4%B9%89%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[起因在搭建好大数据平台之后，集群的两个节点的Hbase regionserver总是莫名其妙的挂掉，检查时间，发现时间和其他机器相差甚远，于是查看ntp服务，发现这两节点的ntp服务已经挂掉，但是找不到相应的日志信息，所以决定给ntpd服务指定相应的日志文件 操作查看ntp.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566[root@vm05 ~]# cat /etc/ntp.conf# For more information about this file, see the man pages# ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).driftfile /var/lib/ntp/drift# Permit time synchronization with our time source, but do not# permit the source to query or modify the service on this system.restrict default nomodify notrap nopeer noquery# Permit all access over the loopback interface. This could# be tightened as well, but to do so would effect some of# the administrative functions.restrict 127.0.0.1restrict ::1# Hosts on local network are less restricted.#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst#设置与主节点ip同步server 172.18.4.11 prefer#设置副设置与本地同步#server 127.127.1.0#fudge 127.127.1.0 stratum 10#broadcast 192.168.1.255 autokey # broadcast server#broadcastclient # broadcast client#broadcast 224.0.1.1 autokey # multicast server#multicastclient 224.0.1.1 # multicast client#manycastserver 239.255.254.254 # manycast server#manycastclient 239.255.254.254 autokey # manycast client# Enable public key cryptography.#cryptoincludefile /etc/ntp/crypto/pw# Key file containing the keys and key identifiers used when operating# with symmetric key cryptography.keys /etc/ntp/keys# Specify the key identifiers which are trusted.#trustedkey 4 8 42# Specify the key identifier to use with the ntpdc utility.#requestkey 8# Specify the key identifier to use with the ntpq utility.#controlkey 8# Enable writing of statistics records.#statistics clockstats cryptostats loopstats peerstats# Disable the monitoring facility to prevent amplification attacks using ntpdc# monlist command when default restrict does not include the noquery flag. See# CVE-2013-5211 for more details.# Note: Monitoring will not be disabled with the limited restriction flag.disable monitor 发现并没有任何有关logs的参数配置 列出所有ntpd的配置文件1234[root@vm05 ~]# rpm -ql ntp|grep conf/etc/ntp.conf/etc/sysconfig/ntpd/usr/share/man/man5/ntp.conf.5.gz 发现除了/etc/ntp.conf,还有/etc/sysconfig/ntpd配置文件同时，查看运行状态下的ntpd服务，可以发现ntpd的启动是使用的/etc/sysconfig/ntpd 查看/etc/sysconfig/ntpd123[root@vm05 ~]# cat /etc/sysconfig/ntpd# Command line options for ntpdOPTIONS="-g" 12345678910111213[root@vm01 ~]# systemctl status ntpd● ntpd.service - Network Time Service Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled) Active: active (running) since Wed 2019-08-28 16:36:55 CST; 4 days ago Main PID: 34251 (ntpd) CGroup: /system.slice/ntpd.service └─34251 /usr/sbin/ntpd -u ntp:ntp -gAug 28 16:36:56 vm01 ntpd[34251]: Listen normally on 2 lo 127.0.0.1 UDP 123Aug 28 16:36:56 vm01 ntpd[34251]: Listen normally on 3 ens192 172.18.4.11 UDP 123Aug 28 16:36:56 vm01 ntpd[34251]: Listen normally on 4 lo ::1 UDP 123Aug 28 16:36:56 vm01 ntpd[34251]: Listening on routing socket on fd #21 for interface updatesAug 28 16:36:56 vm01 ntpd[34251]: 0.0.0.0 c016 06 restar 同时，查看ntpd的参数设置 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061[root@vm05 ~]# ntpd --helpntpd - NTP daemon program - Ver. 4.2.6p5Usage: ntpd [ -&lt;flag&gt; [&lt;val&gt;] | --&lt;name&gt;[&#123;=| &#125;&lt;val&gt;] ]... Flg Arg Option-Name Description -4 no ipv4 Force IPv4 DNS name resolution - prohibits the option 'ipv6' -6 no ipv6 Force IPv6 DNS name resolution - prohibits the option 'ipv4' -a no authreq Require crypto authentication - prohibits the option 'authnoreq' -A no authnoreq Do not require crypto authentication - prohibits the option 'authreq' -b no bcastsync Allow us to sync to broadcast servers -c Str configfile configuration file name -d no debug-level Increase output debug message level - may appear multiple times -D Str set-debug-level Set the output debug message level - may appear multiple times -f Str driftfile frequency drift file name -g no panicgate Allow the first adjustment to be Big - may appear multiple times -i Str jaildir Jail directory -I Str interface Listen on an interface name or address - may appear multiple times -k Str keyfile path to symmetric keys -l Str logfile path to the log file -L no novirtualips Do not listen to virtual interfaces -m no mlock Lock memory -n no nofork Do not fork -N no nice Run at high priority -p Str pidfile path to the PID file - may appear up to 2 times -P Num priority Process priority -q no quit Set the time and quit -r Str propagationdelay Broadcast/propagation delay Str saveconfigquit Save parsed configuration and quit -s Str statsdir Statistics file location -t Str trustedkey Trusted key number - may appear multiple times -u Str user Run as userid (or userid:groupid) - may appear up to 2 times -U Num updateinterval interval in seconds between scans for new or dropped interfaces Str var make ARG an ntp variable (RW) - may appear multiple times Str dvar make ARG an ntp variable (RW|DEF) - may appear multiple times -x no slew Slew up to 600 seconds -! opt version output version information and exit -? no help display extended usage information and exit -! no more-help extended usage information passed thru pagerOptions are specified by doubled hyphens and their name or by a singlehyphen and the flag character.The following option preset mechanisms are supported: - examining environment variables named NTPD_*Please send bug reports to: &lt;http://bugs.ntp.org, bugs@ntp.org&gt;exit 0 发现其中使用了-g的配置参数，并同时发现了-l logfile的参数所以，推出，可以通过添加logfile参数在/etc/sysconfig/ntpd 123[root@vm05 ~]# cat /etc/sysconfig/ntpd# Command line options for ntpdOPTIONS="-g -l /var/log/ntpstats/ntpd.log" 重启服务1234567891011121314[root@vm05 ~]# systemctl restart ntpd[root@vm05 log]# systemctl status ntpd● ntpd.service - Network Time Service Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2019-09-02 18:12:56 CST; 4s ago Process: 86272 ExecStart=/usr/sbin/ntpd -u ntp:ntp $OPTIONS (code=exited, status=0/SUCCESS) Main PID: 86273 (ntpd) CGroup: /system.slice/ntpd.service └─86273 /usr/sbin/ntpd -u ntp:ntp -g -l /var/log/ntpstats/ntpd.logSep 02 18:12:56 vm05 systemd[1]: Starting Network Time Service...Sep 02 18:12:56 vm05 ntpd[86272]: ntpd 4.2.6p5@1.2349-o Fri Apr 13 12:52:27 UTC 2018 (1)Sep 02 18:12:56 vm05 systemd[1]: Started Network Time Service.[root@vm05 log]# 可以看到ntpd的logfile参数生效查看logfile日志 12345678910111213[root@vm05 log]# cat /var/log/ntpstats/ntpd.log 2 Sep 18:12:56 ntpd[86273]: proto: precision = 0.897 usec 2 Sep 18:12:56 ntpd[86273]: 0.0.0.0 c01d 0d kern kernel time sync enabled 2 Sep 18:12:56 ntpd[86273]: ntp_io: estimated max descriptors: 1024, initial socket boundary: 16 2 Sep 18:12:56 ntpd[86273]: Listen and drop on 0 v4wildcard 0.0.0.0 UDP 123 2 Sep 18:12:56 ntpd[86273]: Listen and drop on 1 v6wildcard :: UDP 123 2 Sep 18:12:56 ntpd[86273]: Listen normally on 2 lo 127.0.0.1 UDP 123 2 Sep 18:12:56 ntpd[86273]: Listen normally on 3 ens192 172.18.4.15 UDP 123 2 Sep 18:12:56 ntpd[86273]: Listen normally on 4 lo ::1 UDP 123 2 Sep 18:12:56 ntpd[86273]: Listening on routing socket on fd #21 for interface updates 2 Sep 18:12:56 ntpd[86273]: 0.0.0.0 c016 06 restart 2 Sep 18:12:56 ntpd[86273]: 0.0.0.0 c012 02 freq_set kernel 8.687 PPM 2 Sep 18:12:58 ntpd[86273]: 0.0.0.0 c515 05 clock_sync 其他如果节点时间不正确，且ntp更新缓慢，很久都没办法和主服务器同步时，设置ntp.conf参数，调整只与主服务器同步，不与本地时间同步]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Ntp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ntp使用教程]]></title>
    <url>%2F2019%2F08%2F30%2FBigData%2Fntp%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[ntp简介NTP（Network Time Protocol，网络时间协议）是用来使网络中的各个计算机时间同步的一种协议。它的用途是把计算机的时钟同步到世界协调时UTC，其精度在局域网内可达0.1ms，在互联网上绝大多数的地方其精度可以达到1-50ms。(1s=1000ms) NTP服务器就是利用NTP协议提供时间同步服务的。 ntp原理NTP客户端可以定时自动向NTP服务器发送请求来获取时间，NTP服务器将时间发送给客户端，。 NTP服务器的时间来源有两个 1.网络时间 2.NTP服务器自己的时间 ntp的安装使用1[root@web02 ~]# yum install ntp -y ntp配置集群中主节点：[192.168.133.3] 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# For more information about this file, see the man pages# ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).driftfile /var/lib/ntp/drift# Permit time synchronization with our time source, but do not# permit the source to query or modify the service on this system.restrict default kod nomodify notrap nopeer noqueryrestrict -6 default kod nomodify notrap nopeer noquery# Permit all access over the loopback interface. This could# be tightened as well, but to do so would effect some of# the administrative functions.restrict 127.0.0.1restrict -6 ::1# Hosts on local network are less restricted.# -----------------------------修改部分--------------------------------------------# 允许内网其他机器同步时间restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).# 中国这边最活跃的时间服务器 : http://www.pool.ntp.org/zone/cnserver 210.72.145.44 perfer # 中国国家受时中心server 202.112.10.36 # 1.cn.pool.ntp.orgserver 59.124.196.83 # 0.asia.pool.ntp.org#此处全部注释掉#broadcast 192.168.1.255 autokey # broadcast server#broadcastclient # broadcast client#broadcast 224.0.1.1 autokey # multicast server#multicastclient 224.0.1.1 # multicast client#manycastserver 239.255.254.254 # manycast server#manycastclient 239.255.254.254 autokey # manycast client# allow update time by the upper server# 允许上层时间服务器主动修改本机时间restrict 210.72.145.44 nomodify notrap noqueryrestrict 202.112.10.36 nomodify notrap noqueryrestrict 59.124.196.83 nomodify notrap noquery# Undisciplined Local Clock. This is a fake driver intended for backup# and when no outside source of synchronized time is available.# 外部时间服务器不可用时，以本地时间作为时间服务server 127.127.1.0 # local clockfudge 127.127.1.0 stratum 10# ---------------------------注意修改------------------------------------------# Enable public key cryptography.#cryptoincludefile /etc/ntp/crypto/pw# Key file containing the keys and key identifiers used when operating# with symmetric key cryptography.keys /etc/ntp/keys# Specify the key identifiers which are trusted.#trustedkey 4 8 42# Specify the key identifier to use with the ntpdc utility.#requestkey 8# Specify the key identifier to use with the ntpq utility.#controlkey 8# Enable writing of statistics records.#statistics clockstats cryptostats loopstats peerstats 从节点：[192.168.133.4….] 123456789101112131415161718192021222324252627# the administrative functions.restrict 127.0.0.1restrict ::1# Hosts on local network are less restricted.#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst#设置与主节点ip同步server 192.168.133.3 prefer#设置副设置与本地同步server 127.127.1.0fudge 127.127.1.0 stratum 10#broadcast 192.168.1.255 autokey # broadcast server#broadcastclient # broadcast client#broadcast 224.0.1.1 autokey # multicast server#multicastclient 224.0.1.1 # multicast client#manycastserver 239.255.254.254 # manycast server#manycastclient 239.255.254.254 autokey # manycast client 查看ntp状态12345[root@vm50 /]# ntpq -p remote refid st t when poll reach delay offset jitter============================================================================== 120.25.115.20 10.137.53.7 2 u 5 64 67 40.484 -284697 4.681 203.107.6.88 100.107.25.114 2 u - 64 77 16.579 -284697 7.181 参数解释 位置 标志 含义 remote之前 * 响应的NTP服务器和最精确的服务器 + 响应这个查询请求的NTP服务器 blank(空格) 没有响应的NTP服务器 列表上方 remote 响应这个请求的NTP服务器的名称 refid NTP服务器使用的更高一级服务器的名称 st 正在响应请求的NTP服务器的级别 when 上一次成功请求之后到现在的秒数 poll 本地和远程服务器多少时间进行一次同步，单位秒，在一开始运行NTP的时候这个poll值会比较小，服务器同步的频率大，可以尽快调整到正确的时间范围，之后poll值会逐渐增大，同步的频率也就会相应减小 reach 用来测试能否和服务器连接，是一个八进制值，每成功连接一次它的值就会增加 delay 从本地机发送同步要求到ntp服务器的往返时间 offset 主机通过NTP时钟同步与所同步时间源的时间偏移量，单位为毫秒，offset越接近于0，主机和ntp服务器的时间越接近 jitter 统计了在特定个连续的连接数里offset的分布情况。简单地说这个数值的绝对值越小，主机的时间就越精确 查看123456789[root@vm02 ~]# timedatectl Local time: Fri 2019-08-30 22:34:34 CST Universal time: Fri 2019-08-30 14:34:34 UTC RTC time: Fri 2019-08-30 14:34:34 Time zone: Asia/Shanghai (CST, +0800) NTP enabled: yesNTP synchronized: yes RTC in local TZ: no DST active: n/a]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Ntp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch的学习安装使用]]></title>
    <url>%2F2019%2F08%2F27%2FBigData%2Felasticsearch%E7%9A%84%E5%AD%A6%E4%B9%A0%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[elasticsearch简介Elasticsearch是一个基于Apache Lucene(TM)的开源搜索引擎。无论在开源还是专有领域，Lucene可以被认为是迄今为止最先进、性能最好的、功能最全的搜索引擎库。但是，Lucene只是一个库。想要使用它，你必须使用Java来作为开发语言并将其直接集成到你的应用中，更糟糕的是，Lucene非常复杂，你需要深入了解检索的相关知识来理解它是如何工作的。 Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。不过，Elasticsearch不仅仅是Lucene和全文搜索，我们还能这样去描述它：分布式的实时文件存储，每个字段都被索引并可被搜索分布式的实时分析搜索引擎可以扩展到上百台服务器，处理PB级结构化或非结构化数据 elasticsearch的安装上传，解压 1234567[root@vm03 elasticsearch-7.2.0]# pwd/opt/apps/elasticearch/elasticsearch-7.2.0[root@vm03 elasticearch]# lltotal 329668-rw-r--r--. 1 root root 336647987 Aug 27 15:35 elasticsearch-7.2.0-linux-x86_64.tar.gz-rw-r--r--. 1 root root 928732 Aug 27 15:35 elasticsearch-head-master.zip[root@vm03 elasticearch]# tar -zxvf elasticsearch-7.2.0-linux-x86_64.tar.gz 运行，出错： 1234567891011121314151617[root@vm03 bin]# ./elasticsearchfuture versions of Elasticsearch will require Java 11; your Java version from [/home/jdk1.8.0_181/jre] does not meet this requirement[2019-08-27T15:56:04,870][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [vm03] uncaught exception in thread [main]org.elasticsearch.bootstrap.StartupException: java.lang.RuntimeException: can not run elasticsearch as root at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:163) ~[elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:150) ~[elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[elasticsearch-cli-7.2.0.jar:7.2.0] at org.elasticsearch.cli.Command.main(Command.java:90) ~[elasticsearch-cli-7.2.0.jar:7.2.0] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:115) ~[elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:92) ~[elasticsearch-7.2.0.jar:7.2.0]Caused by: java.lang.RuntimeException: can not run elasticsearch as root at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:105) ~[elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:172) ~[elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:349) ~[elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) ~[elasticsearch-7.2.0.jar:7.2.0] ... 6 more 增加用户es，设置密码为123456,并给与权限 12345678[root@vm03 bin]# adduser es[root@vm03 bin]# passwd esChanging password for user es.New password:BAD PASSWORD: The password contains less than 1 uppercase lettersRetype new password:passwd: all authentication tokens updated successfully.[root@vm03 bin]# chown -R es:es /opt/apps/elasticearch/ 使用es用户启动 12[root@vm03 bin]# su es[es@vm03 bin]$ ./elasticsearch 验证 123456789101112131415161718[root@vm03 ~]# curl 'http://localhost:9200/?pretty'&#123; "name" : "vm03", "cluster_name" : "elasticsearch", "cluster_uuid" : "_emUtf4WQaOremUsXM8FCA", "version" : &#123; "number" : "7.2.0", "build_flavor" : "default", "build_type" : "tar", "build_hash" : "508c38a", "build_date" : "2019-06-20T15:54:18.811730Z", "build_snapshot" : false, "lucene_version" : "8.0.0", "minimum_wire_compatibility_version" : "6.8.0", "minimum_index_compatibility_version" : "6.0.0-beta1" &#125;, "tagline" : "You Know, for Search"&#125; 修改参数主节点 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104[root@vm03 config]# pwd/opt/apps/elasticearch/elasticsearch-7.2.0/config[root@vm03 config]# lltotal 40-rw-rw----. 1 es es 199 Aug 27 15:56 elasticsearch.keystore-rw-rw----. 1 es es 2831 Jun 20 23:50 elasticsearch.yml-rw-rw----. 1 es es 3524 Jun 20 23:50 jvm.options-rw-rw----. 1 es es 17170 Jun 20 23:56 log4j2.properties-rw-rw----. 1 es es 473 Jun 20 23:56 role_mapping.yml-rw-rw----. 1 es es 197 Jun 20 23:56 roles.yml-rw-rw----. 1 es es 0 Jun 20 23:56 users-rw-rw----. 1 es es 0 Jun 20 23:56 users_roles[root@vm03 config]# cat elasticsearch.yml# ======================== Elasticsearch Configuration =========================## NOTE: Elasticsearch comes with reasonable defaults for most settings.# Before you set out to tweak and tune the configuration, make sure you# understand what are you trying to accomplish and the consequences.## The primary way of configuring a node is via this file. This template lists# the most important settings you may want to configure for a production cluster.## Please consult the documentation for further information on configuration options:# https://www.elastic.co/guide/en/elasticsearch/reference/index.html## ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:#cluster.name: my-ES## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:#node.name: ES-1## Add custom attributes to the node:##node.attr.rack: r1## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):#path.data: /u01/ESdata## Path to log files:#path.logs: /u01/ESdata/logs## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:##bootstrap.memory_lock: true## Make sure that the heap size is set to about half the memory available# on the system and that the owner of the process is allowed to use this# limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):#network.host: 172.18.4.13## Set a custom port for HTTP:#http.port: 9093transport.port: 9094## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when this node is started:# The default list of hosts is ["127.0.0.1", "[::1]"]#discovery.seed_hosts: ["vm03:9094", "vm04:9094","vm05:9094"]## Bootstrap the cluster using an initial set of master-eligible nodes:#cluster.initial_master_nodes: ["ES-1", "ES-2","ES-3"]## For more information, consult the discovery and cluster formation module documentation.## ---------------------------------- Gateway -----------------------------------## Block initial recovery after a full cluster restart until N nodes are started:##gateway.recover_after_nodes: 3## For more information, consult the gateway module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: truehttp.cors.enabled: truehttp.cors.allow-origin: "*" 123456789发送到其他节点​```shell[root@vm03 apps]# for i in &#123;4,5&#125;&gt; do&gt; scp -r ./elasticearch/ vm0$i:/opt/apps/&gt; done; 从节点主要修改：node.name: ES-network.host: 172.18.4.1 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677cluster.name: my-ES## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:#node.name: ES-3## Add custom attributes to the node:##node.attr.rack: r1## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):#path.data: /u01/ESdata## Path to log files:#path.logs: /u01/ESdata/logs## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:##bootstrap.memory_lock: true## Make sure that the heap size is set to about half the memory available# on the system and that the owner of the process is allowed to use this# limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):#network.host: 172.18.4.15## Set a custom port for HTTP:##http.port: 9200http.port: 9093transport.port: 9094## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when this node is started:# The default list of hosts is ["127.0.0.1", "[::1]"]##discovery.seed_hosts: ["vm03", "vm04","vm05"]discovery.seed_hosts: ["vm03:9094", "vm04:9094","vm05:9094"]## Bootstrap the cluster using an initial set of master-eligible nodes:#cluster.initial_master_nodes: ["ES-1", "ES-2","ES-3"]## For more information, consult the discovery and cluster formation module documentation.## ---------------------------------- Gateway -----------------------------------## Block initial recovery after a full cluster restart until N nodes are started:##gateway.recover_after_nodes: 3## For more information, consult the gateway module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: truehttp.cors.enabled: truehttp.cors.allow-origin: "*" 从节点创建文件夹，增加es用户 123456789[root@vm04 config]# mkdir /u01/ESdata[root@vm04 config]# mkdir /u01/ESdata/logs[root@vm04 config]# adduser es[root@vm04 config]# passwd esChanging password for user es.New password:BAD PASSWORD: The password contains less than 1 uppercase lettersRetype new password:passwd: all authentication tokens updated successfully. 主从节点分别设置ES储存路径的权限和分组为es,es安装包的组和权限为es 12[root@vm03 bin]# chown -R es:es /u01/ESdata/[root@vm03 bin]# chown -R es:es /opt/apps/elasticearch/ 启动1234[root@vm04 bin]# pwd/opt/apps/elasticearch/elasticsearch-7.2.0/bin[root@vm04 bin]# su es[es@vm04 bin]$ ./elasticsearch 报错： 123[2019-08-27T17:35:24,355][INFO ][o.e.b.BootstrapChecks ] [ES-2] bound or publishing to a non-loopback address, enforcing bootstrap checksERROR: [1] bootstrap checks failed[1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 解决： 123456789101112131415161718[es@vm04 bin]$ exitexit[root@vm04 bin]# vi /etc/sysctl.conf[root@vm04 bin]# cat /etc/sysctl.conf# sysctl settings are defined through files in# /usr/lib/sysctl.d/, /run/sysctl.d/, and /etc/sysctl.d/.## Vendors settings live in /usr/lib/sysctl.d/.# To override a whole file, create a new file with the same in# /etc/sysctl.d/ and put new settings there. To override# only specific settings, add a file with a lexically later# name in /etc/sysctl.d/ and put new settings there.## For more information, see sysctl.conf(5) and sysctl.d(5).vm.max_map_count=655360[root@vm04 bin]# sysctl -pvm.max_map_count = 655360 重新启动]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka的学习安装使用]]></title>
    <url>%2F2019%2F08%2F27%2FBigData%2Fkafka%E7%9A%84%E5%AD%A6%E4%B9%A0%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[kafka简介kafka是什么？是一个消息中间件，消息队列。 kafka名词解释broker：安装了kafka的节点message：消息（要存储的数据）topic：主题partition：分区replica：副本&emsp;主题是由分区的，写入某主题的消息，只存在于一个分区中&emsp;一个节点的不同分区是有leader和follower的&emsp;一个数据的副本也是有leader和follower的&emsp;分区的leader和副本的leader是同一个leader&emsp;但是每一个节点都只能有一个leader&emsp;如果一个节点挂掉，且此节点的副本刚好是leader，则此副本的相应副本，本事follower则就上位成为leader&emsp;kafka有负载均衡机制，当挂掉的节点恢复，则又会恢复到一个节点只有一个leaderleader：某一个分区的管理者follower：其他分区的副本都是从从属这， 用于数据同步，如果leader挂掉，则follower上位producer：生产者，写数据consumer：消费者，读数据 消息队列的应用场景 异步处理：非核心流程异步化，提高系统响应性能 应用解耦：系统不是强耦合，消息接受者可以随意增加，而不需要修改消息发送者的代码。消息发送者的成功不依赖消息接受者（比如有些银行接口不稳定，但调用方并不需要依赖这些接口）不强依赖于非本系统的核心流程，对于非核心流程，可以放到消息队列中让消息消费者去按需消费，而不影响核心主流程最终一致性：最终一致性不是消息队列的必备特性，但确实可以依靠消息队列来做最终一致性的事情。先写消息再操作，确保操作完成后再修改消息状态。定时任务补偿机制实现消息可靠发送接收、业务操作的可靠执行，要注意消息重复与幂等设计所有不保证100%不丢消息的消息队列，理论上无法实现最终一致性。广播：只需要关心消息是否送达了队列，至于谁希望订阅，是下游的事情流量削峰与流控：当上下游系统处理能力存在差距的时候，利用消息队列做一个通用的“漏斗”。在下游有能力处理的时候，再进行分发。日志处理：将消息队列用在日志处理中，比如Kafka的应用，解决大量日志传输的问题消息通讯：消息队列一般都内置了高效的通信机制，因此也可以用于单纯的消息通讯，比如实现点对点消息队列或者聊天室等。 kafka的安装上传kafka压缩包，解压 123456789101112131415[root@vm03 kafka]# pwd/opt/apps/kafka[root@vm03 kafka]# lltotal 55876drwxr-xr-x. 6 root root 89 Jun 20 04:44 kafka_2.12-2.3.0-rw-r--r--. 1 root root 57215197 Aug 27 14:49 kafka_2.12-2.3.0.tgz[root@vm03 kafka]# cd kafka_2.12-2.3.0[root@vm03 kafka_2.12-2.3.0]# lltotal 52drwxr-xr-x. 3 root root 4096 Jun 20 04:44 bindrwxr-xr-x. 2 root root 4096 Jun 20 04:44 configdrwxr-xr-x. 2 root root 4096 Aug 27 14:50 libs-rw-r--r--. 1 root root 32216 Jun 20 04:43 LICENSE-rw-r--r--. 1 root root 337 Jun 20 04:43 NOTICEdrwxr-xr-x. 2 root root 44 Jun 20 04:44 site-docs 新建logs文件夹，作为kafka的储存路径 12345678910[root@vm03 kafka_2.12-2.3.0]# mkdir logs[root@vm03 kafka_2.12-2.3.0]# lltotal 52drwxr-xr-x. 3 root root 4096 Jun 20 04:44 bindrwxr-xr-x. 2 root root 4096 Jun 20 04:44 configdrwxr-xr-x. 2 root root 4096 Aug 27 14:50 libs-rw-r--r--. 1 root root 32216 Jun 20 04:43 LICENSEdrwxr-xr-x. 2 root root 6 Aug 27 14:50 logs-rw-r--r--. 1 root root 337 Jun 20 04:43 NOTICEdrwxr-xr-x. 2 root root 44 Jun 20 04:44 site-docs 修改kafka的参数 12345678910111213141516171819202122232425262728293031323334[root@vm03 kafka_2.12-2.3.0]# cd config/[root@vm03 config]# lltotal 68-rw-r--r--. 1 root root 906 Jun 20 04:43 connect-console-sink.properties-rw-r--r--. 1 root root 909 Jun 20 04:43 connect-console-source.properties-rw-r--r--. 1 root root 5321 Jun 20 04:43 connect-distributed.properties-rw-r--r--. 1 root root 883 Jun 20 04:43 connect-file-sink.properties-rw-r--r--. 1 root root 881 Jun 20 04:43 connect-file-source.properties-rw-r--r--. 1 root root 1552 Jun 20 04:43 connect-log4j.properties-rw-r--r--. 1 root root 2262 Jun 20 04:43 connect-standalone.properties-rw-r--r--. 1 root root 1221 Jun 20 04:43 consumer.properties-rw-r--r--. 1 root root 4727 Jun 20 04:43 log4j.properties-rw-r--r--. 1 root root 1925 Jun 20 04:43 producer.properties-rw-r--r--. 1 root root 6851 Jun 20 04:43 server.properties-rw-r--r--. 1 root root 1032 Jun 20 04:43 tools-log4j.properties-rw-r--r--. 1 root root 1169 Jun 20 04:43 trogdor.conf-rw-r--r--. 1 root root 1023 Jun 20 04:43 zookeeper.properties[root@vm03 config]# vi server.properties############################# Server Basics ############################## The id of the broker. This must be set to a unique integer for each broker.broker.id=0############################# Log Basics ############################## A comma separated list of directories under which to store log fileslog.dirs=/opt/apps/kafka/kafka_2.12-2.3.0/logs############################# Zookeeper ############################## Zookeeper connection string (see zookeeper docs for details).# This is a comma separated host:port pairs, each corresponding to a zk# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".# You can also append an optional chroot string to the urls to specify the# root directory for all kafka znodes.zookeeper.connect=172.18.4.11:2181,172.18.4.12:2181,172.18.4.13:2181,172.18.4.14:2181,172.18.4.15:2181 发送给其他节点 1234[root@vm03 apps]# for i in &#123;4,5&#125;&gt; do&gt; scp -r /opt/apps/kafka/ vm0$i:/opt/apps/&gt; done; 分别修改其他节点的kafka的brokerid 123456789[root@vm04 config]# vi server.properties############################# Server Basics ############################## The id of the broker. This must be set to a unique integer for each broker.broker.id=1[root@vm05 config]# vi server.properties############################# Server Basics ############################## The id of the broker. This must be set to a unique integer for each broker.broker.id=2 命令启动：各节点分别启动 123[root@vm04 kafka_2.12-2.3.0]# pwd/opt/apps/kafka/kafka_2.12-2.3.0[root@vm04 kafka_2.12-2.3.0]# ./bin/kafka-server-start.sh ./config/server.properties]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Azkaban的安装使用]]></title>
    <url>%2F2019%2F08%2F23%2FBigData%2FAzkaban%E7%9A%84%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[下载azkaban 的资源下载链接：https://pan.baidu.com/s/1XGSg2rBU3jJruOnGm7x1VQ 密码：thd3 上传解压12345678910111213[root@vm01 azkaban]# tar -zxvf azkaban-sql-script-2.5.0.tar.gz[root@vm01 azkaban]# unzip -o azkaban-executor-2.5.0.zip[root@vm01 azkaban]# unzip azkaban-web-2.5.0.zip[root@vm01 azkaban]# lltotal 22720drwxr-xr-x 2 root root 4096 Aug 23 17:34 azkaban-2.5.0drwxr-xr-x 7 root root 92 Dec 3 2015 azkaban-executor-2.5.0-rw-r--r-- 1 root root 11150318 Aug 23 17:33 azkaban-executor-2.5.0.zip-rw-r--r-- 1 root root 1928 Aug 23 17:33 azkaban-sql-script-2.5.0.tar.gzdrwxr-xr-x 8 root root 103 Dec 3 2015 azkaban-web-2.5.0-rw-r--r-- 1 root root 12102703 Aug 23 17:33 azkaban-web-2.5.0.zip[root@vm01 azkaban]# pwd/opt/apps/azkaban 存储库操作123456create database azkaban;grant all on azkaban.* to 'root'@'%' identified by 'Mysql_123456';grant all on azkaban.* to 'root'@'vm01' identified by 'Mysql_123456';flush privileges;use azkaban;source /opt/apps/azkaban/azkaban-2.5.0/create-all-sql-2.5.0.sql; 安装ssl安全认证cd到azkaban-web目录 12345678910[root@vm01 azkaban]# cd /opt/apps/azkaban/azkaban-web-2.5.0[root@vm01 azkaban-web-2.5.0]# lltotal 8-rw-r--r-- 1 root root 105 Apr 22 2014 azkaban.versiondrwxr-xr-x 2 root root 112 Dec 3 2015 bindrwxr-xr-x 2 root root 57 Dec 3 2015 confdrwxr-xr-x 2 root root 6 Apr 22 2014 extlibdrwxr-xr-x 2 root root 4096 Dec 3 2015 libdrwxr-xr-x 2 root root 6 Apr 22 2014 pluginsdrwxr-xr-x 6 root root 73 Dec 3 2015 web 执行命令： keytool -keystore keystore -alias jetty -genkey -keyalg RSA 123456789101112131415161718192021222324[root@vm01 azkaban-web-2.5.0]# keytool -keystore keystore -alias jetty -genkey -keyalg RSAEnter keystore password:azkabanRe-enter new password:What is your first and last name? [Unknown]:What is the name of your organizational unit? [Unknown]:What is the name of your organization? [Unknown]:What is the name of your City or Locality? [Unknown]:What is the name of your State or Province? [Unknown]:What is the two-letter country code for this unit? [Unknown]:Is CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=Unknown correct? [no]: YEnter key password for &lt;jetty&gt; (RETURN if same as keystore password):Re-enter new password:Warning:The JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format using "keytool -importkeystore -srckeystore keystore -destkeystore keystore -deststoretype pkcs12". 设置时区为beijing/shanghai如果时区准确就不用设置了 查看： 12[root@vm01 azkaban-web-2.5.0]# date -RFri, 23 Aug 2019 17:52:36 +0800 +0800:东八区-0800:西八区 设置： tzselect]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Azkaban</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[西安项目政府外网大数据平台安装记录]]></title>
    <url>%2F2019%2F08%2F22%2FToWork%2F%E6%94%BF%E5%BA%9C%E5%A4%96%E7%BD%91%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E5%AE%89%E8%A3%85%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[服务器情况简介目前5台服务器处于172段，使用政务云的服务器。无法连接外网，内网是互通的。使用59段的ip对172段的进行映射，然后外部使用工具连接59段，进而控制172段的某台节点。再使用此节点免密登录其他机器进行控制。 特别之处自定义yum源使用4.11作为主服务器，并且自定义为yum源，使集群内网环境yum安装直接连接4.11的自定义yum，这些安装包提前都已下载好了，方法详见：http://www.zzditto.cn/2019/08/19/HDP%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%88%B6%E4%BD%9C%E6%9C%AC%E5%9C%B0yum%E6%BA%90/ 主服务器的local_yum.repo设置为： 123456[root@vm01 yum.repos.d]# cat local_yum.repo[local_yum]name=CentOS-Localbaseurl=file:///var/www/html/local_yumgpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 其他服务器的local_yum.repo设置为： 123456[root@vm02 yum.repos.d]# cat local_yum.repo[local_yum]name=CentOS-Localbaseurl=http://172.18.4.11/local_yum/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 注：web浏览器无法访问172段，但是可以通过59段映射访问。但在集群内部设置地址时，应设置为集群内部可以访问的地址，即172段地址。 自定义HDP源设置同上，对于其他节点。设置地址时，设置为内部访问的地址，即172地址。 mysql原本安装的mysql服务。只安装了client和server，缺少lib和common。卸载重装，结果在删除的时候，删除了mysql.sock。安装之后无法打开。正确步骤：安装mysql服务安装mariadb服务 安装mysql出现，公匙检查不通过 12345678910Total 94 MB/s | 31 MB 00:00:00Retrieving key from file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7The GPG keys listed for the "CentOS-Local" repository are already installed but they are not correct for this package.Check that the correct key URLs are configured for this repository. Failing package is: mysql-community-libs-compat-5.7.27-1.el7.x86_64 GPG Keys are configured as: file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 解决：使用 yum install xxx.rpm –nogpgcheck跳过公钥检查 1[root@vm02 mysql]# yum -y install mysql --nogpgcheck 安装成功后，启动mysql服务： 12[root@vm02 mysql]# systemctl start mysqldFailed to start mysqld.service: Unit not found. 启动失败，需要安装mariadb-server服务 123[root@vm02 mysql]# yum -y install mariadb-server --nogpgcheck[root@vm02 mysql]# systemctl start mysqld 启动成功 mysql执行命令1234567891011121314151617181920212223242526272829303132333435CREATE DATABASE ambari; use ambari; CREATE USER 'ambari'@'%' IDENTIFIED BY 'Mysql_123456'; GRANT ALL PRIVILEGES ON *.* TO 'ambari'@'%'; CREATE USER 'ambari'@'localhost' IDENTIFIED BY 'Mysql_123456';GRANT ALL PRIVILEGES ON *.* TO 'ambari'@'localhost'; CREATE USER 'ambari'@'master' IDENTIFIED BY 'Mysql_123456'; GRANT ALL PRIVILEGES ON *.* TO 'ambari'@'master'; FLUSH PRIVILEGES; source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql show tables; use mysql; select Host,User Password from user where user='ambari'; CREATE DATABASE hive; use hive; CREATE USER 'hive'@'%' IDENTIFIED BY 'Mysql_123456';GRANT ALL PRIVILEGES ON *.* TO 'hive'@'%'; CREATE USER 'hive'@'localhost' IDENTIFIED BY 'Mysql_123456'; GRANT ALL PRIVILEGES ON *.* TO 'hive'@'localhost'; CREATE USER 'hive'@'master' IDENTIFIED BY 'Mysql_123456'; GRANT ALL PRIVILEGES ON *.* TO 'hive'@'master'; FLUSH PRIVILEGES; CREATE DATABASE oozie; use oozie; CREATE USER 'oozie'@'%' IDENTIFIED BY 'Mysql_123456'; GRANT ALL PRIVILEGES ON *.* TO 'oozie'@'%'; CREATE USER 'oozie'@'localhost' IDENTIFIED BY 'Mysql_123456'; GRANT ALL PRIVILEGES ON *.* TO 'oozie'@'localhost';CREATE USER 'oozie'@'master' IDENTIFIED BY 'Mysql_123456'; GRANT ALL PRIVILEGES ON *.* TO 'oozie'@'master';FLUSH PRIVILEGES; ambari-server setup123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[root@vm01 mysql-connector-java-5.1.47]# ambari-server setupUsing python /usr/bin/pythonSetup ambari-serverChecking SELinux...SELinux status is 'disabled'Customize user account for ambari-server daemon [y/n] (n)? yEnter user account for ambari-server daemon (root):Adjusting ambari-server permissions and ownership...Checking firewall status...Checking JDK...[1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8[2] Oracle JDK 1.7 + Java Cryptography Extension (JCE) Policy Files 7[3] Custom JDK==============================================================================Enter choice (1): 3WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts.WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts.Path to JAVA_HOME: /home/jdk1.8.0_181Validating JDK on Ambari Server...done.Checking GPL software agreement...GPL License for LZO: https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.htmlEnable Ambari Server to download and install GPL Licensed LZO packages [y/n] (n)? yCompleting setup...Configuring database...Enter advanced database configuration [y/n] (n)? yConfiguring database...==============================================================================Choose one of the following options:[1] - PostgreSQL (Embedded)[2] - Oracle[3] - MySQL / MariaDB[4] - PostgreSQL[5] - Microsoft SQL Server (Tech Preview)[6] - SQL Anywhere[7] - BDB==============================================================================Enter choice (1): 3Hostname (localhost): vm01Port (3306):Database name (ambari):Username (ambari):Enter Database Password (bigdata):bigdataConfiguring ambari database...Configuring remote database connection properties...WARNING: Before starting Ambari Server, you must run the following DDL against the database to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sqlProceed with configuring remote database connection properties [y/n] (y)? yExtracting system views...ambari-admin-2.6.2.2.1.jar...........Adjusting ambari-server permissions and ownership...Ambari Server 'setup' completed successfully. 启动ambari-server出错查看日志： 12345678910111223 Aug 2019 11:19:23,451 ERROR [main] DBAccessorImpl:119 - Error while creating database accessorjava.sql.SQLException: Access denied for user 'ambari'@'vm01' (using password: YES) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:965) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3978) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3914) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:871) at com.mysql.jdbc.MysqlIO.proceedHandshakeWithPluggableAuthentication(MysqlIO.java:1714) at com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1224) at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2199) at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2230) at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2025) at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:778) 尝试：增加 ‘ambari’@’vm01’ 的远程登录 12345mysql&gt; grant all privileges on *.* to 'ambari'@'vm01' identified by 'Mysql_123456';Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.00 sec) 不能解决。猜测上面远程登录的密码应该为bigdata【即ambari设置时设置的密码，但是使用此密码报不符合此密码策略】所以，修改mysql的登录密码为Mysql_123456。然后设置ambari时密码也为Mysql_123456 12345678mysql&gt; use mysql;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; update user set authentication_string=passworD("Mysql_123456") where user='root';Query OK, 2 rows affected, 1 warning (0.00 sec)Rows matched: 2 Changed: 2 Warnings: 1 重启mysql使用Navicat删除ambari安装时创建的数据库和mysql数据库的user数据 再重新设置ambari-server setup，ambari库设置密码为Mysql_123456 EOF occurred in violation of protocol安装过程中报错信息如下： 1234567INFO 2019-08-23 13:24:08,381 PingPortListener.py:50 - Ping port listener started on port: 8670INFO 2019-08-23 13:24:08,385 main.py:439 - Connecting to Ambari server at https://vm01:8440 (172.18.4.11)INFO 2019-08-23 13:24:08,385 NetUtil.py:70 - Connecting to https://vm01:8440/caERROR 2019-08-23 13:24:08,549 NetUtil.py:96 - EOF occurred in violation of protocol (_ssl.c:618)ERROR 2019-08-23 13:24:08,549 NetUtil.py:97 - SSLError: Failed to connect. Please check openssl library versions.Refer to: https://bugzilla.redhat.com/show_bug.cgi?id=1022468 for more details.WARNING 2019-08-23 13:24:08,550 NetUtil.py:124 - Server at https://vm01:8440 is not reachable, sleeping for 10 seconds... 解决： 12345678910By adding below config in [security] section ofvi /etc/ambari-agent/conf/ambari-agent.ini[security]force_https_protocol=PROTOCOL_TLSv1_2vi /etc/python/cert-verification.cfg[https]verify=disable 参考：https://community.hortonworks.com/questions/218070/ambari-automatic-registration-failed-step-3-confir-1.htmlhttps://my.oschina.net/aubao/blog/1920933 hdfs写spark本地读取hdfs文件时，修改主机名为59段的地址 123val session=SparkSession.builder().master("local[*]").appName(this.getClass.getSimpleName).getOrCreate() val src = session.read.textFile("hdfs://59.218.251.29:8020/test/wd.txt") src.show(20) 但是报错，因为hdfs的3个副本，在不同的服务器，无法直接相连 1219/08/23 16:34:54 WARN DFSClient: Failed to connect to /172.18.4.15:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further informationjava.net.ConnectException: Connection timed out: no further information 所以要想在本地测试读取hdfs文件，需要将其下载，或者在saprk-shell命令行操作 12345scala&gt; sc.textFile("hdfs://172.18.4.11:8020/test/wd.txt")res2: org.apache.spark.rdd.RDD[String] = hdfs://172.18.4.11:8020/test/wd.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:25scala&gt; res2.take(10).toListres8: List[String] = List(html,css,hello,css,spark,html,spark,hahah,hello,nihao,, css,hi,hahah,css,hi,hahah,hello,html,spark,spark,, nihao,html,word,html,word,word,hahah,hi,spark,html,, word,spark,word,hello,nihao,hi,nihao,hahah,css,nihao,, spark,html,nihao,spark,nihao,hi,hi,hahah,html,hahah,, word,hi,spark,hello,hi,spark,hello,hello,html,hi,, word,hi,hello,nihao,css,css,spark,hello,hi,spark,, hi,hi,nihao,hahah,css,hahah,hello,spark,hahah,spark,, hello,hi,css,html,css,word,word,hi,hahah,hello,, hahah,spark,html,spark,hi,hahah,hi,css,nihao,nihao,) 但在集群中可以正常运行，读取测试 namendoe启动报错： 122019-08-23 17:29:08,923 - Retrying after 10 seconds. Reason: Execution of '/usr/hdp/current/hadoop-hdfs-namenode/bin/hdfs dfsadmin -fs hdfs://vm01:8020 -safemode get | grep 'Safe mode is OFF'' returned 1. /etc/profile: line 87: [=]: command not found/etc/profile: line 87: [=]: command not found 解决： 12[hdfs@vm01 liu]$ hdfs dfsadmin -safemode leaveSafe mode is OFF]]></content>
      <categories>
        <category>ToWork</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[yum下载rpm以及依赖包]]></title>
    <url>%2F2019%2F08%2F19%2FBigData%2Fyum%E4%B8%8B%E8%BD%BDrpm%E4%BB%A5%E5%8F%8A%E4%BE%9D%E8%B5%96%E5%8C%85%2F</url>
    <content type="text"><![CDATA[前言yum源的存在使得linux上软件安装变的方便快捷，但在实际的生产环境中，都是内网，离线环境，无法有效的使用yum安装。所以我们需要自定义yum源，而自定的yum源的软件包可以通过以下两种方法下载。需要联网且系统应处于什么都没有安装的阶段，防止某些依赖包不被下载下来。 Downloadonly [yum自身附带的命令]123456#下载wget示例yum install --downloadonly wgetyum install --downloadonly --downloaddir=/opt/apps/wget wget#--downloaddir=/opt/apps/wget:指定下载的目录#如果不指定，默认目录：/var/cache/yum/ 的 rhel-&#123;arch&#125;-channel/packageslocation 目录 Yumdownloader [专门的yum包下载工具，需要安装]123456789101112#安装Yumdownloader插件yum install yum-utils#下载单独的软件包yumdownloader httpd#下载软件包以及依赖包yumdownloader --resolve httpd#指定目录。如果不指定，下载目录在程序执行所在的目录yumdownloader --resolve --destdir=/root/mypackages/ httpdyumdownloader --resolve --destdir /root/mypackages/ httpd linux系统下下载mysql5.7.27安装包及其依赖示例下载 MySQL Yum Repositoryhttps://dev.mysql.com/downloads/repo/yum/注：目前mysql版本为8.0，下载此rpm，可通过编辑repo文件，获取5.7.27版本mysql 上传到linux，并安装123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135root@test mysql]# rpm -ivh mysql80-community-release-el7-3.noarch.rpm#可以看到含有了mysql，但是版本为8.0[root@test mysql]# yum list | grep mysqlmysql80-community-release.noarch el7-3 installedakonadi-mysql.x86_64 1.9.2-4.el7 baseapr-util-mysql.x86_64 1.5.2-6.el7 basedovecot-mysql.x86_64 1:2.2.36-3.el7 basefreeradius-mysql.x86_64 3.0.13-10.el7_6 updateslibdbi-dbd-mysql.x86_64 0.8.3-16.el7 basemysql-community-client.i686 8.0.17-1.el7 mysql80-communitymysql-community-client.x86_64 8.0.17-1.el7 mysql80-communitymysql-community-common.i686 8.0.17-1.el7 mysql80-communitymysql-community-common.x86_64 8.0.17-1.el7 mysql80-communitymysql-community-devel.i686 8.0.17-1.el7 mysql80-communitymysql-community-devel.x86_64 8.0.17-1.el7 mysql80-communitymysql-community-embedded-compat.i686 8.0.17-1.el7 mysql80-communitymysql-community-embedded-compat.x86_64 8.0.17-1.el7 mysql80-communitymysql-community-libs.i686 8.0.17-1.el7 mysql80-communitymysql-community-libs.x86_64 8.0.17-1.el7 mysql80-communitymysql-community-libs-compat.i686 8.0.17-1.el7 mysql80-communitymysql-community-libs-compat.x86_64 8.0.17-1.el7 mysql80-communitymysql-community-release.noarch el7-5 mysql-connectors-communitymysql-community-server.x86_64 8.0.17-1.el7 mysql80-communitymysql-community-test.x86_64 8.0.17-1.el7 mysql80-communitymysql-connector-c++.x86_64 8.0.17-1.el7 mysql-connectors-communitymysql-connector-c++-debuginfo.x86_64 8.0.17-1.el7 mysql-connectors-communitymysql-connector-c++-devel.x86_64 8.0.17-1.el7 mysql-connectors-communitymysql-connector-c++-jdbc.x86_64 8.0.17-1.el7 mysql-connectors-communitymysql-connector-java.noarch 1:5.1.25-3.el7 basemysql-connector-odbc.x86_64 8.0.17-1.el7 mysql-connectors-communitymysql-connector-odbc-debuginfo.x86_64 8.0.17-1.el7 mysql-connectors-communitymysql-connector-odbc-setup.x86_64 8.0.17-1.el7 mysql-connectors-communitymysql-connector-python.noarch 2.0.4-1.el7 mysql-connectors-communitymysql-connector-python.x86_64 8.0.17-1.el7 mysql-connectors-communitymysql-connector-python-cext.x86_64 8.0.17-1.el7 mysql-connectors-communitymysql-connector-python-debuginfo.x86_64 2.1.7-1.el7 mysql-connectors-communitymysql-ref-manual-8.0-en-html-chapter.noarch 1-20190627 mysql80-communitymysql-ref-manual-8.0-en-pdf.noarch 1-20190627 mysql80-communitymysql-router.x86_64 8.0.12-1.el7 mysql-tools-communitymysql-router-community.x86_64 8.0.17-1.el7 mysql-tools-communitymysql-router-debuginfo.x86_64 8.0.12-1.el7 mysql-tools-communitymysql-shell.x86_64 8.0.17-1.el7 mysql-tools-communitymysql-shell-debuginfo.x86_64 8.0.17-1.el7 mysql-tools-communitymysql-utilities.noarch 1.6.5-1.el7 mysql-tools-communitymysql-utilities-extra.noarch 1.5.6-1.el7 mysql-tools-communitymysql-workbench-community.x86_64 8.0.17-1.el7 mysql-tools-communitymysql-workbench-community-debuginfo.x86_64 8.0.17-1.el7 mysql-tools-communitypcp-pmda-mysql.x86_64 4.1.0-5.el7_6 updatesphp-mysql.x86_64 5.4.16-46.el7 basephp-mysqlnd.x86_64 5.4.16-46.el7 baseqt-mysql.i686 1:4.8.7-3.el7_6 updatesqt-mysql.x86_64 1:4.8.7-3.el7_6 updatesqt5-qtbase-mysql.i686 5.9.2-3.el7 baseqt5-qtbase-mysql.x86_64 5.9.2-3.el7 baseredland-mysql.x86_64 1.0.16-6.el7 basersyslog-mysql.x86_64 8.24.0-34.el7 base[root@test mysql]# vi /etc/yum.repos.d/mysql-community.repo# Enable to use MySQL 5.5[mysql55-community]name=MySQL 5.5 Community Serverbaseurl=http://repo.mysql.com/yum/mysql-5.5-community/el/7/$basearch/enabled=0gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql# Enable to use MySQL 5.6[mysql56-community]name=MySQL 5.6 Community Serverbaseurl=http://repo.mysql.com/yum/mysql-5.6-community/el/7/$basearch/enabled=0gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql# Enable to use MySQL 5.7# enabled = 1[mysql57-community]name=MySQL 5.7 Community Serverbaseurl=http://repo.mysql.com/yum/mysql-5.7-community/el/7/$basearch/enabled=1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql# enabled = 0[mysql80-community]name=MySQL 8.0 Community Serverbaseurl=http://repo.mysql.com/yum/mysql-8.0-community/el/7/$basearch/enabled=0gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql[mysql-connectors-community]name=MySQL Connectors Communitybaseurl=http://repo.mysql.com/yum/mysql-connectors-community/el/7/$basearch/enabled=1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql[root@test mysql]# yum clean all[root@test mysql]# yum repolist#可以看到，大部分已经变为5.7版本[root@test mysql]# yum list | grep mysqlmysql80-community-release.noarch el7-3 installedakonadi-mysql.x86_64 1.9.2-4.el7 baseapr-util-mysql.x86_64 1.5.2-6.el7 basedovecot-mysql.x86_64 1:2.2.36-3.el7 basefreeradius-mysql.x86_64 3.0.13-10.el7_6 updateslibdbi-dbd-mysql.x86_64 0.8.3-16.el7 basemysql-community-client.i686 5.7.27-1.el7 mysql57-communitymysql-community-client.x86_64 5.7.27-1.el7 mysql57-communitymysql-community-common.i686 5.7.27-1.el7 mysql57-communitymysql-community-common.x86_64 5.7.27-1.el7 mysql57-communitymysql-community-devel.i686 5.7.27-1.el7 mysql57-communitymysql-community-devel.x86_64 5.7.27-1.el7 mysql57-communitymysql-community-embedded.i686 5.7.27-1.el7 mysql57-communitymysql-community-embedded.x86_64 5.7.27-1.el7 mysql57-communitymysql-community-embedded-compat.i686 5.7.27-1.el7 mysql57-communitymysql-community-embedded-compat.x86_64 5.7.27-1.el7 mysql57-communitymysql-community-embedded-devel.i686 5.7.27-1.el7 mysql57-communitymysql-community-embedded-devel.x86_64 5.7.27-1.el7 mysql57-communitymysql-community-libs.i686 5.7.27-1.el7 mysql57-communitymysql-community-libs.x86_64 5.7.27-1.el7 mysql57-communitymysql-community-libs-compat.i686 5.7.27-1.el7 mysql57-communitymysql-community-libs-compat.x86_64 5.7.27-1.el7 mysql57-communitymysql-community-release.noarch el7-5 mysql-connectors-communitymysql-community-server.x86_64 5.7.27-1.el7 mysql57-communitymysql-community-test.x86_64 5.7.27-1.el7 mysql57-communitymysql-connector-c++.x86_64 8.0.17-1.el7 mysql-connectors-communitymysql-connector-c++-debuginfo.x86_64 8.0.17-1.el7 mysql-connectors-community# 下载其安装包和依赖包[root@test mysql]# yum install --downloadonly --downloaddir=/opt/apps/mysql mysql-community-server.x86_64]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Yum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDP离线安装制作本地yum源]]></title>
    <url>%2F2019%2F08%2F19%2FBigData%2FHDP%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%88%B6%E4%BD%9C%E6%9C%AC%E5%9C%B0yum%E6%BA%90%2F</url>
    <content type="text"><![CDATA[前言YUM 源可以简化在 Linux 上安装软件的过程，但是生产环境通常无法上网，不能连接外网的 YUM 源，所以就无法使用 YUM 命令安装软件了。为了在内网中也可以使用 YUM 安装相关的软件，就需要配置本地 YUM 源了。 yum源原理单独使用：将一系列需要用到的rpm包提前下载保存到某个文件夹，使用createrepo创建新的repo，创建自定义repo文件，添加路径，则就可以直接在这个路径下使用里面的安装包安装软件 集群使用：使用http工具，是的内网中的各个机器能够获取到自定义yum的机器的yum仓库地址，并自定义repo文件，添加路径，即可使用自定义yum源的安装包安装软件 制作步骤安装httpd服务： 12345678910111213141516171819202122232425262728293031323334353637383940[root@test httpd]# pwd/opt/apps/httpd[root@test httpd]# ll总用量 3100-rw-r--r--. 1 root root 105728 8月 19 15:17 apr-1.4.8-3.el7_4.1.x86_64.rpm-rw-r--r--. 1 root root 94132 8月 19 15:17 apr-util-1.5.2-6.el7.x86_64.rpm-rw-r--r--. 1 root root 2844140 8月 19 15:17 httpd-2.4.6-89.el7.centos.1.x86_64.rpm-rw-r--r--. 1 root root 92776 8月 19 15:17 httpd-tools-2.4.6-89.el7.centos.1.x86_64.rpm-rw-r--r--. 1 root root 31264 8月 19 15:17 mailcap-2.1.41-2.el7.noarch.rpm[root@test httpd]# rpm -ivh apr-1.4.8-3.el7_4.1.x86_64.rpm警告：apr-1.4.8-3.el7_4.1.x86_64.rpm: 头V3 RSA/SHA256 Signature, 密钥 ID f4a80eb5: NOKEY准备中... ################################# [100%]正在升级/安装... 1:apr-1.4.8-3.el7_4.1 ################################# [100%][root@test httpd]# rpm -ivh apr-util-1.5.2-6.el7.x86_64.rpm警告：apr-util-1.5.2-6.el7.x86_64.rpm: 头V3 RSA/SHA256 Signature, 密钥 ID f4a80eb5: NOKEY准备中... ################################# [100%]正在升级/安装... 1:apr-util-1.5.2-6.el7 ################################# [100%][root@test httpd]# rpm -ivh mailcap-2.1.41-2.el7.noarch.rpm警告：mailcap-2.1.41-2.el7.noarch.rpm: 头V3 RSA/SHA256 Signature, 密钥 ID f4a80eb5: NOKEY准备中... ################################# [100%]正在升级/安装... 1:mailcap-2.1.41-2.el7 ################################# [100%][root@test httpd]# rpm -ivh httpd-tools-2.4.6-89.el7.centos.1.x86_64.rpm警告：httpd-tools-2.4.6-89.el7.centos.1.x86_64.rpm: 头V3 RSA/SHA256 Signature, 密钥 ID f4a80eb5: NOKEY准备中... ################################# [100%]正在升级/安装... 1:httpd-tools-2.4.6-89.el7.centos.1################################# [100%][root@test httpd]# rpm -ivh httpd-2.4.6-89.el7.centos.1.x86_64.rpm警告：httpd-2.4.6-89.el7.centos.1.x86_64.rpm: 头V3 RSA/SHA256 Signature, 密钥 ID f4a80eb5: NOKEY准备中... ################################# [100%]正在升级/安装... 1:httpd-2.4.6-89.el7.centos.1 ################################# [100%][root@test httpd]# systemctl start httpd[root@test httpd]# systemctl enable httpdCreated symlink from /etc/systemd/system/multi-user.target.wants/httpd.service to /usr/lib/systemd/system/httpd.service. 在httpd服务目录下创建local_yum,并网页登录监测 1234[root@test local_yum]# pwd/var/www/html/[root@test html]# mkdir local_yum 修改yum源为网易云： 12345[root@test postgresql]# yum -y install wget[root@test postgresql]# mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo_bak[root@test postgresql]# wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.163.com/.help/CentOS7-Base-163.repo[root@test postgresql]# yum clean all[root@test postgresql]# yum makecache 下载HDP安装所需的rpm及依赖包: 123456789101112[root@test local_yum]# yum install --downloadonly --downloaddir=/var/www/html/local_yum/ntp ntp[root@test postgresql]# yum install --downloadonly --downloaddir=/var/www/html/local_yum/postgresql postgresql[root@test postgresql]# yum install --downloadonly --downloaddir=/var/www/html/local_yum/libmpc.so.3 libmpc[root@test postgresql]# yum install --downloadonly --downloaddir=/var/www/html/local_yum/libmpfr.so.4 mpfr[root@test postgresql]# yum install --downloadonly --downloaddir=/var/www/html/local_yum/redhat-lsb redhat-lsb[root@test postgresql]# yum install --downloadonly --downloaddir=/var/www/html/local_yum/createrepo createrepo[root@test postgresql]# yum install --downloadonly --downloaddir=/var/www/html/local_yum/gcc gcc[root@test postgresql]# yum install --downloadonly --downloaddir=/var/www/html/local_yum/python-devel python-devel[root@test libtirpc-devel]# yum install --downloadonly --downloaddir=/var/www/html/local_yum/libtirpc-devel libtirpc-devel[root@test libtirpc-devel]# yum install --downloadonly --downloaddir=/var/www/html/local_yum/redhat-lsb-core redhat-lsb-core[root@test libtirpc-devel]# yum install --downloadonly --downloaddir=/var/www/html/local_yum/mariadb mariadb[root@test local_yum]# yum install --downloadonly --downloaddir=/var/www/html/local_yum/yum-plugin-priorities yum-plugin-priorities 安装createrepo： 1234567891011121314151617181920212223[root@test local_yum]# cd createrepo/[root@test createrepo]# ll总用量 460-rw-r--r--. 1 root root 95840 8月 10 2017 createrepo-0.9.9-28.el7.noarch.rpm-rw-r--r--. 1 root root 83984 7月 4 2014 deltarpm-3.6-3.el7.x86_64.rpm-rw-r--r--. 1 root root 252528 6月 24 2016 libxml2-python-2.9.1-6.el7_2.3.x86_64.rpm-rw-r--r--. 1 root root 32084 7月 4 2014 python-deltarpm-3.6-3.el7.x86_64.rpm[root@test createrepo]# rpm -ivh deltarpm-3.6-3.el7.x86_64.rpm准备中... ################################# [100%]正在升级/安装... 1:deltarpm-3.6-3.el7 ################################# [100%][root@test createrepo]# rpm -ivh python-deltarpm-3.6-3.el7.x86_64.rpm准备中... ################################# [100%]正在升级/安装... 1:python-deltarpm-3.6-3.el7 ################################# [100%][root@test createrepo]# rpm -ivh libxml2-python-2.9.1-6.el7_2.3.x86_64.rpm准备中... ################################# [100%]正在升级/安装... 1:libxml2-python-2.9.1-6.el7_2.3 ################################# [100%][root@test createrepo]# rpm -ivh createrepo-0.9.9-28.el7.noarch.rpm准备中... ################################# [100%]正在升级/安装... 1:createrepo-0.9.9-28.el7 ################################# [100%] 制作yum本地源： 12345678910111213141516171819202122[root@test local_yum]# createrepo /var/www/html/local_yum/[root@test local_yum]# cd /etc/yum.repos.d/[root@test yum.repos.d]# ll总用量 36-rw-r--r--. 1 root root 1572 12月 1 2016 CentOS-Base.repo-rw-r--r--. 1 root root 1664 11月 23 2018 CentOS-Base.repo_bak-rw-r--r--. 1 root root 1309 11月 23 2018 CentOS-CR.repo-rw-r--r--. 1 root root 649 11月 23 2018 CentOS-Debuginfo.repo-rw-r--r--. 1 root root 314 11月 23 2018 CentOS-fasttrack.repo-rw-r--r--. 1 root root 630 11月 23 2018 CentOS-Media.repo-rw-r--r--. 1 root root 1331 11月 23 2018 CentOS-Sources.repo-rw-r--r--. 1 root root 5701 11月 23 2018 CentOS-Vault.repo[root@test yum.repos.d]# vi local_yum.repo[root@test yum.repos.d]# cat local_yum.repo[local_yum]name=CentOS-Localbaseurl=file:///var/www/html/local_yumgpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7[root@test local_yum]# yum clean all[root@test local_yum]# yum repolist#local_yum的status应该不为0 其他节点的repo设置： 12345678910[root@test yum.repos.d]# vi local_yum.repo[root@test yum.repos.d]# cat local_yum.repo[local_yum]name=CentOS-Localbaseurl=http://192.168.133.11/local_yum/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7[root@test local_yum]# yum clean all[root@test local_yum]# yum repolist#local_yum的status应该不为0 http服务目录文件如下：]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>HDP</tag>
        <tag>Yum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[西安项目服务器说明]]></title>
    <url>%2F2019%2F08%2F16%2FToWork%2F%E8%A5%BF%E5%AE%89%E9%A1%B9%E7%9B%AE%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[大数据平台服务器和数据服务器数量：8台系统：centos7语言：English系统环境: 虚拟化主机 【注意：默认安装的话，为最小化安装，最小化安装缺少mysql安装的某些组件环境】系统分区情况：&emsp;root：100G&emsp;swap: 同内存大小&emsp;home：40G&emsp;数据盘：剩余容量 web页面访问ip及端口开放 【针对5台大数据平台服务器】映射地址设置（假设）：| 服务器地址 | 映射地址 || — | — || 172.18.4.11 | 59.218.251.11 || 172.18.4.12 | 59.218.251.12 || 172.18.4.13 | || 172.18.4.14 | || 172.18.4.15 | | httpd 服务器地址 映射地址 172.18.4.11：80 59.218.251.11：80 mysql 服务器ip: port 映射 172.18.4.11：3306 59.218.251.11：3306 172.18.4.12：3306 59.218.251.12：3306 Ambari 服务器ip: port 映射 172.18.4.11:8080 59.218.251.11：8080 hdfs 服务器ip: port 映射 172.18.4.11：50070 59.218.251.11：50070 服务器ip: port 映射 172.18.4.11：8020 59.218.251.11：8020 服务器ip: port 映射 172.18.4.11：50075 59.218.251.11：50075 172.18.4.12：50075 59.218.251.12：50075 172.18.4.13：50075 172.18.4.13：50075 172.18.4.15：50075 yarn 服务器ip: port 映射 172.18.4.11：8088 59.218.251.11：8088 172.18.4.12：8088 59.218.251.12：8088 mapreduce 服务器ip: port 映射 172.18.4.11：19888 59.218.251.11：19888 172.18.4.12：19888 59.218.251.12：19888 grafara 服务器ip: port 映射 172.18.4.11：3000 59.218.251.11：3000 hbase 服务器ip: port 映射 172.18.4.11：16010 59.218.251.11：16010 服务器ip: port 映射 172.18.4.11：16020 59.218.251.11：16020 172.18.4.12：16020 59.218.251.12：16020 172.18.4.13：16020 172.18.4.14：16020 172.18.4.15：16020 服务器ip: port 映射 172.18.4.11：16030 59.218.251.11：16030 172.18.4.12：16030 59.218.251.12：16030 172.18.4.13：16030 172.18.4.14：16030 172.18.4.15：16030 spark 服务器ip: port 映射 172.18.4.11：18081 59.218.251.11：18081 服务器ip: port 映射 172.18.4.11：8042 59.218.251.11：8042 172.18.4.12：8042 59.218.251.12：8042 172.18.4.13：8042 172.18.4.14：8042 172.18.4.15：8042 服务器ip: port 映射 172.18.4.11：8088 59.218.251.11：8088 172.18.4.12：8088 59.218.251.12：8088 172.18.4.13：8088 172.18.4.14：8088 172.18.4.15：8088 solo 服务器ip: port 映射 172.18.4.11：8886 59.218.251.11：8886 172.18.4.12：8989 59.218.251.12：8989 zeppelin 服务器ip: port 映射 172.18.4.11：9995 59.218.251.11：9995 172.18.4.11：9060 59.218.251.11：9060 ranger 服务器ip: port 映射 172.18.4.11：6080 59.218.251.11：6080 redis 服务器ip: port 映射 172.18.4.11：6379 59.218.251.11：6379 172.18.4.12：6379 59.218.251.12：6379 172.18.4.13：6379 172.18.4.14：6379 172.18.4.15：6379 azkaban 服务器ip: port 映射 172.18.4.11：8443 59.218.251.11：8443 172.18.4.12：8443 59.218.251.12：8443 其他 服务器ip: port 映射 172.18.4.12：9100 59.218.251.12：9100 服务器ip: port 映射 172.18.4.12：5601 59.218.251.12：5601 3台数据服务器端口：3台服务器：80 ~ 100 ， 1234 服务器ip: port 映射 172.18.4.16：80~100,1234 172.18.4.17：80~100,1234 172.18.4.18：80~100,1234 172.18.4.253：80~100,1234 59.218.251.20：80~100,1234 注：4.20的ip目前没有服务器使用，但后续数据服务会用到，需要确保此ip无人使用，且与59段的20映射成功 数据服务器：mysql 用户：root 密码：Mysql@123456]]></content>
      <categories>
        <category>ToWork</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux添加用户]]></title>
    <url>%2F2019%2F08%2F16%2FBigData%2FLinux%E6%B7%BB%E5%8A%A0%E7%94%A8%E6%88%B7%2F</url>
    <content type="text"><![CDATA[添加用户123456789101112131415#3中方式添加用户[root@vm01 ~]# useradd -m ztgxHDP[root@vm01 ~]# adduser ztgxHDPadduser: user 'ztgxHDP' already exists#区别：useradd + 用户 不能在home文件夹创建同名文件夹[root@vm01 ~]# useradd ztgxHDPuseradd: user 'ztgxHDP' already exists#设置用户密码[root@vm01 ~]# passwd ztgxHDPChanging password for user ztgxHDP.New password:123456BAD PASSWORD: The password is shorter than 8 charactersRetype new password:123456passwd: all authentication tokens updated successfully. 删除用户123[root@vm01 ~]# userdel ztgxHDP#区别：-r 会删除此用户在home下的文件夹[root@vm01 ~]# userdel -r ztgxHDP 普通用户拥有root权限/etc/passwd 将用户名的id和id组修改为01234567[root@vm01 home]# vi /etc/passwdsshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologinntp:x:38:38::/etc/ntp:/sbin/nologinchrony:x:994:991::/var/lib/chrony:/sbin/nologintcpdump:x:72:72::/:/sbin/nologin#ztgxHDP:x:1000:1000::/home/ztgxHDP:/bin/bashztgxHDP:x:0:0::/home/ztgxHDP:/bin/bash /etc/sudoers1234567[root@vm03 etc]# chmod +w /etc/sudoers#### Allow root to run any commands anywhereroot ALL=(ALL) ALLztgxHDP ALL=(ALL) ALL## Allows members of the 'sys' group to run networking, software,]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDP安装常见问题及解决]]></title>
    <url>%2F2019%2F08%2F14%2FBigData%2FHDP%E5%AE%89%E8%A3%85%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[Requires: redhat-lsb-core问题描述： 12345678resource_management.core.exceptions.ExecutionFailed: Execution of '/usr/bin/yum -d 0 -e 0 -y install hbase_2_6_5_0_292' returned 1. Error: Package: hadoop_2_6_5_0_292-hdfs-2.7.3.2.6.5.0-292.x86_64 (HDP-2.6.5.0) Requires: libtirpc-develError: Package: hadoop_2_6_5_0_292-2.7.3.2.6.5.0-292.x86_64 (HDP-2.6.5.0) Requires: redhat-lsb-core You could try using --skip-broken to work around the problem** Found 2 pre-existing rpmdb problem(s), 'yum check' output follows:2:postfix-2.10.1-6.el7.x86_64 has missing requires of libmysqlclient.so.18()(64bit)2:postfix-2.10.1-6.el7.x86_64 has missing requires of libmysqlclient.so.18(libmysqlclient_18)(64bit) 解决：在新的机器上下载所需的软件包 12345678910111213141516171819202122232425262728293031323334[root@test apps]# yum -y install --downloadonly --downloaddir=/opt/apps/redhat-lsb-core redhat-lsb-core[root@test redhat-lsb-core]# ll总用量 1928-rw-r--r--. 1 root root 52232 11月 12 2018 at-3.1.13-24.el7.x86_64.rpm-rw-r--r--. 1 root root 62960 4月 25 2018 avahi-libs-0.6.31-19.el7.x86_64.rpm-rw-r--r--. 1 root root 117272 7月 4 2014 bc-1.06.95-13.el7.x86_64.rpm-rw-r--r--. 1 root root 154336 4月 25 2018 cups-client-1.6.3-35.el7.x86_64.rpm-rw-r--r--. 1 root root 365828 4月 25 2018 cups-libs-1.6.3-35.el7.x86_64.rpm-rw-r--r--. 1 root root 73448 7月 4 2014 ed-1.9-4.el7.x86_64.rpm-rw-r--r--. 1 root root 262480 11月 25 2015 m4-1.4.16-10.el7.x86_64.rpm-rw-r--r--. 1 root root 250776 4月 25 2018 mailx-12.5-19.el7.x86_64.rpm-rw-r--r--. 1 root root 112768 5月 10 2018 patch-2.7.1-10.el7_5.x86_64.rpm-rw-r--r--. 1 root root 144300 8月 11 2017 psmisc-22.20-15.el7.x86_64.rpm-rw-r--r--. 1 root root 38428 3月 27 2015 redhat-lsb-core-4.1-27.el7.centos.1.x86_64.rpm-rw-r--r--. 1 root root 15616 3月 27 2015 redhat-lsb-submod-security-4.1-27.el7.centos.1.x86_64.rpm-rw-r--r--. 1 root root 265768 7月 27 2015 spax-1.5.2-13.el7.x86_64.rpm-rw-r--r--. 1 root root 31064 7月 4 2014 time-1.7-45.el7.x86_64.rpm#安装顺序如下正在安装 : avahi-libs-0.6.31-19.el7.x86_64 1/14正在安装 : 1:cups-libs-1.6.3-35.el7.x86_64 2/14正在安装 : 1:cups-client-1.6.3-35.el7.x86_64 3/14正在安装 : bc-1.06.95-13.el7.x86_64 4/14正在安装 : m4-1.4.16-10.el7.x86_64 5/14正在安装 : psmisc-22.20-15.el7.x86_64 6/14正在安装 : ed-1.9-4.el7.x86_64 7/14正在安装 : patch-2.7.1-10.el7_5.x86_64 8/14正在安装 : mailx-12.5-19.el7.x86_64 9/14正在安装 : spax-1.5.2-13.el7.x86_64 10/14正在安装 : at-3.1.13-24.el7.x86_64 11/14正在安装 : time-1.7-45.el7.x86_64 12/14正在安装 : redhat-lsb-submod-security-4.1-27.el7.centos.1.x86_64 13/14正在安装 : redhat-lsb-core-4.1-27.el7.centos.1.x86_64 14/14 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#安装代码[root@vm01 redhat-lsb-core]# rpm -ivh avahi-libs-0.6.31-19.el7.x86_64.rpmPreparing... ################################# [100%] file /usr/lib64/libavahi-client.so.3.2.9 from install of avahi-libs-0.6.31-19.el7.x86_64 conflicts with file from package avahi-libs-0.6.31-17.el7.x86_64 file /usr/lib64/libavahi-common.so.3.5.3 from install of avahi-libs-0.6.31-19.el7.x86_64 conflicts with file from package avahi-libs-0.6.31-17.el7.x86_64#说明系统中含有此软件，版本不同。卸载旧版本，安装新版本。[root@vm01 redhat-lsb-core]# rpm -e --nodeps avahi-libs-0.6.31-17.el7.x86_64[root@vm01 redhat-lsb-core]# rpm -ivh avahi-libs-0.6.31-19.el7.x86_64.rpmPreparing... ################################# [100%]Updating / installing... 1:avahi-libs-0.6.31-19.el7 ################################# [100%][root@vm01 redhat-lsb-core]# rpm -ivh cups-libs-1.6.3-35.el7.x86_64.rpmPreparing... ################################# [100%] package cups-libs-1:1.6.3-35.el7.x86_64 is already installed#提示一句安装，版本相同，则不用卸载[root@vm01 redhat-lsb-core]# rpm -ivh cups-client-1.6.3-35.el7.x86_64.rpmPreparing... ################################# [100%] package cups-client-1:1.6.3-35.el7.x86_64 is already installed[root@vm01 redhat-lsb-core]# rpm -ivh bc-1.06.95-13.el7.x86_64.rpmPreparing... ################################# [100%] package bc-1.06.95-13.el7.x86_64 is already installed[root@vm01 redhat-lsb-core]# rpm -ivh m4-1.4.16-10.el7.x86_64.rpmPreparing... ################################# [100%] package m4-1.4.16-10.el7.x86_64 is already installed[root@vm01 redhat-lsb-core]# rpm -ivh psmisc-22.20-15.el7.x86_64.rpmPreparing... ################################# [100%] file /usr/bin/killall from install of psmisc-22.20-15.el7.x86_64 conflicts with file from package psmisc-22.20-11.el7.x86_64 file /usr/bin/peekfd from install of psmisc-22.20-15.el7.x86_64 conflicts with file from package psmisc-22.20-11.el7.x86_64 file /usr/bin/prtstat from install of psmisc-22.20-15.el7.x86_64 conflicts with file from package psmisc-22.20-11.el7.x86_64 file /usr/bin/pstree from install of psmisc-22.20-15.el7.x86_64 conflicts with file from package psmisc-22.20-11.el7.x86_64 file /usr/sbin/fuser from install of psmisc-22.20-15.el7.x86_64 conflicts with file from package psmisc-22.20-11.el7.x86_64 file /usr/share/man/man1/fuser.1.gz from install of psmisc-22.20-15.el7.x86_64 conflicts with file from package psmisc-22.20-11.el7.x86_64 file /usr/share/man/man1/peekfd.1.gz from install of psmisc-22.20-15.el7.x86_64 conflicts with file from package psmisc-22.20-11.el7.x86_64[root@vm01 redhat-lsb-core]# rpm -e --nodeps psmisc-22.20-11.el7.x86_64[root@vm01 redhat-lsb-core]# rpm -ivh psmisc-22.20-15.el7.x86_64.rpmPreparing... ################################# [100%]Updating / installing... 1:psmisc-22.20-15.el7 ################################# [100%][root@vm01 redhat-lsb-core]# rpm -ivh ed-1.9-4.el7.x86_64.rpmPreparing... ################################# [100%] package ed-1.9-4.el7.x86_64 is already installed[root@vm01 redhat-lsb-core]# rpm -ivh patch-2.7.1-10.el7_5.x86_64.rpmPreparing... ################################# [100%] package patch-2.7.1-10.el7_5.x86_64 is already installed[root@vm01 redhat-lsb-core]# rpm -ivh mailx-12.5-19.el7.x86_64.rpmPreparing... ################################# [100%] file /bin/mailx from install of mailx-12.5-19.el7.x86_64 conflicts with file from package mailx-12.5-12.el7_0.x86_64 file /usr/share/man/man1/mailx.1.gz from install of mailx-12.5-19.el7.x86_64 conflicts with file from package mailx-12.5-12.el7_0.x86_64[root@vm01 redhat-lsb-core]# rpm -e --nodeps mailx-12.5-12.el7_0.x86_64[root@vm01 redhat-lsb-core]# rpm -ivh mailx-12.5-19.el7.x86_64.rpmPreparing... ################################# [100%]Updating / installing... 1:mailx-12.5-19.el7 ################################# [100%][root@vm01 redhat-lsb-core]# rpm -ivh spax-1.5.2-13.el7.x86_64.rpmPreparing... ################################# [100%] package spax-1.5.2-13.el7.x86_64 is already installed[root@vm01 redhat-lsb-core]# rpm -ivh at-3.1.13-24.el7.x86_64.rpmPreparing... ################################# [100%] file /usr/sbin/atd from install of at-3.1.13-24.el7.x86_64 conflicts with file from package at-3.1.13-22.el7.x86_64 file /usr/bin/at from install of at-3.1.13-24.el7.x86_64 conflicts with file from package at-3.1.13-22.el7.x86_64 file /var/spool/at from install of at-3.1.13-24.el7.x86_64 conflicts with file from package at-3.1.13-22.el7.x86_64 file /usr/share/man/man8/atd.8.gz from install of at-3.1.13-24.el7.x86_64 conflicts with file from package at-3.1.13-22.el7.x86_64 file /var/spool/at/spool from install of at-3.1.13-24.el7.x86_64 conflicts with file from package at-3.1.13-22.el7.x86_64[root@vm01 redhat-lsb-core]# rpm -e --nodeps at-3.1.13-22.el7.x86_64[root@vm01 redhat-lsb-core]# rpm -ivh at-3.1.13-24.el7.x86_64.rpmPreparing... ################################# [100%]Updating / installing... 1:at-3.1.13-24.el7 ################################# [100%][root@vm01 redhat-lsb-core]# rpm -ivh time-1.7-45.el7.x86_64.rpmPreparing... ################################# [100%] package time-1.7-45.el7.x86_64 is already installed[root@vm01 redhat-lsb-core]# rpm -ivh redhat-lsb-submod-security-4.1-27.el7.centos.1.x86_64.rpmPreparing... ################################# [100%] package redhat-lsb-submod-security-4.1-27.el7.centos.1.x86_64 is already installed[root@vm01 redhat-lsb-core]# rpm -ivh redhat-lsb-core-4.1-27.el7.centos.1.x86_64.rpmPreparing... ################################# [100%] package redhat-lsb-core-4.1-27.el7.centos.1.x86_64 is already installed 12345678910[root@test libtirpc-devel]# yum -y install --downloadonly --downloaddir=/opt/apps/other libtirpc-devel[root@test libtirpc-devel]# ll总用量 184-rw-r--r--. 1 root root 91124 11月 12 2018 libtirpc-0.2.4-0.15.el7.x86_64.rpm-rw-r--r--. 1 root root 93048 11月 12 2018 libtirpc-devel-0.2.4-0.15.el7.x86_64.rpm#安装顺序正在安装 : libtirpc-0.2.4-0.15.el7.x86_64 1/2正在安装 : libtirpc-devel-0.2.4-0.15.el7.x86_64 2/2 EOF occurred in violation of protocol安装过程中报错信息如下： 1234567INFO 2019-08-23 13:24:08,381 PingPortListener.py:50 - Ping port listener started on port: 8670INFO 2019-08-23 13:24:08,385 main.py:439 - Connecting to Ambari server at https://vm01:8440 (172.18.4.11)INFO 2019-08-23 13:24:08,385 NetUtil.py:70 - Connecting to https://vm01:8440/caERROR 2019-08-23 13:24:08,549 NetUtil.py:96 - EOF occurred in violation of protocol (_ssl.c:618)ERROR 2019-08-23 13:24:08,549 NetUtil.py:97 - SSLError: Failed to connect. Please check openssl library versions.Refer to: https://bugzilla.redhat.com/show_bug.cgi?id=1022468 for more details.WARNING 2019-08-23 13:24:08,550 NetUtil.py:124 - Server at https://vm01:8440 is not reachable, sleeping for 10 seconds... 解决： 12345678910By adding below config in [security] section ofvi /etc/ambari-agent/conf/ambari-agent.ini[security]force_https_protocol=PROTOCOL_TLSv1_2vi /etc/python/cert-verification.cfg[https]verify=disable 参考：https://community.hortonworks.com/questions/218070/ambari-automatic-registration-failed-step-3-confir-1.htmlhttps://my.oschina.net/aubao/blog/1920933]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>HDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDP集群卸载]]></title>
    <url>%2F2019%2F08%2F14%2FBigData%2FHDP%E9%9B%86%E7%BE%A4%E5%8D%B8%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[ambari-server stop [master]123456[root@vm110 ~]# ambari-server stopUsing python /usr/bin/pythonStopping ambari-serverWaiting for server stop...Ambari Server stopped[root@vm110 ~]# ambari-agent stop remove [all]12[root@vm110 ~]# yum remove ambari-server[root@vm110 ~]# yum remove ambari-agent 组件 [all]12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@vm110 ~]# yum list installed | grep ambari[root@vm110 ~]# yum remove -y ambari*[root@vm110 ~]# yum remove -y smartsense*[root@vm110 ~]# yum list installed | grep hdp[root@vm110 ~]# yum remove -y hdp*[root@vm110 ~]# yum list installed | grep hdfs[root@vm110 ~]# yum -y remove ranger*[root@vm110 ~]# yum list installed | grep yarn[root@vm110 ~]# yum remove -y spark*yum remove -y spark*yum -y remove ranger*yum remove -y hdp*yum remove -y smartsense*yum remove -y ambari*yum remove -y hive*yum remove -y hdfs*yum remove -y yarn*yum remove -y mapreduce2*yum remove -y tez*yum remove -y hbase*yum remove -y pig*yum remove -y sqoop*yum remove -y oozie*yum remove -y zookeeper*yum remove -y falcon*yum remove -y storm*yum remove -y flume*yum remove -y accumulo*yum remove -y Ambari Infra*yum remove -y Ambari Metrics*yum remove -y Atlas*yum remove -y Kafka*yum remove -y Knox*yum remove -y Log Search*yum remove -y Ranger*yum remove -y Ranger KMS*yum remove -y SmartSense*yum remove -y Zeppelin Notebook*yum remove -y Druid*yum remove -y Mahout*yum remove -y Slider*yum remove -y atlas-metadata_2_6_5_0_292-storm-plugin 0.8.0.2.6.5.0-292yum remove -y atlas-metadata_2_6_5_0_292-sqoop-plugin 0.8.0.2.6.5.0-292yum remove -y ranger_2_6_5_0_292-yarn-plugin 0.7.0.2.6.5.0-292yum remove -y bigtop-jsvc 1.0.15-29yum remove -y spark2_2_6_5_0_292-yarn-shuffle 2.3.0.2.6.5.0-292yum remove -y ranger_2_6_5_0_292-hdfs-plugin 0.7.0.2.6.5.0-292yum remove -y ranger_2_6_5_0_292-storm-plugin 0.7.0.2.6.5.0-292yum remove -y ranger_2_6_5_0_292-kafka-plugin 0.7.0.2.6.5.0-292yum remove -y atlas-metadata_2_6_5_0_292-hive-plugin 0.8.0.2.6.5.0-292yum remove -y slider_2_6_5_0_292 0.92.0.2.6.5.0-29yum remove -y ranger_2_6_5_0_292-hbase-plugin 0.7.0.2.6.5.0-292yum remove -y ranger_2_6_5_0_292-hive-plugin 0.7.0.2.6.5.0-292yum remove -y spark_2_6_5_0_292-yarn-shuffle 1.6.3.2.6.5.0-292 删除用户1234567891011121314151617181920212223242526272829303132333435userdel nagiosuserdel hiveuserdel ambari-qauserdel hbaseuserdel oozieuserdel hcatuserdel mapreduserdel hdfsuserdel rrdcacheduserdel zookeeperuserdel mysqluserdel sqoop userdel puppet userdel yarn userdel tez userdel hadoop userdel knox userdel storm userdel falcon userdel flume userdel nagios userdel admin userdel postgres userdel hdfs userdel zookeeper userdel hbase userdel amsuserdel sparkuserdel kafkauserdel rangeruserdel kmsuserdel zookeeperuserdel ambari-qauserdel hdfsuserdel mapred 删除文件夹123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687rm -rf /hadoop/* rm -rf /etc/ambari* rm -rf /etc/zookeeper/ rm -rf /etc/ranger* rm -rf /etc/rc.d/init.d/ranger* rm -rf /etc/hadoop/ rm -rf /etc/hadoop/ rm -rf /etc/hbase/ rm -rf /etc/hive rm -rf /usr/hdp/ rm -rf /etc/zookeeper/ rm -rf /tmp/ambari-qa rm -rf /tmp/sqoop-ambari-qa/ rm -rf /kafka-logs/ rm -rf /var/lib/ambari* rm -rf /var/lib/hive2* rm -rf /var/run/kafka/ rm -rf /etc/flumerm -rf /etc/hive-hcatalogrm -rf /etc/hive-webhcat rm -rf /etc/phoenixrm -rf /etc/ambari-metrics-collector rm -rf /etc/ambari-metrics-monitorrm -rf /tmp/hdfs rm -rf /tmp/hcat rm -rf /etc/kafka rm -rf /etc/oozierm -rf /etc/stormrm -rf /etc/tezrm -rf /etc/falconrm -rf /etc/sliderrm -rf /etc/pigrm -rf /etc/rangerrm -rf /var/log/hadooprm -rf /var/log/hbaserm -rf /var/log/hiverm -rf /var/log/oozierm -rf /var/log/sqooprm -rf /var/log/zookeeperrm -rf /var/log/flumerm -rf /var/log/stormrm -rf /var/log/hive-hcatalogrm -rf /var/log/falconrm -rf /var/log/webhcatrm -rf /var/log/hadoop-hdfsrm -rf /var/log/hadoop-yarnrm -rf /var/log/hadoop-mapreducerm -rf /var/log/sparkrm -rf /var/log/rangerrm -rf /usr/lib/flumerm -rf /usr/lib/stormrm -rf /var/lib/hiverm -rf /var/lib/oozierm -rf /var/lib/zookeeperrm -rf /var/lib/flumerm -rf /var/lib/hadoop-hdfsrm -rf /var/lib/sliderrm -rf /var/lib/rangerrm -rf /var/tmp/oozierm -rf /var/tmp/sqooprm -rf /tmp/hiverm -rf /tmp/hadoop-hdfsrm -rf /etc/sqooprm -rf /var/log/ambari-metrics-collectorrm -rf /var/log/ambari-metrics-monitorrm -rf /usr/lib/ambari-metrics-collectorrm -rf /var/lib/hadoop-yarnrm -rf /var/lib/hadoop-mapreducerm -rf /var/lib/ambari-metrics-collectorrm -rf /var/log/kafkarm -rf /tmp/hadoop-yarnrm -rf /hadoop/zookeeperrm -rf /hadoop/hdfsrm -rf /kafka-logsrm -rf /etc/storm-slider-clientrm -rf /var/run/hadooprm -rf /var/run/hbaserm -rf /var/run/hiverm -rf /var/run/sqooprm -rf /var/run/zookeeperrm -rf /var/run/hive-hcatalogrm -rf /var/run/webhcatrm -rf /var/run/hadoop-hdfsrm -rf /var/run/hadoop-yarnrm -rf /var/run/hadoop-mapreducerm -rf /var/run/sparkrm -rf /var/run/ambari-metrics-monitor]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>HDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark错误记录]]></title>
    <url>%2F2019%2F08%2F13%2FBigData%2Fspark%E9%94%99%E8%AF%AF%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[lost executor问题描述: 119/08/13 14:27:26 ERROR YarnScheduler: Lost executor 7 on vm54: Slave lost 123456java.io.IOException: Connection from /192.168.200.52:52124 closed at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146) at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:108) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231) at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224) 1234rg.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 5 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 说明：数据量大，且代码中含有shuffle过程较多 可能原因分析： yarn资源不够 节点内存分配太少，yarn kill 了spark application rdd太大，内存资源不够 解决：增加executor内存，同时增加每个executor的cpu 解决记录： before： 123456789101112131415161718192021#! /bin/bashlist="delete replace_ETL Merge"FILE_NAME=`date -d "1 days ago" +%Y-%m-%d`for type in $list ;doecho $type/usr/hdp/current/spark2-client/bin/spark-submit \--master yarn \--name $&#123;type&#125; \--class cn.sic_credit.bigdata.update_prism.report_shareholder.report_shareholder_$&#123;type&#125; \--queue Liu \--num-executors 3 \--executor-memory 2g \--executor-cores 3 \--jars $(echo /usr/hdp/2.6.5.0-292/hbase/lib/*.jar | tr ' ' ',') \/usr/local/src/liu/update_prism_sql/ztgx3-1.0-SNAPSHOT.jar \$&#123;FILE_NAME&#125;done after: 123456789101112131415161718192021#! /bin/bashlist="delete replace_ETL Merge"FILE_NAME=`date -d "1 days ago" +%Y-%m-%d`for type in $list ;doecho $type/usr/hdp/current/spark2-client/bin/spark-submit \--master yarn \--name $&#123;type&#125; \--class cn.sic_credit.bigdata.update_prism.report_shareholder.report_shareholder_$&#123;type&#125; \--queue Liu \--num-executors 3 \--executor-memory 10g \--executor-cores 3 \--jars $(echo /usr/hdp/2.6.5.0-292/hbase/lib/*.jar | tr ' ' ',') \/usr/local/src/liu/update_prism_sql/ztgx3-1.0-SNAPSHOT.jar \$&#123;FILE_NAME&#125;done 参考：Lost executor 原因分析及解决方案-记录Spark程序运行常见错误解决方法以及优化 explode炸裂函数问题描述： 1234567891011121314151617181920212223"com_bus_risks-intellectual_property": [], "com_bus_risks-judicial_sale ": [ &#123; "notice": "大耳朵图图", "date": "2019-8-20 15:26", "executive_court": "北京丰台", "sale_object": "ztgx" &#125;, &#123; "notice": "大耳朵图图", "date": "2019-8-20 15:26", "executive_court": "北京丰台", "sale_object": "ztgx" &#125;, &#123; "notice": "大耳朵图图", "date": "2019-8-20 15:26", "executive_court": "北京丰台", "sale_object": "ztgx" &#125; ], "com_bus_risks-mortgage": [], "com_bus_risks-out_abnormal_operation": [], 如上json所示，使用spark-sql解析json文件时，可以使用sql解析，可以使用core（fastjson）解析。但是遇到jsonArray即一个json数组里面含有多个json对象的情况，这个时候，对于core来说，需要解析，就需要将其数组遍历，把数组里的json对象当成多个，在外定义stringbuffer，在里面使用+=，将多个json对象同一个key的不同的值拼接起来，后面再分别取值然后赋key。 但如果使用explode炸裂函数，这种情况就很好解决了。 123456789101112131415161718192021import org.apache.spark.sql.functions._import session.implicits._val df: DataFrame = session.read.format("json").load("file:///D:\\ZTGX\\tycdata\\new") val df1 = df.select("com_bus_risks-judicial_sale ").show(100, false) val df2 = df.select( $"com_name", $"com_credit_code", (explode($"com_bus_risks-judicial_sale ") as "judicial_sale") ) df2.show(100, false) val df3 = df2.select( $"com_name", $"com_credit_code", $"judicial_sale.notice", $"judicial_sale.date", $"judicial_sale.executive_court", $"judicial_sale.sale_object" ) df3.show(100, false) df1.show 12345678910+------------------------------------------------------------------+|com_bus_risks-judicial_sale |+------------------------------------------------------------------+|[[2019-8-20,北京丰台,大耳朵图图,ztgx]] ||[[2019-8-202,北京丰台2,大耳朵图图2,ztgx2], [2019-8-203,北京丰台3,大耳朵图图3,ztgx3]]||[] ||[] ||[] ||[[2019-8-20 15:26,北京丰台,大耳朵图图,ztgx]] |+------------------------------------------------------------------+ df2.show 12345678+------------+---------------+---------------------------------+|com_name |com_credit_code|judicial_sale |+------------+---------------+---------------------------------+|敖汉旗丰收邢玉新建材门市|- |[2019-8-20,北京丰台,大耳朵图图,ztgx] ||吴君丽-湖南省平江县 |- |[2019-8-202,北京丰台2,大耳朵图图2,ztgx2] ||吴君丽-湖南省平江县 |- |[2019-8-203,北京丰台3,大耳朵图图3,ztgx3] ||敖汉旗丰收邢玉新建材门市|- |[2019-8-20 15:26,北京丰台,大耳朵图图,ztgx]|+------------+---------------+---------------------------------+ df3.show 12345678+------------+---------------+------+---------------+---------------+-----------+|com_name |com_credit_code|notice|date |executive_court|sale_object|+------------+---------------+------+---------------+---------------+-----------+|敖汉旗丰收邢玉新建材门市|- |大耳朵图图 |2019-8-20 |北京丰台 |ztgx ||吴君丽-湖南省平江县 |- |大耳朵图图2|2019-8-202 |北京丰台2 |ztgx2 ||吴君丽-湖南省平江县 |- |大耳朵图图3|2019-8-203 |北京丰台3 |ztgx3 ||敖汉旗丰收邢玉新建材门市|- |大耳朵图图 |2019-8-20 15:26|北京丰台 |ztgx |+------------+---------------+------+---------------+---------------+-----------+]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kerberos学习]]></title>
    <url>%2F2019%2F08%2F11%2FBigData%2Fkerberos%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[kerberos是什么强大的身份验证和建立用户身份是Hadoop安全访问的基础。用户需要能够可靠地“识别”自己，然后在整个Hadoop集群中传播该身份。完成此操作后，这些用户可以访问资源（例如文件或目录）或与集群交互（如运行MapReduce作业）。除了用户之外，Hadoop集群资源本身（例如主机和服务）需要相互进行身份验证，以避免潜在的恶意系统或守护程序“冒充”受信任的集群组件来获取数据访问权限。 Hadoop使用Kerberos作为用户和服务的强身份验证和身份传播的基础。Kerberos是一种计算机网络认证协议，它允许某实体在非安全网络环境下通信，向另一个实体以一种安全的方式证明自己的身份。 Kerberos是第三方认证机制，其中用户和服务依赖于第三方（Kerberos服务器）来对彼此进行身份验证。 Kerberos服务器本身称为密钥分发中心或KDC。 在较高的层面上，它有三个部分： 它知道的用户和服务（称为主体）及其各自的Kerberos密码的数据库一个认证服务器（AS）执行初始认证并颁发票证授予票证（TGT）一个票据授权服务器（TGS）发出基于初始后续服务票证TGT一个用户主要来自AS请求认证。AS返回使用用户主体的Kerberos密码加密的TGT，该密码仅为用户主体和AS所知。用户主体使用其Kerberos密码在本地解密TGT，从那时起，直到ticket到期，用户主体可以使用TGT从TGS获取服务票据。服务票证允许委托人访问各种服务。 Kerberos简单来说就是一个用于安全认证第三方协议，它采用了传统的共享密钥的方式，实现了在网络环境不一定保证安全的环境下，client和server之间的通信，适用于client/server模型，由MIT开发和实现。 Kerberos服务是单点登录系统，这意味着您对于每个会话只需向服务进行一次自我验证，即可自动保护该会话过程中所有后续事务的安全。 由于每次解密TGT时群集资源（主机或服务）都无法提供密码，因此它们使用称为keytab的特殊文件，该文件包含资源主体的身份验证凭据。 Kerberos服务器控制的主机，用户和服务集称为领域。]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Kerberos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache zeppelin学习]]></title>
    <url>%2F2019%2F08%2F11%2FBigData%2FApache-zeppelin%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[zeppelin是什么zeppelin是apache的一个孵化项目，是一个基于web的笔记本。Ambari平台上集成了此组件，一般来说，开发人员可以将一些开发文档，笔记记录在此组件上，供多人查阅和修改。可以使用sql，shell，scala，spark(命令行)，makedown的语法做出数据驱动，交互，协作的文档各语言可以实时执行，实时查阅结果类似于ipython notebook，可以直接在浏览器中写代码，笔记并且共享 zeppelin安装和使用基本使用界面： Create new note makedown12## test创建了test文档。设置文档类型为makedown格式 shell123%shDATE=20170902echo $DATE scala1234%spark2val one=1val two="hello"println("hello world") spark.sql12%spark2.sqlshow databases html12%angular&lt;p&gt;html语言&lt;/p&gt; zeppelin其他]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Zeppelin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ranger学习]]></title>
    <url>%2F2019%2F08%2F11%2FBigData%2FRanger%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[Ranger是什么Ranger是大数据领域的一个集中式安全管理框架，他可以对诸如hdfs，hive，hbase，kafka进行细粒度的权限控制。一般来说，大数据产品提供给多人使用，是需要进行权限控制的，对于数据安全，产品可靠性都是很重要的 例如：控制某个用户或者一组用户对hdfs访问的权限，读写，或者控制访问的路径控制某个用户或者一组用户对hive的表的访问权限，读写，控制到哪个数据库，哪个表，哪一列… Ranger支持的组件 Ranger权限模型Ranger的权限模型主要是由权限策略组成。权限策略分为三个方面：用户，资源，权限 用户：Ranger可以对用户进行分组，一个用户可以属于多个组。Ranger可以对用户或者组进行资源的权限控制 资源：对于不同的组件，资源有着不同的表现形式。对于HDFS来说，文件路径就是资源，对于hive来说，database，table，column都是资源 权限：Ranger可以对各个资源的读，写，访问进行控制。具体可以配置白名单，白名单排除，黑名单，黑名单排斥 在大数据集群中安装好Ranger，并进行相应的权限策略配置之后，用户访问资源就会进行Ranger的权限校验，校验流程如下： 用户请求资源，会先获取所有相关的策略，并且遍历这些策略，然后根据黑白名单去判断用户访问资源的权限。由上面流程图可以看出，黑白名单的优先级。 黑名单的优先级高于白名单 黑名单排除的优先级高于黑名单 白名单排除的优先级高于白名单 决策下发如果没有匹配到任何策略，一般情况是没有权限拒绝访问。Ranger也可以设置决策下发交给系统自身的权限控制。 Ranger架构 Ranger-adminranger-admin是基于Jersey+Spring+EclipseLink框框开发的Web服务，对外提供了Restful风格的http服务。Ranger-Admin模块同样内嵌了jsp界面，用于管理员管理用户、资源、权限等信息。同样，我们可以基于它的Restful Api来编写自己的权限管理sdk。 虽然Jersey和SpringMVC同样实现了JAX-RS(javaee6提供Java API for RESTful Web Services)规范，但是jersey更加适合构建restful风格的服务，因为它天生就是为restful而生的。EclipseLink 是 JPA(Java Persistence Api)的一种实现，是java的一个ORM框架。 Plugin那么ranger是如何实现对大数据组件的权限控制访问呢？这就和ranger实现的一个个plugin有关系了。 因为几乎所有的大数据组件都有提供一个抽象的验证接口，ranger就是根据这些接口为各个大数据组件实现了对应的plugin，plugin的工作主要是从Ranger-Admin处拉取该组件配置的所有策略，然后缓存到本地，然后当有用户来请求时提供鉴权服务。 Ranger的安装使用参考文献https://blog.csdn.net/u013332124/article/details/86360756]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Ranger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume情况简介]]></title>
    <url>%2F2019%2F08%2F04%2FToWork%2Fflume%E6%83%85%E5%86%B5%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[生产集群路径：[root@vm51 myconf]# cd /opt/apps/apache-flume-1.7.0-bin/ conf文件：[root@vm51 myconf]# cd /opt/apps/apache-flume-1.7.0-bin/conf/myconf/ avro_memory_hdfs.conf： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#192.168.200.51 avro_memory_hdfsa1.sources = r1a1.channels = c1a1.sinks = k1#sourcea1.sources.r1.type = avroa1.sources.r1.channels = c1a1.sources.r1.bind = 192.168.200.51a1.sources.r1.port = 4545#memorya1.channels = c1a1.channels.c1.type = memorya1.channels.c1.capacity = 100000a1.channels.c1.transactionCapacity = 10000a1.channels.c1.keep-alive = 30#sinksa1.sinks.k1.type = hdfsa1.sinks.k1.channel = c1#指定目录a1.sinks.k1.hdfs.path = /flume/logs/%&#123;dataType&#125;/%Y-%m-%d#文件的命名, 前缀a1.sinks.k1.hdfs.filePrefix = logs-#24h就改目录（创建目录）， （这些参数影响/flume/events/%y-%m-%d/%H%M/）a1.sinks.k1.hdfs.round = truea1.sinks.k1.hdfs.roundValue = 24a1.sinks.k1.hdfs.roundUnit = hour#目录里面有文件#------start----符合其中一个就满足---#文件滚动之前的等待时间(秒)a1.sinks.k1.hdfs.rollInterval = 2400#文件滚动的大小限制(bytes) 127Ma1.sinks.k1.hdfs.rollSize = 133169152#写入多少个event数据后滚动文件(事件个数)a1.sinks.k1.hdfs.rollCount = 0#-------end-----#100个事件就往里面写入a1.sinks.k1.hdfs.batchSize = 100#用本地时间格式化目录a1.sinks.k1.hdfs.useLocalTimeStamp = true#下沉后, 生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本a1.sinks.k1.hdfs.fileType = DataStream#操作hdfs超时时间（毫秒）a1.sinks.k1.hdfs.callTimeout = 50000 爬虫集群200路径：root@ubuntu:~# cd /opt/apps/apache-flume-1.7.0-bin/ conf：root@ubuntu:~# cd /opt/apps/apache-flume-1.7.0-bin/conf/myconf/ taildir_memory_avro2.conf： 123456789101112131415161718192021222324252627282930313233343536#192.168.1.200 taildir_memory_avroa1.sources = r1a1.channels = c1a1.sinks = k1#sourcea1.sources.r1.type = TAILDIRa1.sources.r1.channels = c1#记录偏移量路径a1.sources.r1.positionFile = /home/FlumePositionFile/taildir_position.jsona1.sources.r1.filegroups = f1#监控文件路径a1.sources.r1.filegroups.f1 = /home/data1/flume_data/tyc_data/.*.loga1.sources.r1.headers.f1.headerKey1 = value1a1.sources.r1.fileHeader = true#静态拦截器a1.sources.r1.interceptors = i1a1.sources.r1.interceptors.i1.type=statica1.sources.r1.interceptors.i1.key= dataTypea1.sources.r1.interceptors.i1.value=tyc_dataa1.sources.r1.interceptors.i1.preserveExisting=false#memorya1.channels = c1a1.channels.c1.type = memorya1.channels.c1.capacity = 10000a1.channels.c1.transactionCapacity = 1000#sinksa1.sinks.k1.type = avroa1.sinks.k1.channel = c1a1.sinks.k1.hostname = 192.168.200.51a1.sinks.k1.port = 4545 201路径：root@ubuntu:~# cd /opt/apps/apache-flume-1.7.0-bin/ conf：root@ubuntu:~# cd /opt/apps/apache-flume-1.7.0-bin/conf/myconf/ taildir_memory_avro2.conf： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#192.168.1.201 taildir_memory_avroa1.sources = r1 r2 r3a1.channels = c1a1.sinks = k1#source r1a1.sources.r1.type = TAILDIRa1.sources.r1.channels = c1a1.sources.r1.positionFile = /home/FlumePositionFile/taildir_position.jsona1.sources.r1.filegroups = f1a1.sources.r1.filegroups.f1 = /home/data1/flume_data/tyc_data/.*.loga1.sources.r1.headers.f1.headerKey1 = value1a1.sources.r1.fileHeader = true#静态拦截器 r1a1.sources.r1.interceptors = i1a1.sources.r1.interceptors.i1.type=statica1.sources.r1.interceptors.i1.key= dataTypea1.sources.r1.interceptors.i1.value=tyc_dataa1.sources.r1.interceptors.i1.preserveExisting=false#source r2a1.sources.r2.type = TAILDIRa1.sources.r2.channels = c1a1.sources.r2.positionFile = /home/FlumePositionFile/tyc_relation_data/taildir_position.jsona1.sources.r2.filegroups = f1a1.sources.r2.filegroups.f1 = /home/data1/flume_data/tyc_relation_data/.*.loga1.sources.r2.headers.f1.headerKey1 = value1a1.sources.r2.fileHeader = true#静态拦截器 r2a1.sources.r2.interceptors = i2a1.sources.r2.interceptors.i2.type=statica1.sources.r2.interceptors.i2.key= dataTypea1.sources.r2.interceptors.i2.value=tyc_relation_dataa1.sources.r2.interceptors.i2.preserveExisting=false#source r3a1.sources.r3.type = TAILDIRa1.sources.r3.channels = c1a1.sources.r3.positionFile = /home/FlumePositionFile/qcc_data/taildir_position.jsona1.sources.r3.filegroups = f1a1.sources.r3.filegroups.f1 = /home/data1/flume_data/qcc_data/.*.loga1.sources.r3.headers.f1.headerKey1 = value1a1.sources.r3.fileHeader = true#r2静态拦截器 r3a1.sources.r3.interceptors = i3a1.sources.r3.interceptors.i3.type=statica1.sources.r3.interceptors.i3.key= dataTypea1.sources.r3.interceptors.i3.value=qcc_dataa1.sources.r3.interceptors.i3.preserveExisting=false#memorya1.channels = c1a1.channels.c1.type = memorya1.channels.c1.capacity = 10000a1.channels.c1.transactionCapacity = 1000#sinksa1.sinks.k1.type = avroa1.sinks.k1.channel = c1a1.sinks.k1.hostname = 192.168.200.51a1.sinks.k1.port = 4545 运行flume后台运行：nohup ./bin/flume-ng agent -n a1 -c ./conf/ -f ./conf/myconf/avro_memory_hdfs.conf &amp; 日志路径：cd /opt/apps/apache-flume-1.7.0-bin/logs/ 查看： 123root@ubuntu:/opt/apps/apache-flume-1.7.0-bin/conf/myconf# ps -ef | grep flumeroot 1551 1 1 Jul01 ? 11:52:15 /usr/local/custom/jdk1.8.0_74/bin/java -Xms2048m -Xmx2048m -Xss256k -Xmn1g -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:-UseGCOverheadLimit -cp /opt/apps/apache-flume-1.7.0-bin/conf:/opt/apps/apache-flume-1.7.0-bin/lib/*:/lib/* -Djava.library.path= org.apache.flume.node.Application -n a1 -f ./conf/myconf/taildir_memory_avro2.confroot 17247 16911 0 22:16 pts/0 00:00:00 grep --color=auto flume kill：kill -9 1551 注意才安装的flume，需要设置flume的jvm启动参数，【内存大小】.否则文件太大会报内存溢出的错flume系列之Java heap space大小设置]]></content>
      <categories>
        <category>ToWork</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统，Linux桌面系统]]></title>
    <url>%2F2019%2F08%2F01%2FBigData%2FLinux%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[操作系统英文：operating system缩写：OS 是管理计算机硬件和软件资源的计算机程序，同时也是计算机系统的内核和基石。操作系统需要处理如管理与配置内存、决定系统资源供需的优先次序、控制输入设备与输出设备、操作网络与管理文件系统等基本事务。操作系统也提供一个让用户与系统交互的操作界面。 操作系统的类型非常多样，不同机器安装的操作系统可从简单到复杂，可从移动电话的嵌入式系统到超级计算机的大型操作系统。许多操作系统制造者对它涵盖范畴的定义也不尽一致，例如有些操作系统集成了图形用户界面，而有些仅使用命令行界面，而将图形用户界面视为一种非必要的应用程序。 常见操作系统Android, iOS, Linux, Windows,Mac OS X 类Unix系统类Unix系统（英文：Unix-like）指各种传统的Unix系统（比如FreeBSD、OpenBSD、SUN公司的Solaris）以及各种与传统Unix类似的系统（例如Minix、Linux、QNX等）。它们虽然有的是自由软件，有的是商业软件，但都相当程度地继承了原始UNIX的特性，有许多相似处，并且都在一定程度上遵守POSIX规范。类Unix通常指的是比原先的Unix包含更多特征的OS。 使用范围最广的是linux系统 Linux系统Linux是一套免费使用和自由传播的类Unix操作系统，是一个基于POSIX和UNIX的多用户、多任务、支持多线程和多CPU的操作系统。它能运行主要的UNIX工具软件、应用程序和网络协议。它支持32位和64位硬件。Linux继承了Unix以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。Linux操作系统诞生于1991 年10 月5 日（这是第一次正式向外公布时间）。Linux存在着许多不同的Linux版本，但它们都使用了Linux内核。Linux可安装在各种计算机硬件设备中，比如手机、平板电脑、路由器、视频游戏控制台、台式计算机、大型机和超级计算机。严格来讲，Linux这个词本身只表示Linux内核，但实际上人们已经习惯了用Linux来形容整个基于Linux内核，并且使用GNU 工程各种工具和数据库的操作系统。 Linux发行版本从技术上来说，李纳斯•托瓦兹开发的 Linux 只是一个内核。内核指的是一个提供设备驱动、文件系统、进程管理、网络通信等功能的系统软件，内核并不是一套完整的操作系统，它只是操作系统的核心。一些组织或厂商将 Linux 内核与各种软件和文档包装起来，并提供系统安装界面和系统配置、设定与管理工具，就构成了 Linux 的发行版本。 在 Linux 内核的发展过程中，各种 Linux 发行版本起了巨大的作用，正是它们推动了 Linux 的应用，从而让更多的人开始关注 Linux。因此，把 Red Hat、Ubuntu、SUSE 等直接说成 Linux 其实是不确切的，它们是 Linux 的发行版本，更确切地说，应该叫作“以Linux为核心的操作系统软件包”。 Linux 的各个发行版本使用的是同一个 Linux 内核，因此在内核层不存在什么兼容性问题，每个版本有不一样的感觉，只是在发行版本的最外层（由发行商整合开发的应用）才有所体现。 Linux 的发行版本可以大体分为两类：商业公司维护的发行版本，以著名的 Red Hat 为代表；社区组织维护的发行版本，以 Debian 为代表。 Linux部分发行版本Red Hat Linux可能这是最著名的Linux版本了，Red Hat Linux已经创造了自己的品牌，越来越多的人听说过它。Red Hat在1994年创业，当时聘用了全世界500多名员工，他们都致力于开放的源代码体系。Red Hat Linux是公共环境中表现上佳的服务器。它拥有自己的公司，能向用户提供一套完整的服务，这使得它特别适合在公共网络中使用。这个版本的Linux也使用最新的内核，还拥有大多数人都需要使用的主体软件包。Red Hat Linux的安装过程也十分简单明了。它的图形安装过程提供简易设置服务器的全部信息。磁盘分区过程可以自动完成，还可以选择GUI工具完成，即使对于 Linux新手来说这些都非常简单。选择软件包的过程也与其他版本类似；用户可以选择软件包种类或特殊的软件包。系统运行起来后，用户可以从Web站点和 Red Hat那里得到充分的技术支持。我发现Red Hat是一个符合大众需求的最优版本。在服务器和桌面系统中它都工作得很好。Red Hat的唯一缺陷是带有一些不标准的内核补丁，这使得它难于按用户的需求进行定制。 Red Hat通过论坛和邮件列表提供广泛的技术支持，它还有自己公司的电话技术支持，后者对要求更高技术支持水平的集团客户更有吸引力。 Ubuntu LinuxUbuntu是一个以桌面应用为主的Linux操作系统，其名称来自非洲南部祖鲁语或豪萨语的“ubuntu”一词（译为吾帮托或乌班图），意思是“人性”、“我的存在是因为大家的存在”，是非洲传统的一种价值观，类似华人社会的“仁爱”思想。Ubuntu基于Debian发行版和unity桌面环境，与Debian的不同在于它每6个月会发布一个新版本。Ubuntu的目标在于为一般用户提供一个最新的、同时又相当稳定的主要由自由软件构建而成的操作系统。Ubuntu具有庞大的社区力量，用户可以方便地从社区获得帮助。随着云计算的流行，ubuntu推出了一个云计算环境搭建的解决方案，可以在其官方网站找到相关信息。于2012年4月26日发布最终版ubuntu 12.04，ubuntu 12.04是长期支持的版本。Red Hat Linux CentosCentOS（Community ENTerprise Operating System）是Linux发行版之一，它是来自于Red Hat Enterprise Linux依照开放源代码规定释出的源代码所编译而成。由于出自同样的源代码，因此有些要求高度稳定性的服务器以CentOS替代商业版的Red Hat Enterprise Linux使用。两者的不同，在于CentOS并不包含封闭源代码软件,CentOS 是一个基于Red Hat Linux 提供的可自由使用源代码的企业级Linux发行版本。每个版本的 CentOS都会获得十年的支持（通过安全更新方式）。新版本的 CentOS 大约每两年发行一次，而每个版本的 CentOS 会定期（大概每六个月）更新一次，以便支持新的硬件。这样，建立一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。CentOS是Community Enterprise Operating System的缩写。CentOS 是RHEL（Red Hat Enterprise Linux）源代码再编译的产物，而且在RHEL的基础上修正了不少已知的 Bug ，相对于其他 Linux 发行版，其稳定性值得信赖。 DebianDebian Project诞生于1993年8月13日，它的目标是提供一个稳定容错的Linux版本。支持Debian的不是某家公司，而是许多在其改进过程中投入了大量时间的开发人员，这种改进吸取了早期Linux的经验。Debian以其稳定性著称，虽然它的早期版本Slink有一些问题，但是它的现有版本Potato已经相当稳定了。这个版本更多的使用了 pluggable authentication modules (PAM)，综合了一些更易于处理的需要认证的软件（如winbind for Samba）。Debian的安装完全是基于文本的，对于其本身来说这不是一件坏事。但对于初级用户来说却并非这样。因为它仅仅使用fdisk 作为分区工具而没有自动分区功能，所以它的磁盘分区过程令人十分讨厌。磁盘设置完毕后，软件工具包的选择通过一个名为dselect的工具实现，但它不向用户提供安装基本工具组（如开发工具）的简易设置步骤。最后需要使用anXious工具配置X Windows，这个过程与其他版本的X Windows配置过程类似。完成这些配置后，Debian就可以使用了。Debian主要通过基于Web的论坛和邮件列表来提供技术支持。作为服务器平台，Debian提供一个稳定的环境。为了保证它的稳定性，开发者不会在其中随意添加新技术，而是通过多次测试之后才选定合适的技术加入。当前最新正式版本是Debian 6，采用的内核是Linux 2.6.32。Debian 6 第一次 包含了一个100%开源的Linux内核，这个内核中不再包含任何闭源的硬件驱动。所有的闭源软件都被隔离成单独的软件包，放到Debian软件源的 “non-free” 部分。由此，Debian用户便可以自由地选择是使用一个完全开源的系统还是添加一些闭源驱动。 SuSe总部设在德国的SuSE AG在商界已经奋斗了8年多，它一直致力于创建一个连接数据库的最佳Linux版本。为了实现这一目的，SuSE与Oracle 和IBM合作，以使他们的产品能稳定地工作。SuSE还开发了SuSE Linux eMail Server III，一个非常稳定的电子邮件群组应用。基于2.4.10内核的SuSE 7.3，在原有版本的基础上提高了易用性。安装过程通过GUI完成，磁盘分区过程也非常简单，但它没有为用户提供更多的控制和选择。在SuSE 操作系统下，可以非常方便地访问Windows磁盘，这使得两种平台之间的切换，以及使用双系统启动变得更容易。SuSE的硬件检测非常优秀，该版本在服务器和工作站上都用得很好。SuSE拥有界面友好的安装过程，还有图形管理工具，可方便地访问Windows磁盘，对于终端用户和管理员来说使用它同样方便，这使它成为了一个强大的服务器平台。 SuSE也通过基于Web的论坛提供技术支持，另外我还发现它有电话技术支持。 Linux MintLinux Mint是一个基于Ubuntu的发行版，最早于2006年由居住在爱尔兰的法国出生的IT专家Clement Lefebvre发布。最初维护一个专门为新Linux用户提供帮助，技巧和文档的Linux网站，笔者看到了开发Linux发行版的必要性，该发行版致力于解决那些技术性较强的产品的使用问题，让它们更易于使用。在他的网站上向访问者征求反馈意见之后，他继续把许多人提到的“改进的Ubuntu”或“Ubuntu完善版”的东西建立起来。注：Ubuntu就是以易用，对新手友好著称的。可想而知Mint的目标更进一步，让Linux更加的贴近了普通用户。 但是，Linux Mint不仅仅是一个具有新的应用程序和更新的桌面主题的Ubuntu。自开始以来，开发人员一直在增加各种Mint下的图形工具以提高可用性;这包括mintDesktop – 用于配置桌面环境的实用程序，mintMenu – 一个新的，优雅的菜单结构，以方便导航，mintInstall – 一个易于使用的软件安装程序，mintUpdate – 一个软件更新程序，提供了一些更突出的几个工具和数百个额外的改进。该项目还开发了很多替代的专有程序以避免一些潜在的法律版权问题，其中包括专利和专利设计的多媒体编解码器，这些编解码器在很多发行版中通常是不存在的。因此，Mint在易用性方面的声誉得到了进一步的加强，也许Linux Mint的最佳特性之一就是开发人员倾听用户的意见，并总是快速地实施好的建议。 因为Linux Mint是可以免费下载，因此该项目通过捐赠，广告和专业支持服务获得收入。它没有固定的发布时间表或者计划的功能列表，但是在每个Ubuntu长期支持版本发布几周后，可以预期Linux Mint的新版本。除Mint的MATE和Cinnamon桌面两个主要版本之外，该项目还使用包括KDE和Xfce在内的其他桌面版本构建版本。这些版本通常在两个“主要”版本几周后完成，有时可能会缺少一些主要分支中中的一些“Mint”工具和其他功能。 Mint系列的另一个版本是基于Debian稳定版分支的“Debian版”。 Linux Mint的Debian版本提供了非常稳定的基础，而桌面软件包的更新速度比Mint的“主要分支”版本更快。 Linux Mint不适用软件自由原则，也不会发布安全公告。 优点：精心整理的内部开发的“Mint”工具，数百个用户友好的增强功能，包含多媒体编解码器缺点：“社区”版本，因此可能并不总是包含最新的功能。另外，项目不会发布安全建议软件包管理： mintInstall包管理器,使用DEB包（与Ubuntu兼容）可用的版本：“主”版本（MATE和Cinnamon桌面），“社区”版本（KDE和Xfce桌面），Linux Mint“Debian”版本（MATE或Cinnamon桌面）替代选择：Ubuntu, elementary OS, Zorin OS, Lubuntu, Xubuntu, Peppermint OS MageiaMageia可能是这个列表中的最新发行版，但它的来源可以追溯到1998年7月，当时GalDuval发布了Mandrake Linux。当时它只是一个红帽Linux的分支，KDE作为默认的桌面，更完善的硬件支持和一些用户友好的功能，加上媒体的积极评论，它获得了一定的知名度。Mandrake Linux后来变成了一个商业版本，并在2010年几乎破产之前更名为Mandriva（为了避免一些与商标有关的麻烦，并纪念与巴西的Conectiva合并），最终由一家俄罗斯风险投资公司拯救了，新管理层因为巨大的开支而决定在该公司巴黎总部裁减大部分的Mandriva开发人员。在没有工作的情况下，他们决定组建一个Mageia，这个社区项目是Mandrake和Mandriva的核心延续，或许比Mandriva本身更为合理。 Mageia主要是一个桌面版本。其最受欢迎的功能是最优秀的软件应用，精良的系统管理套件（Mageia控制中心），吸引了大量志愿者贡献者以及广泛的国际化支持。它具有最简单但功能强大的系统安装程序之一，同时还可以使用KDE或GNOME桌面和全面的语言支持。而且可以来直接从桌面安装系统，无需刻录到U盘。该发行版具有良好的软件包管理功能，具有强大的命令行选项和图形化软件管理模块，可以轻松访问数千个软件包。独特的Mageia控制中心随着每个版本的不断改进，为Linux的新手提供了一个强大的工具来配置他们的计算机的任何方面，而无需使用终端命令行。 尽管Mageia自2010年9月成立以来一直处于起步阶段，但仍有人担心其是否有能力维持长期开发的工作，毕竟大部分工作是由志愿者在完成的。此外，它缺乏一些更大的Linux发行版的完善的基础架构。项目的文档也需要做一些改进，而9个月的发布周期在引起新闻和媒体兴趣方面也可以被视为一个缺点，特别是与其他使用6个月的短期开发过程的主要发行版相比。 优点：适合初学者;优秀的中央配置工具;支持数十种语言的开箱即用支持;可安装的Live镜像缺点：与Mandriva分开之后，缺乏声誉和资源，有人担心开发者没有能力长期维持开发软件包管理：使用RPM软件包，Rpmdrake（URPMI的图形前端）的URPMI包管理器可用版本：用于32位（i586）和64位（x86_64）处理器的安装DVD;可安装32位（i586）处理器的live CD Fedora虽然Fedora仅在2004年9月才正式发布，但它的起源可追溯到1995年，当时它是由Bob Young和Marc Ewing以Red Hat Linux的名义发布的。该公司的第一款产品Red Hat Linux 1.0“母亲节”在同一年发布，之后很快又进行了一些错误修复更新。 1997年，红帽公司推出了革命性的RPM软件包管理系统，具有依赖解决方案和其他先进功能，极大地促进了分发的迅速普及并超越Slackware Linux成为世界上使用最广泛的Linux发行版。在以后的几年中，红帽将按照正常的6个月发布时间表进行开发。 在2003年刚发布Red Hat Linux 9之后，该公司对其产品系列进行了一些根本性的改变。它保留了红帽商业产品的商标，特别是红帽企业Linux，并引入了Fedora Core（后来改名为Fedora），这是一个红帽赞助的，但面向社区的发行版，专为“Linux爱好者”设计。从刚开始的批评后，Linux社区接受了“新的”发行版作为Red Hat Linux的核心延续版本。 Fedora重新成为一个高质量的版本，成为市场上最受欢迎的操作系统之一。与此同时，红帽公司迅速成为全球规模最大，盈利能力最强的Linux公司，拥有创新的产品阵容，出色的客户支持以及红帽认证工程师（RHCE）认证计划等其他受欢迎的计划。 尽管Fedora的方向仍然由Red Hat，Inc.主要控制，并且该产品有时被看作是对红帽企业Linux的测试平台(小白鼠)，无论是正确的还是错误的，无可否认，Fedora是最具创新性的分发版之一。它对Linux内核，glibc和GCC的贡献是众所周知的，它最近集成了SELinux功能，虚拟化技术，系统服务管理器，先进的日志文件系统以及其他企业级功能， 。不利的一面是，Fedora仍然缺乏明确的面向桌面的策略，以使产品更容易用于“Linux爱好者”目标以外的用户。 优点：高度创新;突出的安全功能;大量支持的软件包;严格遵守自由软件的理念;具有许多流行桌面环境的Live CD的可用性缺点：Fedora的优先级倾向于倾向于企业功能，而不是桌面可用性;一些出色的边缘功能，比如早期切换到KDE 4和GNOME 3，偶尔会疏远一些桌面用户软件包管理：使用RPM软件包的YUM图形和命令行工具可用的版本：用于32位（i386）和64位（x86_64）处理器的Fedora;还有GNOME，KDE，LXDE，MATE和Xfce桌面的CD版本基于Fedora的替代方案：Korora（GNOME，KDE，LXDE桌面或Xfce桌面的Live DVD）基于红帽的备选方案：CentOS，Scientific Linux Arch LinuxArch Linux的KISS（保持简单愚蠢）哲学是在2002年由加拿大计算机科学专业毕业生Judd Vinet在2002年推出的，几年来，它一直是一个为中级和高级Linux用户设计的边缘项目。但是它“滚动更新”，只需要安装一次，然后保持一直更新，不要从头安装新的系统。这都要感谢其强大的包管理器和一个总是最新的软件库。因此，Arch Linux的“发行版”很少，而且现在只限于一个基本的安装光盘，只有在基本系统发生相当大的变化时，才会发行新的安装介质。 Arch Linux除了拥有备受推崇的“滚动发布”更新机制之外，还以其快速和强大的软件包管理器“Pacman”而闻名，能够从源代码安装软件包，并且由于其AUR基础架构，以及经过充分测试的软件包不断增加的软件库。其高度重视的文档，以及卓越的Arch Linux手册，使得一些高级Linux用户可以自行安装和定制分发。用户可以使用的强大工具意味着发行版可以无限定制到最细微的细节，并且没有两个安装可能是相同的。 不利的一面是，任何滚动更新更新机制都有其危险性：人为错误，库或依赖关系丢失，已存在于存储库中的应用程序的新版本有一个尚未报告的严重错误都可能导致系统的不稳定。在Pacman升级之后，最终导致无法启动的系统是经常遇到的。因此，Arch Linux是一种需要用户警觉并具有足够的知识来解决任何这种可能的问题的发行版。此外，偶尔安装的发行版意味着有时由于重要的系统更改或在较早的Linux内核中缺少硬件支持而无法使用旧版本。 优点：优秀的软件管理基础设施无与伦比的定制和调整选项;一流的在线文档缺点：偶尔会出现不稳定和风险软件包管理：使用TAR.XZ软件包的“Pacman”包管理器可用的版本：64位（x86_64）处理器的最小安装CD和网络安装CD映像基于Arch Linux的发行版：Manjaro Linux（与Cinnamon，Enlightenment，KDE，LXDE，MATE，Openbox，Xfce一起使用），Antergos（与GNOME 3一起使用），ArchBang Linux（使用Openbox的轻量级），Chakra GNU / Linux （使用KDE的Live CD），Bridge Linux（使用GNOME，KDE，LXDE和Xfce），Parabola GNU / Linux（免费软件），KaOS（使用KDE） PCLinuxOSPCLinuxOS于2003年由比尔·雷诺兹（Bill Reynolds）首先宣布，被称为“Texstar”。在创建自己的发行版之前，Texstar已经是Mandrake Linux社区用户的知名开发人员构建的最新的RPM包，并提供免费下载。 2003年，他决定建立一个新的发行版，最初基于Mandrake Linux，但有几个显著的可用性改进。理念是应该对初学者是友好的，具有专有内核模块，浏览器插件和媒体编解码器的开箱即用的支持，并应作为一个简单直观的图形安装程序的Live CD。 几年后的发展，PCLinuxOS正在迅速接近其预期的状态。就可用性而言，该项目为大多数Windows到Linux移民希望从他们的新操作系统中获得的许多技术提供了开箱即用的支持。在软件方面，PCLinuxOS是一个面向KDE的发行版，具有定制且始终最新版本的流行桌面环境。不断增长的软件存储库包含其他桌面，并为许多常见任务提供各种各样的桌面软件包。对于系统配置，PCLinuxOS保留了很多Mandriva优秀的控制中心，但是用APT和Synaptic（一个图形化的包管理前端）取代了它的包管理系统。 不利的一面是，PCLinuxOS缺乏任何形式的路线图或发布目标。尽管越来越多的社区参与这个项目，大多数的发展和决策仍然掌握在Texstar的手中，他们在判断发布的稳定性时倾向于保守的一面。因此，PCLinuxOS的开发过程往往是艰巨的。例如，尽管频繁要求64位版本，但开发者直到最近才开始生产64位版本。此外，该项目不提供任何安全建议，而是依靠用户通过所包括的管理工具保持系统最新的状态。 优点：对图形驱动程序，浏览器插件和媒体编解码器的开箱即用支持;滚动更新机制;最新的软件缺点：对非英语语言没有开箱即用的支持;缺乏发布计划和安全建议软件包管理：使用RPM包的高级包工具（APT）可用的版本：KDE，完整的Monty，KDE Minime，LXDE，LXDE Mini，Openbox，Openbox盆景，用于64位（x86_64）处理器体系结构的KDE GentooGentoo是Linux世界最年轻的发行版本，正因为年轻，所以能吸取在她之前的所有发行版本的优点。Gentoo最初由Daniel Robbins（FreeBSD的开发者之一）创建，首个稳定版本发布于2002年。由于开发者对FreeBSD的熟识，所以Gentoo拥有媲美FreeBSD的广受美誉的ports系统 ——Portage包管理系统。 FreeBSDFreeBSD是AT＆T UNIX通过Berkeley Software Distribution（BSD）的间接后裔，它的历史可以追溯到1993年。与Linux发行版不同，Linux发行版被定义为由Linux内核和数千个软件应用程序组成的集成软件解决方案， 而FreeBSD是一个紧密集成的操作系统，由BSD内核和所谓的“用户空间”构成（因此即使没有额外的应用程序也可以使用）。一旦安装在普通的计算机系统上，这种区别就不明显了 – 就像许多Linux发行版一样，大量易于安装的（大部分）开源应用程序也是可支持FreeBSD核心。 FreeBSD已经发展成为一个快速，高性能和非常稳定的操作系统，尤其适用于Web服务和类似的任务。许多具有关键任务计算基础设施的大型网络搜索引擎和组织已经在他们的计算机系统上部署和使用FreeBSD多年。与Linux相比，FreeBSD是在一个限制少得多的许可证下分发的，它允许为任何目的而实际上不受限制的重用和修改源代码。即使是苹果公司的Mac OS X也是从FreeBSD派生出来的。除了核心操作系统之外，该项目还提供了超过24,000个二进制和源代码形式的软件应用程序，以方便安装在FreeBSD核心上。 虽然FreeBSD当然可以用作桌面操作系统，但是它与这个部门中流行的Linux发行版并没有很好的比较。文本模式系统安装程序在硬件检测或系统配置方面提供的功能很少，在安装后的设置中将大部分配置工作留给了用户。在对现代硬件的支持方面，FreeBSD通常落后于Linux，尤其是在支持诸如无线网卡或数码相机等，高端的台式机和笔记本电脑方面。那些试图在桌面或工作站上开发项目的用户，以充分利用FreeBSD的速度和稳定性，而不是FreeBSD本身。 优点：快速稳定;安装24000多个软件应用程序（或“端口”）的可用性;非常好的文档缺点：在支持新颖和异乎寻常的硬件方面，往往落后于Linux，商业应用程序的可用性有限;缺少图形化配置工具软件包管理：使用二进制包或基于源的“端口”（TBZ）的完整命令行包管理基础架构，可用版本：用于AMD64，ARM / ARMEL，i386，IA64，MIPS / MIPSEL，PC98 PowerPC，SPARC64和Xbox处理器的安装CD基于FreeBSD的替代方案：PC-BSD（桌面），GhostBSD（带有GNOME的live DVD）其他BSD的替代品：OpenBSD，NetBSD，DragonFly BSD 下面给为选择一个Linux发行版本犯愁的朋友一些建议如果你只是需要一个桌面系统，而且既不想使用盗版，又不想花大量的钱购买商业软件，那么你就需要一款适合桌面使用的Linux发行版本了，如果你不想自己定制任何东西，不想在系统上浪费太多时间，那么很简单，你就根据自己的爱好在ubuntu、kubuntu以及xubuntu中选一款吧，三者的区别仅仅是桌面程序的不一样。如果你需要一个桌面系统，而且还想非常灵活的定制自己的Linux系统，想让自己的机器跑得更欢，不介意在Linux系统安装方面浪费一点时间，那么你的唯一选择就是Gentoo，尽情享受Gentoo带来的自由快感吧！如果你需要的是一个服务器系统，而且你已经非常厌烦各种Linux的配置，只是想要一个比较稳定的服务器系统而已，那么你最好的选择就是CentOS了，安装完成后，经过简单的配置就能提供非常稳定的服务了。如果你需要的是一个坚如磐石的非常稳定的服务器系统，那么你的唯一选择就是FreeBSD。如果你需要一个稳定的服务器系统，而且想深入摸索一下Linux的各个方面的知识，想自己定制许多内容，那么我推荐你使用Gentoo。 Linux桌面环境早期的 Linux 系统都是不带界面的，只能通过命令来管理，比如运行程序、编辑文档、删除文件等。所以，要想熟练使用 Linux，就必须记忆很多命令。 后来随着 Windows 的普及，计算机界面变得越来越漂亮，点点鼠标就能完成很多工作，人们已经习惯了图形界面化的操作，很难再忍受一片漆黑的命令行窗口了。这推动了 Linux 社区进行变革，很快推出了 Linux 系统的图形界面环境。 完成工作的方式不止一种，Linux 一直以来都以此而闻名，在图形桌面上更是如此，Linux 有各种各样的图形化桌面可供选择。 Linux 中的桌面环境也是一个程序，它和内核不是绑定的，两者的开发也不是同步的；给不带界面的 Linux 系统安装上一个桌面环境，你就能看到各种漂亮的窗口，并能用鼠标点击它们了。 上面所说各种Linux发行版其实默认附带了某种桌面环境，但如果你喜欢折腾，可以更换其他桌面环境 下面给大家介绍几款比较流行的桌面环境： KDEKDE 是 K Desktop Environment 的缩写，中文译为“K桌面环境”。 KDE 是基于大名鼎鼎的 Qt 的，最初于 1996 年作为开源项目公布，并在 1998 年发布了第一个版本，现在 KDE 几乎是排名第一的桌面环境了。 许多流行的 Linux 发行版都提供了 KDE 桌面环境，比如 Ubuntu、Linux Mint、OpenSUSE、Fedora、Kubuntu、PC Linux OS 等。 KDE 和 Windows 比较类似，各位初学者相信都是 Windows 的用户，所以切换到 KDE 也不会有太大的障碍。 KDE 允许你把应用程序图标和文件图标放置在桌面的特定位置上。单击应用程序图标，Linux 系统就会运行该应用程序。单击文件图标，KDE 桌面就会确定使用哪种应用程序来处理该文件。 KDE 是所有桌面环境中最容易定制的。在其他桌面环境中，你需要几个插件、窗口组件和调整工具才可以定制环境，KDE 将所有工具和窗口组件都塞入到系统设置中。借助先进的设置管理器，可以控制一切，不需要任何第三方工具，就可以根据用户的喜好和要求来美化及调整桌面。 KDE 项目组还还发了大量的可运行在 KDE 环境中的应用程序，包括 Dolphin（文件管理工具）、Konsole（终端）、Kate（文本编辑工具）、Gwenview（图片查看工具）、Okular（文档及PDF查看工具）、Digikam（照片编辑和整理工具）、KMail（电子邮件客户软件）、Quassel（IRC客户软件）、K3b（DVD刻录程序）、Krunner（启动器）等，它们都是默认安装的。 对 KDE 优缺点的总结：优点：KDE 几乎是最先进最强大的桌面环境，它外观优美、高度可定制、兼容比较旧的硬件设备缺点：Kmail 等一些组件的配置对新手来说过于复杂。 GNOMEGNOME 是 the GNU Network Object Model Environment 的缩写，中文译为“GNU网络对象模型环境”。 GNOME 于 1999 年首次发布，现已成为许多Linux发行版默认的桌面环境（不过用得最多的是 Red Hat Linux）。 GNOME 的特点是简洁、运行速度快，但是没有太多的定制选项，用户需要安装第三方工具来实现。 GNOME 甚至不包括一些简单的调整选项，比如更改主题、更改字体等，就这两种基本的调整而言，用户都需要安装第三方工具。所以，GONME 适合那些不需要高度定制界面的用户。 GNOME 被用作 Fedora 中的默认桌面环境，提供在几款流行的 Linux 发行版中，比如 Ubuntu、Debian、OpenSUSE 等。 2011 年，GNOME 3 进行了重大更新，不再采用传统的 Windows 风格的界面，而是进行了全新的设计，惊艳了很多用户。GNOME 3 的这种行为也导致部分用户和开发人员不满，他们又开发了多款其他的桌面环境，比如 MATE 和 Cinnamon。 对 GNOME 优缺点的总结：优点：简单易用，可通过插件来扩展功能。缺点：对插件的管理能力比较差，也缺少其它桌面环境拥有的许多功能。 UnityUnity 是由 Ubuntu 的母公司 Canonical 开发的一款外壳。之所以说它是外壳，是因为 Unity 运行在 GNOME 桌面环境之上，使用了所有 GNOME 的核心应用程序。 2010 年，Unity 第一个版本发布，此后经过数次改进，如今和其它的桌面环境一样，也可以安装到其它的 Linux 发行版上了。 Unity 使用了不同的界面风格，如果你用的是 Ubuntu Linux 发行版，你会注意到 Unity 与 KDE 和 GNOME 桌面环境有些不一样。 Unity 在左边有一个启动器，位于启动器顶部的是搜索图标，又叫“Dash”。在 Dash 上搜索文件时，不仅会给出来自硬盘的搜索结果，还会给出来自在线来源的搜索结果，比如 Google Drive、Facebook、Picasa、Flick 及其他。 Unity 还提供了隐藏启动器、触摸侧边栏就显示的选项，用户还可以调高/调低显示启动器菜单的灵敏度。 Unity 很简单、运行速度快，但 Unity 在系统设置下却没有定制桌面的太多选项，要想安装主题或者定制另外不同的选项，比如系统菜单是否应该总是可见，或者“从启动器图标一次点击最小化”，用户需要安装第三方工具。CCSM 和 Unity Tweak Tool 是面向 Unity 桌面环境的非常流行的定制工具。对 Unity 优缺点的总结：优点：界面简洁直观，可以通过第三方工具来深度定制，而且使用了平视显示器（HUD）等新技术。缺点：默认的定制功能比较差劲，通知机制一般。 MATE上面我们提到，GNOME 3 进行了全新的界面设计，这招致一些用户的不满，他们推出了其它的桌面环境，MATE 就是其中之一。 MATE 是一种从现在无人维护的 GNOME 2 代码库派生出来的桌面环境。 MATE 让人觉得在使用旧的桌面环境，但是结合了历年来界面方面的诸多改进。MATE 还非常适用于低配计算机，所以如果你有一台旧的或速度较慢的计算机，可以使用 MATE。 MATE 还是许多流行的 Linux 发行版随带的，比如 Ubuntu、Linux Mint、Mageia、Debian 及另外更多发行版。Ubuntu MATE 头一回是官方版本。 “欢迎首次发布的 Ubuntu MATE 官方版本。现在，用户将更容易更新软件，因为所有组件现在都在 Ubuntu 软件库中。” MATE 自带的应用程序包括 Caja（文件管理工具）、Pluma（文本编辑工具）、Atril（文档查看工具）、Eye of MATE（图像查看工具）等，如果用户不需要其他功能完备的桌面环境的所有额外功能，那么 MATE 对他们来说是一款简单的轻量级桌面环境。 对 META 优缺点的总结：优点：轻量级的桌面环境，能够兼容教旧的硬件设备。缺点：暂无 Cinnamon与 MATE 类似，Cinnamon 是由 Linux Mint 团队因为不满 Gnome 3 的改进而开发的另一种桌面环境。但 Cinnamon 与 MATE 不同之处在于，Cinnamon 建立在 Gnome 3 的基础上。Cinnamon 是新的，而且在积极开发之中，但这款出色的桌面环境没有因新颖而在功能方面有所减弱。 Cinnamon 拥有 GNOME 和 Unity 等其它桌面环境所没有的种种功能。Cinnamon 是高度可定制的桌面环境，不需要任何外部插件、窗口组件和调整工具来定制桌面。Cinnamon 甚至可以通过设置管理器本身来下载并安装主题，甚至不需要打开互联网浏览器。 由于种种出色的所需功能，Cinnamon 对任何刚接触 Linux 的新用户来说都非常方便。许多用户放弃使用 Linux，是因为他们并不了解 Linux 的工作方式，但是我强烈建议新手应从 Cinnamon 桌面环境开始入手。 许多流行的 Linux 发行版提供了各自版本的 Cinnamon，比如 Ubuntu、Fedora、OpenSUSE、Gentoo、Arch Linux 等。Cinnamon 还是 Linux Mint 的默认桌面环境。 对 Cinnamon 优缺点的总结：优点：成熟完美，高度可性质，适合 Linux 新手。缺点：有时候可能会有软件错误]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows下mysql的卸载和安装使用]]></title>
    <url>%2F2019%2F07%2F30%2FLearn%2Fwindows%E4%B8%8Bmysql%E7%9A%84%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[mysql的卸载停止服务，卸载右键开始菜单，打开powershell(管理员)，cd到windows版mysql的解压路径 1234567891011121314151617181920212223PS C:\Windows\system32&gt; cd D:\ZTGX\github\mysql-8.0.16-winx64PS D:\ZTGX\github\mysql-8.0.16-winx64&gt; ls 目录: D:\ZTGX\github\mysql-8.0.16-winx64Mode LastWriteTime Length Name---- ------------- ------ ----da---- 2019/4/13 22:39 bind----- 2019/7/29 19:42 datada---- 2019/4/13 22:31 docsda---- 2019/4/13 22:31 etcda---- 2019/4/13 22:31 includeda---- 2019/4/13 22:39 libda---- 2019/4/13 22:31 runda---- 2019/4/13 22:31 shareda---- 2019/4/13 22:31 var-a---- 2019/4/13 19:46 335809 LICENSE-a---- 2019/4/13 19:46 101807 LICENSE.router-a---- 2019/7/21 13:14 715 my.ini-a---- 2019/4/13 19:46 687 README-a---- 2019/4/13 19:46 700 README.router 执行服务停止命令 123PS D:\ZTGX\github\mysql-8.0.16-winx64&gt; net stop mysqlMySQL 服务正在停止..MySQL 服务已成功停止。 卸载mysql 12PS D:\ZTGX\github\mysql-8.0.16-winx64&gt; mysqld removeService successfully removed. 打开注册表，删除mysql相关目录win+r打开运行，输入regedit，进入注册表编辑器，删除以下文件 HKEY_LOCAL_MACHINE\SYSTEM\ControlSet001\Services\EventLog\Application\MySQLD Service mysql的安装下载mysql压缩包https://dev.mysql.com/downloads/mysql/ 配置环境变量12345NAME: MYSQL_HOMEVALUE: D:\develop_installation_package\win-mysql-8.0\mysql-8.0.16-winx64NAME: pathVALUE: %MYSQL_HOME%\bin 新建my.ini文件,data文件夹路径：D:\develop_installation_package\win-mysql-8.0\mysql-8.0.16-winx64 my.ini文件： 12345678910111213141516171819202122232425262728[mysqld]# 设置mysql的安装目录basedir=D:\develop_installation_package\win-mysql-8.0\mysql-8.0.16-winx64# 设置mysql数据库的数据的存放目录datadir=D:\develop_installation_package\win-mysql-8.0\mysql-8.0.16-winx64/data# 设置默认使用的端口port=3306# 允许最大连接数max_connections=200# 允许连接失败的次数。这是为了防止有人试图攻击数据库max_connect_errors=10# 服务端使用的字符集character-set-server=utf8mb4# 数据库字符集对应一些排序等规则使用的字符集collation-server=utf8mb4_general_ci# 创建新表时将使用的默认存储引擎default-storage-engine=INNODB# 默认使用“mysql_native_password”插件作为认证加密方式# MySQL8.0默认认证加密方式为caching_sha2_passworddefault_authentication_plugin=mysql_native_password[mysql]# 设置mysql客户端默认字符集default-character-set=utf8mb4[client]default-character-set=utf8mb4port=3306 以管理员身份运行命令提示符12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061PS C:\Windows\system32&gt; cd D:\develop_installation_package\win-mysql-8.0\mysql-8.0.16-winx64#初始化MySQL数据库PS D:\develop_installation_package\win-mysql-8.0\mysql-8.0.16-winx64&gt; mysqld --initialize --console2019-07-30T10:50:30.522984Z 0 [System] [MY-013169] [Server] D:\develop_installation_package\win-mysql-8.0\mysql-8.0.16-winx64\bin\mysqld.exe (mysqld 8.0.16) initializing of server in progress as process 209522019-07-30T10:50:54.333588Z 5 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: u3qnRFr&gt;(M#a2019-07-30T10:51:03.275327Z 0 [System] [MY-013170] [Server] D:\develop_installation_package\win-mysql-8.0\mysql-8.0.16-winx64\bin\mysqld.exe (mysqld 8.0.16) initializing of server has completed#安装MySQL服务PS D:\develop_installation_package\win-mysql-8.0\mysql-8.0.16-winx64&gt; mysqld installService successfully installed.#启动MySQL服务PS D:\develop_installation_package\win-mysql-8.0\mysql-8.0.16-winx64&gt; net start mysqlMySQL 服务正在启动 ..MySQL 服务已经启动成功。#使用临时密码登录【初始化mysql时产生】PS D:\develop_installation_package\win-mysql-8.0\mysql-8.0.16-winx64&gt; mysql -uroot -pEnter password: ************Welcome to the MySQL monitor. Commands end with ; or \g.#修改mysql登录密码mysql&gt; alter user 'root'@'localhost' identified by '123456';Query OK, 0 rows affected (0.52 sec)#设置mysql远程登录mysql&gt; use mysqlDatabase changedmysql&gt; select host, user, authentication_string, plugin from user;+-----------+------------------+------------------------------------------------------------------------+-----------------------+| host | user | authentication_string | plugin |+-----------+------------------+------------------------------------------------------------------------+-----------------------+| localhost | mysql.infoschema | $A$005$THISISACOMBINATIONOFINVALIDSALTANDPASSWORDTHATMUSTNEVERBRBEUSED | caching_sha2_password || localhost | mysql.session | $A$005$THISISACOMBINATIONOFINVALIDSALTANDPASSWORDTHATMUSTNEVERBRBEUSED | caching_sha2_password || localhost | mysql.sys | $A$005$THISISACOMBINATIONOFINVALIDSALTANDPASSWORDTHATMUSTNEVERBRBEUSED | caching_sha2_password || localhost | root | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 | mysql_native_password |+-----------+------------------+------------------------------------------------------------------------+-----------------------+4 rows in set (0.00 sec)mysql&gt; create user 'root'@'%' identified by '123456';Query OK, 0 rows affected (0.62 sec)mysql&gt; select host, user, authentication_string, plugin from user;+-----------+------------------+------------------------------------------------------------------------+-----------------------+| host | user | authentication_string | plugin |+-----------+------------------+------------------------------------------------------------------------+-----------------------+| % | root | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 | mysql_native_password || localhost | mysql.infoschema | $A$005$THISISACOMBINATIONOFINVALIDSALTANDPASSWORDTHATMUSTNEVERBRBEUSED | caching_sha2_password || localhost | mysql.session | $A$005$THISISACOMBINATIONOFINVALIDSALTANDPASSWORDTHATMUSTNEVERBRBEUSED | caching_sha2_password || localhost | mysql.sys | $A$005$THISISACOMBINATIONOFINVALIDSALTANDPASSWORDTHATMUSTNEVERBRBEUSED | caching_sha2_password || localhost | root | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 | mysql_native_password |+-----------+------------------+------------------------------------------------------------------------+-----------------------+5 rows in set (0.00 sec)mysql&gt; GRANT ALL ON *.* TO 'root'@'%';Query OK, 0 rows affected (0.47 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.13 sec) 说明：mysql 8.0之后，创建用户和赋予权限是分开的，所以需要分开操作。不能再像5.7一样，一条命令创建+赋予远程登录权限]]></content>
      <categories>
        <category>Learn</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[solo使用教程]]></title>
    <url>%2F2019%2F07%2F30%2FLearn%2Fsolo%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[注：需要在windows下安装mysql，具体步骤：http://www.zzditto.cn/2019/07/30/windows%E4%B8%8Bmysql%E7%9A%84%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8/#more 登录mysql，创建solo数据库12mysql&gt; create database solo;Query OK, 1 row affected (0.64 sec) 下载solo安装包https://github.com/b3log/solo/releases?utm_source=hacpai.com 下载 solo-v3.6.3.war 解压，进入解压目录执行解压目录下，shift+鼠标右键，选择powershell窗口 1234567PS D:\develop_installation_package\solo\solo-v3.6.3&gt; java -cp "WEB-INF/lib/*;WEB-INF/classes" org.b3log.solo.Starter[INFO ]-[2019-07-30 20:42:23]-[org.b3log.solo.util.Markdowns:135]: [markdown-http] is not available, uses built-in [flexmark] for markdown processing. Please read FAQ section in user guide (https://hacpai.com/article/1492881378588) for more details.[INFO ]-[2019-07-30 20:42:23]-[org.b3log.solo.SoloServletListener:99]: Solo is booting [ver=3.6.3, servletContainer=jetty/9.4.12.v20180830, os=Windows 10, isDocker=false, markdownHttpAvailable=false, pid=7640, runtimeDatabase=MYSQL, runtimeMode=PRODUCTION, jdbc.username=root, jdbc.URL=jdbc:mysql://localhost:3306/solo?useUnicode=yes&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=UTC][INFO ]-[2019-07-30 20:42:23]-[com.zaxxer.hikari.HikariDataSource:110]: HikariPool-1 - Starting...[INFO ]-[2019-07-30 20:42:23]-[com.zaxxer.hikari.HikariDataSource:123]: HikariPool-1 - Start completed.[INFO ]-[2019-07-30 20:42:23]-[org.b3log.solo.service.InitService:191]: It's your first time setup Solo, initialize tables in database [MYSQL][WARN ]-[2019-07-30 20:42:28]-[org.b3log.solo.service.InitService:161]: Solo has not been initialized, please open your browser to init Solo 访问web页面localhost:8080 需要注册github账号，注册地址：https://github.com/join?source=header-home点击：开始使用，输入github账号即可使用]]></content>
      <categories>
        <category>Learn</category>
      </categories>
      <tags>
        <tag>Solo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo使用教程]]></title>
    <url>%2F2019%2F07%2F30%2FLearn%2Fhexo%E7%9A%84%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[windows环境 安装node.jshttps://nodejs.org/en/ 安装githttps://git-scm.com/download/win 使用管理员权限操作命令行开始菜单点击鼠标右键，选择powershell(管理员) 执行命令安装hexo1234567PS C:\Windows\system32&gt; npm install -g hexo-cliC:\Users\LWZ22\AppData\Roaming\npm\hexo -&gt; C:\Users\LWZ22\AppData\Roaming\npm\node_modules\hexo-cli\bin\hexonpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.9 (node_modules\hexo-cli\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.9: wanted &#123;"os":"darwin","arch":"any"&#125; (current: &#123;"os":"win32","arch":"x64"&#125;)+ hexo-cli@2.0.0added 187 packages from 432 contributors in 108.919s 初始化hexo。路径为自定义路径【这里是在D盘下新建blog文件夹】 123456789101112131415161718192021222324PS C:\Windows\system32&gt; hexo init D:\blogINFO Cloning hexo-starter https://github.com/hexojs/hexo-starter.gitCloning into 'D:\blog'...remote: Enumerating objects: 9, done.remote: Counting objects: 100% (9/9), done.remote: Compressing objects: 100% (7/7), done.remote: Total 77 (delta 4), reused 5 (delta 2), pack-reused 68Unpacking objects: 100% (77/77), done.Submodule 'themes/landscape' (https://github.com/hexojs/hexo-theme-landscape.git) registered for path 'themes/landscape'Cloning into 'D:/blog/themes/landscape'...remote: Enumerating objects: 43, done.remote: Counting objects: 100% (43/43), done.remote: Compressing objects: 100% (34/34), done.remote: Total 956 (delta 19), reused 19 (delta 8), pack-reused 913Receiving objects: 100% (956/956), 3.16 MiB | 30.00 KiB/s, done.Resolving deltas: 100% (509/509), done.Submodule path 'themes/landscape': checked out '73a23c51f8487cfcd7c6deec96ccc7543960d350'[32mINFO [39m Install dependencies系统找不到指定的路径。Error: JAVA_HOME is incorrectly set. Please update D:\install\hadoop-2.7.6.tar\hadoop-2.7.6\conf\hadoop-env.cmd'-Dhadoop.log.dir' 不是内部或外部命令，也不是可运行的程序或批处理文件。INFO Start blogging with Hexo! cd到博客目录，执行安装程序 123456789PS C:\Windows\system32&gt; cd D:\blogPS D:\blog&gt; npm installnpm WARN deprecated core-js@1.2.7: core-js@&lt;2.6.8 is no longer maintained. Please, upgrade to core-js@3 or at least to actual version of core-js@2.npm notice created a lockfile as package-lock.json. You should commit this file.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.9 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.9: wanted &#123;"os":"darwin","arch":"any"&#125; (current: &#123;"os":"win32","arch":"x64"&#125;)added 340 packages from 500 contributors and audited 6879 packages in 68.479sfound 0 vulnerabilities 安装完成，打开web页面hexo s 或 hexo server 123PS D:\blog&gt; hexo serverINFO Start processingINFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. 浏览器访问 localhost:4000 将hexo部署到github 点击 git Bash 【需要上传的命令都需要git bash，使用cmd.exe会报错】点击github页面，新建仓库。仓库名为 用户名.github.io。然后复制仓库总网址 在blog目录下安装git插件123456789LWZ@DESKTOP-AB2NRQ5 MINGW64 /d/blog$ npm install --save hexo-deployer-gitnpm WARN babel-eslint@10.0.2 requires a peer of eslint@&gt;= 4.12.1 but none is installed. You must install peer dependencies yourself.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.9 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.9: wanted &#123;"os":"darwin","arch":"any"&#125; (current: &#123;"os":"win32","arch":"x64"&#125;)+ hexo-deployer-git@1.0.0added 24 packages from 10 contributors and audited 9166 packages in 21.011sfound 0 vulnerabilities 编辑blog的_config.yml文件【设置type为git，添加repo和branch】123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: https://github.com/zzditto/zzditto.github.io.git branch: master 在git bash中设置github的邮箱和用户名如果不输入，直接hexo d上传，报错 12345*** Please tell me who you are.Run git config --global user.email "you@example.com" git config --global user.name "Your Name" 在命令行输入： 12345LWZ@DESKTOP-AB2NRQ5 MINGW64 /d/blog$ git config --global user.email "18332779639@163.com"LWZ@DESKTOP-AB2NRQ5 MINGW64 /d/blog$ git config --global user.name "zzditto" 上传命令： hexo d 然后会提示输入github的用户名和密码，登录，稍等片刻，上传成功 使用github访问https://zzditto.github.io/ 创建第一篇博客在blog即博客安装的总目录下，鼠标右键，打开git bash窗口 创建md文件123LWZ@DESKTOP-AB2NRQ5 MINGW64 /d/blog$ hexo new '第一篇博客'INFO Created: D:\blog\source\_posts\第一篇博客.md 在md文件中添加内容打开win下的博客目录，使用makedown编辑器打开此文件，添加内容，ctrl+s保存 清除原来缓存1234LWZ@DESKTOP-AB2NRQ5 MINGW64 /d/blog$ hexo cleanINFO Deleted database.INFO Deleted public folder. 重新生成html页面文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172LWZ@DESKTOP-AB2NRQ5 MINGW64 /d/blog$ hexo gINFO Start processingINFO Files loaded in 659 msINFO Generated: index.htmlINFO Generated: tags/index.htmlINFO Generated: about/index.htmlINFO Generated: schedule/index.htmlINFO Generated: categories/index.htmlINFO Generated: CNAMEINFO Generated: archives/index.htmlINFO Generated: images/cc-by-nc-nd.svgINFO Generated: images/avatar.gifINFO Generated: images/algolia_logo.svgINFO Generated: images/cc-by-nd.svgINFO Generated: images/cc-by-nc-sa.svgINFO Generated: images/cc-by-nc.svgINFO Generated: images/cc-by-sa.svgINFO Generated: images/cc-by.svgINFO Generated: images/apple-touch-icon-next.pngINFO Generated: images/cc-zero.svgINFO Generated: images/logo.svgINFO Generated: images/quote-l.svgINFO Generated: images/favicon-16x16-next.pngINFO Generated: images/quote-r.svgINFO Generated: images/favicon-32x32-next.pngINFO Generated: archives/2019/07/index.htmlINFO Generated: categories/大数据/linux/index.htmlINFO Generated: categories/实战教程/index.htmlINFO Generated: tags/Linux/index.htmlINFO Generated: tags/Ambari/index.htmlINFO Generated: tags/实战教程/index.htmlINFO Generated: tags/HDP/index.htmlINFO Generated: archives/2019/index.htmlINFO Generated: categories/大数据/index.htmlINFO Generated: lib/font-awesome/HELP-US-OUT.txtINFO Generated: js/algolia-search.jsINFO Generated: css/main.cssINFO Generated: js/affix.jsINFO Generated: js/exturl.jsINFO Generated: js/js.cookie.jsINFO Generated: js/post-details.jsINFO Generated: js/next-boot.jsINFO Generated: js/scroll-cookie.jsINFO Generated: js/scrollspy.jsINFO Generated: lib/font-awesome/bower.jsonINFO Generated: js/motion.jsINFO Generated: live2dw/lib/L2Dwidget.min.jsINFO Generated: lib/velocity/velocity.ui.min.jsINFO Generated: lib/font-awesome/fonts/fontawesome-webfont.woffINFO Generated: js/utils.jsINFO Generated: live2dw/lib/L2Dwidget.min.js.mapINFO Generated: lib/font-awesome/fonts/fontawesome-webfont.woff2INFO Generated: lib/velocity/velocity.min.jsINFO Generated: lib/velocity/velocity.ui.jsINFO Generated: 2019/07/30/第一篇博客/index.htmlINFO Generated: 2019/07/23/hello-world/index.htmlINFO Generated: 2019/07/24/test-md/index.htmlINFO Generated: live2dw/lib/L2Dwidget.0.min.jsINFO Generated: js/schemes/pisces.jsINFO Generated: lib/font-awesome/fonts/fontawesome-webfont.eotINFO Generated: 2019/07/30/hexo使用教程/index.htmlINFO Generated: lib/font-awesome/css/font-awesome.css.mapINFO Generated: js/schemes/muse.jsINFO Generated: lib/font-awesome/css/font-awesome.min.cssINFO Generated: lib/font-awesome/css/font-awesome.cssINFO Generated: 2019/07/24/Ambari离线安装/index.htmlINFO Generated: live2dw/lib/L2Dwidget.0.min.js.mapINFO Generated: lib/jquery/index.jsINFO Generated: lib/velocity/velocity.jsINFO 66 files generated in 1.46 sINFO Congratulations! Your are using the latest version of theme NexT. 启动12345LWZ@DESKTOP-AB2NRQ5 MINGW64 /d/blog$ hexo sINFO Start processingINFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop.INFO Congratulations! Your are using the latest version of theme NexT. 同步至github123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108LWZ@DESKTOP-AB2NRQ5 MINGW64 /d/blog$ hexo dINFO Deploying: gitINFO Clearing .deploy_git folder...INFO Copying files from public folder...INFO Copying files from extend dirs...warning: LF will be replaced by CRLF in 2019/07/23/hello-world/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in 2019/07/24/Ambari离线安装/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in 2019/07/24/test-md/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in about/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in archives/2019/07/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in archives/2019/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in archives/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in categories/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in categories/大数据/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in css/main.css.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in js/affix.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in js/algolia-search.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in js/exturl.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in js/js.cookie.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in js/motion.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in js/next-boot.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in js/post-details.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in js/schemes/muse.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in js/schemes/pisces.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in js/scroll-cookie.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in js/scrollspy.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in js/utils.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in lib/font-awesome/css/font-awesome.css.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in lib/font-awesome/css/font-awesome.min.css.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in lib/jquery/index.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in lib/velocity/velocity.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in lib/velocity/velocity.min.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in lib/velocity/velocity.ui.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in lib/velocity/velocity.ui.min.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in live2dw/lib/L2Dwidget.0.min.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in live2dw/lib/L2Dwidget.min.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in schedule/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in tags/Ambari/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in tags/HDP/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in tags/Linux/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in tags/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in 2019/07/30/hexo使用教程/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in 2019/07/30/第一篇博客/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in categories/大数据/linux/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in categories/实战教程/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in tags/实战教程/index.html.The file will have its original line endings in your working directory[master 4c2c499] Site updated: 2019-07-30 13:19:35 21 files changed, 6538 insertions(+), 88 deletions(-) create mode 100644 "2019/07/30/hexo\344\275\277\347\224\250\346\225\231\347\250\213/index.html" create mode 100644 "2019/07/30/\347\254\254\344\270\200\347\257\207\345\215\232\345\256\242/index.html" create mode 100644 "categories/\345\244\247\346\225\260\346\215\256/linux/index.html" create mode 100644 "categories/\345\256\236\346\210\230\346\225\231\347\250\213/index.html" create mode 100644 "tags/\345\256\236\346\210\230\346\225\231\347\250\213/index.html"Enumerating objects: 84, done.Counting objects: 100% (84/84), done.Delta compression using up to 12 threadsCompressing objects: 100% (31/31), done.Writing objects: 100% (48/48), 10.89 KiB | 857.00 KiB/s, done.Total 48 (delta 21), reused 0 (delta 0)remote: Resolving deltas: 100% (21/21), completed with 15 local objects.To https://github.com/zzditto/zzditto.github.io.git d462eae..4c2c499 HEAD -&gt; masterBranch 'master' set up to track remote branch 'master' from 'https://github.com/zzditto/zzditto.github.io.git'.INFO Deploy done: git 访问本机：localhost:4000 网络:http://zzditto.github.io 绑定域名后:http://www.zzditto.cn/]]></content>
      <categories>
        <category>Learn</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDP离线安装]]></title>
    <url>%2F2019%2F07%2F24%2FBigData%2FHDP%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[注意在实际生产环境下，需要离线安装，但是一般都是自定义yum源，将所需的rpm软件包及其依赖提前准备好并制作一个yum源服务器，供集群的各节点使用。本文采用的方法很笨，具体yum源的制作请看https://zzditto.cn/2019/08/19/BigData/HDP%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%88%B6%E4%BD%9C%E6%9C%AC%E5%9C%B0yum%E6%BA%90/;https://zzditto.cn/2019/08/19/BigData/yum%E4%B8%8B%E8%BD%BDrpm%E4%BB%A5%E5%8F%8A%E4%BE%9D%E8%B5%96%E5%8C%85/,下文对软件的安装在yum源制作完成之后，都可使用yum -y install packagename 安装。 1.检查系统版本HDP2.6需要centos7版本注：系统需要linux的虚拟化主机版或者桌面版。最小化安装缺乏mysql安装的部分依赖包 cat /etc/redhat-release 适用于Redhat系linux12[root@localhost ~]# cat /etc/redhat-releaseCentOS Linux release 7.6.1810 (Core) 2.检查分区硬盘大小123456789[root@localhost ~]# df -THFilesystem Type Size Used Avail Use% Mounted on/dev/mapper/centos-root xfs 19G 11G 7.8G 58% /devtmpfs devtmpfs 3.1G 0 3.1G 0% /devtmpfs tmpfs 3.1G 0 3.1G 0% /dev/shmtmpfs tmpfs 3.1G 13M 3.1G 1% /runtmpfs tmpfs 3.1G 0 3.1G 0% /sys/fs/cgroup/dev/sda1 xfs 1.1G 153M 911M 15% /boottmpfs tmpfs 607M 0 607M 0% /run/user/0 系统要求：【需在装载系统时分好】root：100Gswap：同内存大小home：40G数据盘：剩余容量 3.准备离线资源包JDKhttps://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html ntphttpdyummysqlmysql-connectmariadbpostgresqlAmbarihttps://docs.hortonworks.com/HDPDocuments/Ambari-2.6.2.2/bk_ambari-installation/content/ambari_repositories.html HDPhttps://docs.hortonworks.com/HDPDocuments/Ambari-2.6.2.2/bk_ambari-installation/content/hdp_26_repositories.html 1234567891011[root@vm01 apps]# lltotal 0drwxr-xr-x. 2 root root 100 Aug 15 10:08 httpddrwxr-xr-x. 2 root root 40 Aug 15 10:07 jdkdrwxr-xr-x. 2 root root 54 Aug 15 10:08 mysqldrwxr-xr-x. 2 root root 48 Aug 15 10:09 mysql-connectdrwxr-xr-x. 2 root root 143 Aug 15 10:09 ntpdrwxr-xr-x. 2 root root 150 Aug 15 10:12 postgresqldrwxr-xr-x. 2 root root 183 Aug 15 10:13 yum[root@vm01 apps]# pwd/opt/apps HDP和Ambari体积较大，可提前上传 4.新机器要干的事情设置ip【all】12345678910111213141516171819[root@localhost ~]# cd /etc/sysconfig/network-scripts/[root@localhost network-scripts]# vi ifcfg-ens33TYPE=EthernetBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=ens33DEVICE=ens33ONBOOT=yesIPADDR=192.168.133.3NETMASK=255.255.255.0GATEWAY=192.168.133.2DNS1=8.8.8.8DNS2=114.114.114.114 集群中每个节点需不同设置，不同之处在于IPADDR。设置完之后重启网络服务 1[root@localhost network-scripts]# systemctl restart network 查看本机ip是否更改成功，并尝试ping其他节点和外网123456789101112131415161718192021222324[root@localhost network-scripts]# ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:6f:d4:70 brd ff:ff:ff:ff:ff:ff inet 192.168.133.3/24 brd 192.168.133.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::1a15:8411:8d59:624e/64 scope link noprefixroute valid_lft forever preferred_lft forever[root@localhost network-scripts]# ping 192.168.133.4PING 192.168.133.4 (192.168.133.4) 56(84) bytes of data.64 bytes from 192.168.133.4: icmp_seq=1 ttl=64 time=0.349 ms64 bytes from 192.168.133.4: icmp_seq=2 ttl=64 time=0.232 ms^C--- 192.168.133.4 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1000msrtt min/avg/max/mdev = 0.232/0.290/0.349/0.060 ms[root@localhost network-scripts]# ping www.baidu.comping: www.baidu.com: Name or service not known离线安装，无法ping通外网 修改主机名【all】12[root@localhost network-scripts]# vi /etc/hostnamevm01 每个节点分别修改，修改需要重启生效！！！ 修改/etc/sysconfig/network【all】123[root@vm01 ~]# vi /etc/sysconfig/network# Created by anacondaNETWORKING=yes 每个节点分别修改 关闭防火墙【all】关闭防火墙，设置开机不自启。每个节点分别设置 1234567[root@vm01 jdk]# systemctl stop firewalld[root@vm01 jdk]# systemctl disable firewalld[root@vm01 jdk]# systemctl status firewalld● firewalld.service - firewalld - dynamic firewall daemon Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled) Active: inactive (dead) Docs: man:firewalld(1) 修改主机映射【all】123456[root@vm01 ~]# vi /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.133.3 vm01192.168.133.4 vm02192.168.133.5 vm03 发送到每个节点 123456[root@vm01 apps]# for i in &#123;2,3&#125;;&gt; do&gt; scp /etc/hosts vm0$i:/etc/&gt; done;hosts 100% 221 183.4KB/s 00:00hosts 100% 221 122.0KB/s 00:00 ssh免密登录【master】注意：发送公匙时，主节点也需要发送 123456789101112131415161718192021[root@vm01 ~]# ssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa):Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:N/7YjYkiwm7xGSUtkb/qRagW1X7GWoc8O7STCBz3eKU root@vm01The key's randomart image is:+---[RSA 2048]----+| . || o. || o+o . || oo=+= + || . ++S.E . || .o.o.@ B || .oo ++ B || .+ =.. B + || o.o.. .o = . |+----[SHA256]-----+ 12345678910[root@vm01 ~]# ssh-copy-id vm01/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/root/.ssh/id_rsa.pub"/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@vm01's password:Number of key(s) added: 1Now try logging into the machine, with: "ssh 'vm01'"and check to make sure that only the key(s) you wanted were added. 12345678910[root@vm01 ~]# ssh-copy-id vm02/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/root/.ssh/id_rsa.pub"/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@vm02's password:Number of key(s) added: 1Now try logging into the machine, with: "ssh 'vm02'"and check to make sure that only the key(s) you wanted were added. 12345678910[root@vm01 ~]# ssh-copy-id vm03/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/root/.ssh/id_rsa.pub"/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@vm03's password:Number of key(s) added: 1Now try logging into the machine, with: "ssh 'vm03'"and check to make sure that only the key(s) you wanted were added. 测试免密登录是否成功： 1234[root@vm01 ~]# ssh vm01 date;ssh vm02 date;ssh vm03 date;Wed Jul 24 14:07:28 CST 2019Wed Jul 24 14:07:28 CST 2019Wed Jul 24 13:26:38 CST 2019 如果发现ssh连接其他机器较慢解决： 123456789101112131415[root@vm01 apps]# vi /etc/ssh/sshd_config#PermitUserEnvironment no#Compression delayed#ClientAliveInterval 0#ClientAliveCountMax 3#ShowPatchLevel noUseDNS no#PidFile /var/run/sshd.pid#MaxStartups 10:30:100#PermitTunnel no#ChrootDirectory none[root@vm01 apps]# systemctl restart sshd 修改ssh设置里的usedns为no，此值被注释，默认是yes参考：https://blog.csdn.net/kanghaha/article/details/51986771 重启网络服务【all】 1[root@localhost network-scripts]# systemctl restart network 安装jdk【all】上传，解压 123456[root@vm01 jdk]# pwd/opt/apps/jdk[root@vm01 jdk]# lltotal 181168drwxr-xr-x. 8 10 143 255 Jul 22 2017 jdk1.8.0_144-rw-r--r--. 1 root root 185515842 Jul 24 11:33 jdk-8u144-linux-x64.tar.gz 分发jdk包 12345# &#123;2,5&#125;代表循环到2和5，不是2~5[root@vm01 apps]# for i in &#123;2,3&#125;&gt; do&gt; scp -r ./jdk/jdk1.8.0_144/ vm0$i:/opt/apps/jdk/&gt; done; 配置环境变量 123[root@vm01 jdk1.8.0_144]# vi /etc/profileexport JAVA_HOME=/opt/apps/jdk/jdk1.8.0_144export PATH=$PATH:$JAVA_HOME/bin 更新，并分发 123456789[root@vm01 apps]# source /etc/profile[root@vm01 apps]# jps3543 Jps[root@vm01 apps]# for i in &#123;2,3&#125;&gt; do&gt; scp /etc/profile vm0$i:/etc/&gt; done;profile 100% 1898 1.5MB/s 00:00profile 100% 1898 1.0MB/s 00:00 其余节点，同更新检查是否配置成功 12[root@vm01 jdk1.8.0_144]# jps20110 Jps 修改文件限制【all】1234567891011121314[root@vm01 jdk]# vi /etc/security/limits.conf#在conf文件末尾添加#End of file* soft nofile 65536* hard nofile 65536* soft nproc 131072* hard nproc 131072[root@vm01 apps]# for i in &#123;2,3&#125;&gt; do&gt; scp /etc/security/limits.conf vm0$i:/etc/security/&gt; done;limits.conf 100% 2502 2.0MB/s 00:00limits.conf 100% 2502 1.8MB/s 00:00 关闭selinux【all】设置并分发 12345678910111213141516171819[root@vm01 jdk1.8.0_144]# vi /etc/selinux/config# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of three values:# targeted - Targeted processes are protected,# minimum - Modification of targeted policy. Only selected processes are protected.# mls - Multi Level Security protection.SELINUXTYPE=targeted[root@vm01 apps]# for i in &#123;2,3&#125;&gt; do&gt; scp /etc/selinux/config vm0$i:/etc/selinux/&gt; done;config 100% 546 907.1KB/s 00:00config 100% 546 732.2KB/s 00:00 同步时钟【all】上传ntp的rpm包，安装。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[root@vm03 ntp]# lltotal 2428-rw-r--r--. 1 root root 67624 Aug 16 18:15 autogen-libopts-5.18-5.el7.x86_64.rpm-rw-r--r--. 1 root root 562052 Aug 16 18:15 ntp-4.2.6p5-28.el7.centos.x86_64.rpm-rw-r--r--. 1 root root 88004 Aug 16 18:15 ntpdate-4.2.6p5-28.el7.centos.x86_64.rpm-rw-r--r-- 1 root root 504632 Aug 16 18:14 openssl-1.0.2k-16.el7_6.1.x86_64.rpm-rw-r--r-- 1 root root 1251908 Aug 16 18:14 openssl-libs-1.0.2k-16.el7_6.1.x86_64.rpm[root@vm01 apps]# for i in &#123;2,3&#125;&gt; do&gt; scp -r /opt/apps/ntp/ vm0$i:/opt/apps/&gt; done;#报错，这是需要升级。卸载旧的，安装新的即可[root@vm03 ntp]# rpm -ivh openssl-libs-1.0.2k-16.el7_6.1.x86_64.rpmwarning: openssl-libs-1.0.2k-16.el7_6.1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEYPreparing... ################################# [100%] file /usr/lib64/.libcrypto.so.10.hmac from install of openssl-libs-1:1.0.2k-16.el7_6.1.x86_64 conflicts with file from package openssl-libs-1:1.0.1e-60.el7.x86_64[root@vm03 ntp]# rpm -e --nodeps openssl-libs-1:1.0.1e-60.el7.x86_64[root@vm03 ntp]# rpm -ivh openssl-libs-1.0.2k-16.el7_6.1.x86_64.rpmwarning: openssl-libs-1.0.2k-16.el7_6.1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:openssl-libs-1:1.0.2k-16.el7_6.1 ################################# [100%][root@vm03 ntp]# rpm -e --nodeps openssl-1:1.0.1e-60.el7.x86_64[root@vm03 ntp]# rpm -ivh openssl-1.0.2k-16.el7_6.1.x86_64.rpmwarning: openssl-1.0.2k-16.el7_6.1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:openssl-1:1.0.2k-16.el7_6.1 ################################# [100%][root@vm03 ntp]# rpm -e --nodeps ntpdate-4.2.6p5-25.el7.centos.x86_64[root@vm03 ntp]# rpm -ivh ntpdate-4.2.6p5-28.el7.centos.x86_64.rpmwarning: ntpdate-4.2.6p5-28.el7.centos.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:ntpdate-4.2.6p5-28.el7.centos ################################# [100%][root@vm03 ntp]# rpm -ivh autogen-libopts-5.18-5.el7.x86_64.rpmwarning: autogen-libopts-5.18-5.el7.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEYPreparing... ################################# [100%] package autogen-libopts-5.18-5.el7.x86_64 is already installed[root@vm03 ntp]# rpm -ivh ntp-4.2.6p5-28.el7.centos.x86_64.rpmwarning: ntp-4.2.6p5-28.el7.centos.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:ntp-4.2.6p5-28.el7.centos ################################# [100%]成都 安装时，需按照依赖性顺序安装，否则会报错安装好后，启动，设置开机自启。分节点安装设置 123456789101112131415161718192021[root@vm01 ntp]# systemctl start ntpd[root@vm01 ntp]# systemctl enable ntpdCreated symlink from /etc/systemd/system/multi-user.target.wants/ntpd.service to /usr/lib/systemd/system/ntpd.service.[root@vm01 ntp]# systemctl status ntpd● ntpd.service - Network Time Service Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled) Active: active (running) since Wed 2019-07-24 11:58:03 CST; 12s ago Main PID: 20257 (ntpd) CGroup: /system.slice/ntpd.service └─20257 /usr/sbin/ntpd -u ntp:ntp -gJul 24 11:58:03 vm01 ntpd[20257]: proto: precision = 0.028 usecJul 24 11:58:03 vm01 ntpd[20257]: 0.0.0.0 c01d 0d kern kernel time sync enabledJul 24 11:58:03 vm01 ntpd[20257]: ntp_io: estimated max descriptors: 1024, initial socket boundary: 16Jul 24 11:58:03 vm01 ntpd[20257]: Listen and drop on 0 v4wildcard 0.0.0.0 UDP 123Jul 24 11:58:03 vm01 ntpd[20257]: Listen and drop on 1 v6wildcard :: UDP 123Jul 24 11:58:03 vm01 ntpd[20257]: Listen normally on 2 lo 127.0.0.1 UDP 123Jul 24 11:58:03 vm01 ntpd[20257]: Listen normally on 3 ens33 192.168.133.3 UDP 123Jul 24 11:58:03 vm01 ntpd[20257]: Listen normally on 4 lo ::1 UDP 123Jul 24 11:58:03 vm01 ntpd[20257]: Listen normally on 5 ens33 fe80::1a15:8411:8d59:624e UDP 123Jul 24 11:58:03 vm01 ntpd[20257]: Listening on routing socket on fd #22 for interface updates 配置ntp参数 内网环境，无法连通外网。主节点这里使用date -s调整时间【安装系统时设置时间】，ntp设为与本地同步从节点设置与主节点同步目的：保障集群各节点时间相同即可 【如果集群中某台机器可以连接外网，则设置此集群为ntp服务器，其他机器与此机器进行ntp时间同步】 主节点： 12345678910111213141516171819202122232425262728[root@vm01 ~]# vi /etc/ntp.conf# Permit all access over the loopback interface. This could# be tightened as well, but to do so would effect some of# the administrative functions.restrict 127.0.0.1restrict ::1# Hosts on local network are less restricted.#允许内网其他机器同步时间restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst#设置与本地时间同步server 127.127.1.0 preferfudge 127.127.1.0 stratum 10#broadcast 192.168.1.255 autokey # broadcast server#broadcastclient # broadcast client#broadcast 224.0.1.1 autokey # multicast server#multicastclient 224.0.1.1 # multicast client#manycastserver 239.255.254.254 # manycast server#manycastclient 239.255.254.254 autokey # manycast client 查看： 1234[root@vm01 ~]# ntpq -p remote refid st t when poll reach delay offset jitter==============================================================================*LOCAL(0) .LOCL. 5 l 19 64 77 0.000 0.000 0.000 从节点： 123456789101112131415161718192021222324252627[root@vm02 ~]# vi /etc/ntp.confrestrict 127.0.0.1restrict ::1# Hosts on local network are less restricted.#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst#设置与主节点ip同步server 192.168.133.3 prefer#设置副设置与本地同步server 127.127.1.0fudge 127.127.1.0 stratum 10#broadcast 192.168.1.255 autokey # broadcast server#broadcastclient # broadcast client#broadcast 224.0.1.1 autokey # multicast server#multicastclient 224.0.1.1 # multicast client#manycastserver 239.255.254.254 # manycast server#manycastclient 239.255.254.254 autokey # manycast client 查看： 12345[root@vm03 ntp]# ntpq -p remote refid st t when poll reach delay offset jitter============================================================================== vm01 LOCAL(0) 11 u 26 64 1 0.297 9.103 0.000*LOCAL(0) .LOCL. 10 l 25 64 1 0.000 0.000 0.000 5.修改yum源，增加HDP的yum源安装httpd服务器上传，安装，启动，设置开机自启 12345678910111213141516171819202122232425262728293031323334353637383940[root@vm01 httpd]# lltotal 3100-rw-r--r--. 1 root root 105728 Aug 15 11:59 apr-1.4.8-3.el7_4.1.x86_64.rpm-rw-r--r--. 1 root root 94132 Aug 15 11:59 apr-util-1.5.2-6.el7.x86_64.rpm-rw-r--r--. 1 root root 2844220 Aug 15 10:08 httpd-2.4.6-89.el7.centos.x86_64.rpm-rw-r--r--. 1 root root 92616 Aug 15 10:08 httpd-tools-2.4.6-89.el7.centos.x86_64.rpm-rw-r--r--. 1 root root 31264 Aug 15 11:59 mailcap-2.1.41-2.el7.noarch.rpm[root@vm01 httpd]# pwd/opt/apps/httpd[root@vm01 httpd]# rpm -ivh apr-1.4.8-3.el7_4.1.x86_64.rpmwarning: apr-1.4.8-3.el7_4.1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:apr-1.4.8-3.el7_4.1 ################################# [100%][root@vm01 httpd]# rpm -ivh apr-util-1.5.2-6.el7.x86_64.rpmwarning: apr-util-1.5.2-6.el7.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:apr-util-1.5.2-6.el7 ################################# [100%][root@vm01 httpd]# rpm -ivh mailcap-2.1.41-2.el7.noarch.rpmwarning: mailcap-2.1.41-2.el7.noarch.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:mailcap-2.1.41-2.el7 ################################# [100%][root@vm01 httpd]# rpm -ivh httpd-tools-2.4.6-89.el7.centos.x86_64.rpmwarning: httpd-tools-2.4.6-89.el7.centos.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:httpd-tools-2.4.6-89.el7.centos ################################# [100%][root@vm01 httpd]# rpm -ivh httpd-2.4.6-89.el7.centos.x86_64.rpmwarning: httpd-2.4.6-89.el7.centos.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:httpd-2.4.6-89.el7.centos ################################# [100%][root@vm01 httpd]# systemctl start httpd[root@vm01 httpd]# systemctl enable httpdCreated symlink from /etc/systemd/system/multi-user.target.wants/httpd.service to /usr/lib/systemd/system/httpd.service. HDP安装包上传至/var/www/html文件夹12345678[root@vm01 ambari]# lltotal 8949860-rw-r--r--. 1 root root 1823779835 Jul 23 18:26 ambari-2.6.2.2-centos7.tar.gz-rw-r--r--. 1 root root 7249933344 Jul 23 18:24 HDP-2.6.5.0-centos7-rpm.tar.gz-rw-r--r--. 1 root root 328623 Jul 23 18:24 HDP-GPL-2.6.5.0-centos7-gpl.tar.gz-rw-r--r--. 1 root root 90606616 Jul 23 18:24 HDP-UTILS-1.1.0.22-centos7.tar.gz[root@vm01 ambari]# pwd/var/www/html/ambari 解压： 1234[root@vm01 ambari]# tar -zxvf ambari-2.6.2.2-centos7.tar.gz[root@vm01 ambari]# tar -zxvf HDP-2.6.5.0-centos7-rpm.tar.gz[root@vm01 ambari]# tar -zxvf HDP-GPL-2.6.5.0-centos7-gpl.tar.gz[root@vm01 ambari]# tar -zxvf HDP-UTILS-1.1.0.22-centos7.tar.gz 删除： 1234[root@vm01 ambari]# rm -rf ambari-2.6.2.2-centos7.tar.gz[root@vm01 ambari]# rm -rf HDP-2.6.5.0-centos7-rpm.tar.gz[root@vm01 ambari]# rm -rf HDP-GPL-2.6.5.0-centos7-gpl.tar.gz[root@vm01 ambari]# rm -rf HDP-UTILS-1.1.0.22-centos7.tar.gz 访问：http://192.168.133.3/ambari/ 查看是否成功 制作hdp本地源上传，安装 12345678910111213141516171819202122232425[root@vm01 yum]# pwd/opt/apps/yum[root@vm01 yum]# lltotal 244-rw-r--r-- 1 root root 95840 Aug 10 2017 createrepo-0.9.9-28.el7.noarch.rpm-rw-r--r-- 1 root root 83984 Jul 4 2014 deltarpm-3.6-3.el7.x86_64.rpm-rw-r--r-- 1 root root 32084 Jul 4 2014 python-deltarpm-3.6-3.el7.x86_64.rpm-rw-r--r-- 1 root root 29388 Nov 12 2018 yum-plugin-priorities-1.1.31-50.el7.noarch.rpm[root@vm01 yum]# rpm -ivh deltarpm-3.6-3.el7.x86_64.rpmPreparing... ################################# [100%]Updating / installing... 1:deltarpm-3.6-3.el7 ################################# [100%][root@vm01 yum]# rpm -ivh python-deltarpm-3.6-3.el7.x86_64.rpmPreparing... ################################# [100%]Updating / installing... 1:python-deltarpm-3.6-3.el7 ################################# [100%][root@vm01 yum]# rpm -ivh createrepo-0.9.9-28.el7.noarch.rpmPreparing... ################################# [100%]Updating / installing... 1:createrepo-0.9.9-28.el7 ################################# [100%][root@vm01 yum]# rpm -ivh yum-plugin-priorities-1.1.31-50.el7.noarch.rpmPreparing... ################################# [100%]Updating / installing... 1:yum-plugin-priorities-1.1.31-50.e################################# [100%] 12345678[root@vm01 yum]# createrepo ./Spawning worker 0 with 4 pkgsWorkers FinishedSaving Primary metadataSaving file lists metadataSaving other metadataGenerating sqlite DBsSqlite DBs complete 修改源的地址123456789101112131415161718192021222324252627282930[root@vm01 yum]# vi /var/www/html/ambari/ambari/centos7/2.6.2.2-1/ambari.repo#VERSION_NUMBER=2.6.2.2-1[ambari-2.6.2.2]name=ambari Version - ambari-2.6.2.2baseurl=http://192.168.133.3/ambari/ambari/centos7/2.6.2.2-1/gpgcheck=1gpgkey=http://192.168.133.3/ambari/ambari/centos7/2.6.2.2-1/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1[root@vm01 yum]# cp /var/www/html/ambari/ambari/centos7/2.6.2.2-1/ambari.repo /etc/yum.repos.d/[root@vm01 yum]# vi /var/www/html/ambari/HDP/centos7/2.6.5.0-292/hdp.repo#VERSION_NUMBER=2.6.5.0-292[HDP-2.6.5.0]name=HDP Version - HDP-2.6.5.0baseurl=http://192.168.133.3/ambari/HDP/centos7/2.6.5.0-292/gpgcheck=1gpgkey=http://192.168.133.3/ambari/HDP/centos7/2.6.5.0-292/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1[HDP-UTILS-1.1.0.22]name=HDP-UTILS Version - HDP-UTILS-1.1.0.22baseurl=http://192.168.133.3/ambari/HDP/centos7/2.6.5.0-292/gpgcheck=1gpgkey=http://192.168.133.3/ambari/HDP/centos7/2.6.5.0-292/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1[root@vm01 yum]# cp /var/www/html/ambari/HDP/centos7/2.6.5.0-292/hdp.repo /etc/yum.repos.d/ 将ambari.repo；hdp.repo源拷贝到其他节点 12345678[root@vm01 yum]# scp /etc/yum.repos.d/ambari.repo vm02:/etc/yum.repos.d/ambari.repo 100% 267 248.2KB/s 00:00[root@vm01 yum]# scp /etc/yum.repos.d/ambari.repo vm03:/etc/yum.repos.d/ambari.repo 100% 267 117.9KB/s 00:00[root@vm01 yum]# scp /etc/yum.repos.d/hdp.repo vm03:/etc/yum.repos.d/hdp.repo 100% 508 422.7KB/s 00:00[root@vm01 yum]# scp /etc/yum.repos.d/hdp.repo vm02:/etc/yum.repos.d/hdp.repo 100% 508 451.1KB/s 00:00 6.安装ambari-sever【以mysql为数据库安装】安装mysql【系统如果为迷你版，则部分组件没有被安装，再次安装时需要联网进行】注意：linux系统为最小化安装，通过下载官网压缩包离线安装时，还需要系统方面的某些小依赖包。导致mysql离线安装失败linux系统为虚拟化主机或者桌面版就可以直接安装 系统为虚拟化主机版本：上传官网的压缩包，解压，按照顺序安装 12345678910111213141516171819202122232425262728293031323334353637383940[root@localhost mysql_5.7.27]# ll总用量 1037224-rw-r--r--. 1 root root 531056640 7月 24 16:09 mysql-5.7.27-1.el7.x86_64.rpm-bundle.tar-rw-r--r--. 1 7155 31415 25365436 6月 12 14:42 mysql-community-client-5.7.27-1.el7.x86_64.rpm-rw-r--r--. 1 7155 31415 281248 6月 12 14:42 mysql-community-common-5.7.27-1.el7.x86_64.rpm-rw-r--r--. 1 7155 31415 3833396 6月 12 14:42 mysql-community-devel-5.7.27-1.el7.x86_64.rpm-rw-r--r--. 1 7155 31415 47074656 6月 12 14:42 mysql-community-embedded-5.7.27-1.el7.x86_64.rpm-rw-r--r--. 1 7155 31415 24079736 6月 12 14:42 mysql-community-embedded-compat-5.7.27-1.el7.x86_64.rpm-rw-r--r--. 1 7155 31415 129991352 6月 12 14:42 mysql-community-embedded-devel-5.7.27-1.el7.x86_64.rpm-rw-r--r--. 1 7155 31415 2272032 6月 12 14:42 mysql-community-libs-5.7.27-1.el7.x86_64.rpm-rw-r--r--. 1 7155 31415 2116432 6月 12 14:42 mysql-community-libs-compat-5.7.27-1.el7.x86_64.rpm-rw-r--r--. 1 7155 31415 173500088 6月 12 14:43 mysql-community-server-5.7.27-1.el7.x86_64.rpm-rw-r--r--. 1 7155 31415 122530756 6月 12 14:43 mysql-community-test-5.7.27-1.el7.x86_64.rpm#删除mariadb，否则会有冲突[root@vm01 mysql]# rpm -qa | grep mariadbmariadb-libs-5.5.56-2.el7.x86_64[root@vm01 mysql]# rpm -e --nodeps mariadb-libs-5.5.56-2.el7.x86_64#安装mysql，按照顺序[root@localhost mysql_5.7.27]# rpm -ivh mysql-community-common-5.7.27-1.el7.x86_64.rpm警告：mysql-community-common-5.7.27-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY准备中... ################################# [100%]正在升级/安装... 1:mysql-community-common-5.7.27-1.e################################# [100%][root@localhost mysql_5.7.27]# rpm -ivh mysql-community-libs-5.7.27-1.el7.x86_64.rpm警告：mysql-community-libs-5.7.27-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY准备中... ################################# [100%]正在升级/安装... 1:mysql-community-libs-5.7.27-1.el7################################# [100%][root@localhost mysql_5.7.27]# rpm -ivh mysql-community-client-5.7.27-1.el7.x86_64.rpm警告：mysql-community-client-5.7.27-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY准备中... ################################# [100%]正在升级/安装... 1:mysql-community-client-5.7.27-1.e################################# [100%][root@localhost mysql_5.7.27]# rpm -ivh mysql-community-server-5.7.27-1.el7.x86_64.rpm警告：mysql-community-server-5.7.27-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY准备中... ################################# [100%]正在升级/安装... 1:mysql-community-server-5.7.27-1.e################################# [100%] 启动，设置开机自启123456789101112131415161718[root@vm01 mysql_5.7.27]# systemctl start mysqld[root@vm01 mysql_5.7.27]# systemctl enable mysqld[root@vm01 mysql_5.7.27]# systemctl status mysqld● mysqld.service - MySQL Server Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled) Active: active (running) since Wed 2019-07-24 16:22:28 CST; 15s ago Docs: man:mysqld(8) http://dev.mysql.com/doc/refman/en/using-systemd.html Main PID: 11260 (mysqld) CGroup: /system.slice/mysqld.service └─11260 /usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pidJul 24 16:22:25 vm01 systemd[1]: Starting MySQL Server...Jul 24 16:22:28 vm01 systemd[1]: Started MySQL Server.[root@vm01 mysql_5.7.27]# vi /etc/rc.local#增加systemctl start mysqld 获取临时密码登录mysql，设置密码，设置远程登录12345[root@vm01 mysql_5.7.27]# grep 'temporary password' /var/log/mysqld.log2019-07-24T08:22:25.615573Z 1 [Note] A temporary password is generated for root@localhost: 8BdMN2JWj*q&gt;[root@vm01 mysql_5.7.27]# mysql -uroot -pEnter password:Welcome to the MySQL monitor. Commands end with ; or \g. 123456789101112131415#设置mysql登录密码为123456mysql&gt; alter user 'root'@'localhost' identified by '123456';ERROR 1819 (HY000): Your password does not satisfy the current policy requirements#出错，密码不符合mysql的安全策略，降低mysql的全局密码验证策略mysql&gt; set global validate_password_policy=0;Query OK, 0 rows affected (0.00 sec)#设置密码长度mysql&gt; set global validate_password_length=6;Query OK, 0 rows affected (0.00 sec)#设置密码为123456mysql&gt; alter user 'root'@'localhost' identified by '123456';Query OK, 0 rows affected (0.00 sec) 设置远程登录 12345mysql&gt; grant all privileges on *.* to 'root'@'%' identified by '123456';Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.00 sec) 安装postgresql123456789101112131415161718[root@vm01 postgresql]# lltotal 7256-rw-r--r-- 1 root root 3175716 Aug 24 2018 postgresql-9.2.24-1.el7_5.x86_64.rpm-rw-r--r-- 1 root root 239640 Aug 24 2018 postgresql-libs-9.2.24-1.el7_5.x86_64.rpm-rw-r--r-- 1 root root 4006220 Aug 24 2018 postgresql-server-9.2.24-1.el7_5.x86_64.rpm[root@vm01 postgresql]# rpm -ivh postgresql-libs-9.2.24-1.el7_5.x86_64.rpmPreparing... ################################# [100%]Updating / installing... 1:postgresql-libs-9.2.24-1.el7_5 ################################# [100%][root@vm01 postgresql]# rpm -ivh postgresql-9.2.24-1.el7_5.x86_64.rpmPreparing... ################################# [100%]Updating / installing... 1:postgresql-9.2.24-1.el7_5 ################################# [100%][root@vm01 postgresql]# rpm -ivh postgresql-server-9.2.24-1.el7_5.x86_64.rpmPreparing... ################################# [100%]Updating / installing... 1:postgresql-server-9.2.24-1.el7_5 ################################# [100%] 安装ambari-server123456789101112131415[root@vm01 yum.repos.d]# lltotal 40-rw-r--r--. 1 root root 267 Aug 15 13:40 ambari.repo-rw-r--r--. 1 root root 1664 Apr 29 2018 CentOS-Base.repo-rw-r--r--. 1 root root 1309 Apr 29 2018 CentOS-CR.repo-rw-r--r--. 1 root root 649 Apr 29 2018 CentOS-Debuginfo.repo-rw-r--r--. 1 root root 314 Apr 29 2018 CentOS-fasttrack.repo-rw-r--r--. 1 root root 630 Apr 29 2018 CentOS-Media.repo-rw-r--r--. 1 root root 1331 Apr 29 2018 CentOS-Sources.repo-rw-r--r--. 1 root root 4768 Apr 29 2018 CentOS-Vault.repo-rw-r--r--. 1 root root 514 Aug 15 13:42 hdp.repo#yum默认使用的是base网络源，将其改名。离线状态下将其mv掉，使用我们自己的yum源[root@vm01 yum.repos.d]# mv CentOS-Base.repo CentOS-Base.repo_bak[root@vm01 yum.repos.d]# yum -y install ambari-server 登录mysql，执行以下语句1234567891011121314151617181920212223242526272829303132333435CREATE DATABASE ambari; use ambari; CREATE USER 'ambari'@'%' IDENTIFIED BY '123456'; GRANT ALL PRIVILEGES ON *.* TO 'ambari'@'%'; CREATE USER 'ambari'@'localhost' IDENTIFIED BY '123456';GRANT ALL PRIVILEGES ON *.* TO 'ambari'@'localhost'; CREATE USER 'ambari'@'master' IDENTIFIED BY '123456'; GRANT ALL PRIVILEGES ON *.* TO 'ambari'@'master'; FLUSH PRIVILEGES; source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql show tables; use mysql; select Host,User Password from user where user='ambari'; CREATE DATABASE hive; use hive; CREATE USER 'hive'@'%' IDENTIFIED BY '123456';GRANT ALL PRIVILEGES ON *.* TO 'hive'@'%'; CREATE USER 'hive'@'localhost' IDENTIFIED BY '123456'; GRANT ALL PRIVILEGES ON *.* TO 'hive'@'localhost'; CREATE USER 'hive'@'master' IDENTIFIED BY '123456'; GRANT ALL PRIVILEGES ON *.* TO 'hive'@'master'; FLUSH PRIVILEGES; CREATE DATABASE oozie; use oozie; CREATE USER 'oozie'@'%' IDENTIFIED BY '123456'; GRANT ALL PRIVILEGES ON *.* TO 'oozie'@'%'; CREATE USER 'oozie'@'localhost' IDENTIFIED BY '123456'; GRANT ALL PRIVILEGES ON *.* TO 'oozie'@'localhost';CREATE USER 'oozie'@'master' IDENTIFIED BY '123456'; GRANT ALL PRIVILEGES ON *.* TO 'oozie'@'master';FLUSH PRIVILEGES; 建立mysql和ambari-server的连接上传mysql-connection-java-5.1.40.tar.gz包，解压 123456789101112131415161718192021[root@vm01 mysql_connect]# lltotal 4348drwxr-xr-x 3 root root 178 Aug 7 2018 mysql-connector-java-5.1.47-rw-r--r-- 1 root root 4452049 Jul 24 17:03 mysql-connector-java-5.1.47.tar.gz[root@vm01 mysql_connect]# cd mysql-connector-java-5.1.47[root@vm01 mysql-connector-java-5.1.47]# lltotal 2448-rw-r--r-- 1 root root 91845 Aug 7 2018 build.xml-rw-r--r-- 1 root root 248527 Aug 7 2018 CHANGES-rw-r--r-- 1 root root 18122 Aug 7 2018 COPYING-rw-r--r-- 1 root root 1007505 Aug 7 2018 mysql-connector-java-5.1.47-bin.jar-rw-r--r-- 1 root root 1007502 Aug 7 2018 mysql-connector-java-5.1.47.jar-rw-r--r-- 1 root root 61407 Aug 7 2018 README-rw-r--r-- 1 root root 63658 Aug 7 2018 README.txtdrwxr-xr-x 8 root root 79 Aug 7 2018 src[root@vm01 mysql-connector-java-5.1.47]# mkdir /usr/share/java[root@vm01 mysql-connector-java-5.1.47]# cp mysql-connector-java-5.1.47.jar /usr/share/java/mysql-connector-java.jar[root@vm01 mysql-connector-java-5.1.47]# cp /usr/share/java/mysql-connector-java.jar /var/lib/ambari-server/resources/mysql-jdbc-driver.jar[root@vm01 mysql-connector-java-5.1.47]# vi /etc/ambari-server/conf/ambari.properties#增加server.jdbc.driver.path=/usr/share/java/mysql-connector-java.jar 初始化设置ambari-server12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@vm01 mysql_5.7.27]# ambari-server setupUsing python /usr/bin/pythonSetup ambari-serverChecking SELinux...SELinux status is 'disabled'Customize user account for ambari-server daemon [y/n] (n)? yEnter user account for ambari-server daemon (root):Adjusting ambari-server permissions and ownership...Checking firewall status...Checking JDK...[1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8[2] Oracle JDK 1.7 + Java Cryptography Extension (JCE) Policy Files 7[3] Custom JDK==============================================================================#自定义jdk位置Enter choice (1): 3WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts.WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts.Path to JAVA_HOME: /opt/apps/jdk/jdk1.8.0_144Validating JDK on Ambari Server...done.Checking GPL software agreement...GPL License for LZO: https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.htmlEnable Ambari Server to download and install GPL Licensed LZO packages [y/n] (n)? yCompleting setup...Configuring database...Enter advanced database configuration [y/n] (n)? yConfiguring database...==============================================================================Choose one of the following options:[1] - PostgreSQL (Embedded)[2] - Oracle[3] - MySQL / MariaDB[4] - PostgreSQL[5] - Microsoft SQL Server (Tech Preview)[6] - SQL Anywhere[7] - BDB==============================================================================#选择ambari储存的数据库为mysqlEnter choice (1): 3#设置数据库的配置信息，括号内为默认，可以更改Hostname (localhost): vm01Port (3306):Database name (ambari):Username (ambari):Enter Database Password (bigdata):Re-enter password:Configuring ambari database...Configuring remote database connection properties...WARNING: Before starting Ambari Server, you must run the following DDL against the database to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sqlProceed with configuring remote database connection properties [y/n] (y)? yExtracting system views...ambari-admin-2.6.2.2.1.jar...........Adjusting ambari-server permissions and ownership...Ambari Server 'setup' completed successfully. 启动ambari1234567891011[root@vm01 mysql_5.7.27]# ambari-server startUsing python /usr/bin/pythonStarting ambari-serverAmbari Server running with administrator privileges.Organizing resource files at /var/lib/ambari-server/resources...Ambari database consistency check started...Server PID at: /var/run/ambari-server/ambari-server.pidServer out at: /var/log/ambari-server/ambari-server.outServer log at: /var/log/ambari-server/ambari-server.logWaiting for server start............................................................ERROR: Exiting with exit code 1.REASON: Server not yet listening on http port 8080 after 50 seconds. Exiting. 如果没有启动成功，修改等待时间： 1234567891011121314151617181920[root@vm01 mysql_5.7.27]# vi /etc/ambari-server/conf/ambari.properties#增加server.startup.web.timeout=120[root@vm01 mysql_5.7.27]# ambari-server restartUsing python /usr/bin/pythonRestarting ambari-serverWaiting for server stop...Ambari Server stoppedAmbari Server running with administrator privileges.Organizing resource files at /var/lib/ambari-server/resources...Ambari database consistency check started...Server PID at: /var/run/ambari-server/ambari-server.pidServer out at: /var/log/ambari-server/ambari-server.outServer log at: /var/log/ambari-server/ambari-server.logWaiting for server start.......................Server started listening on 8080DB configs consistency check: no errors and warnings were found. 7.错误处理如果在安装过程中出现一些问题，则将一切都回滚清除，重新设置注意：最好删除mysql中创建的数据库，然后mysql中user表创建的用户【重新设置时需要从上面的步骤重新导入】 123[root@vm01 mysql_5.7.27]# ambari-server stop[root@vm01 mysql_5.7.27]# ambari-server reset[root@vm01 mysql_5.7.27]# ambari-server setup 8.安装HDP集群组件登录：http://192.168.133.3:8080/#/login用户名，密码：admin依照安装向导进行设置 安装向导：点击launch install wizard 输入集群名称 设置版本，设置本地仓库地址 设置主机名，配置id_rsa 完成免密登录配置后，将id_rsa文件下载到本地 1234567[root@vm01 ssh]# cd ~/.ssh/[root@vm01 .ssh]# lltotal 16-rw-------. 1 root root 391 Aug 15 10:20 authorized_keys-rw-------. 1 root root 1679 Aug 15 10:19 id_rsa-rw-r--r--. 1 root root 391 Aug 15 10:19 id_rsa.pub-rw-r--r--. 1 root root 540 Aug 15 10:21 known_hosts 完成配置 选择组件 选择组件安装位置 选择slaves和clients 配置参数 配置hive时注意： 1234567[root@vm01 .ssh]# ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jarUsing python /usr/bin/pythonSetup ambari-serverCopying /usr/share/java/mysql-connector-java.jar to /var/lib/ambari-server/resourcesIf you are updating existing jdbc driver jar for mysql with mysql-connector-java.jar. Please remove the old driver jar, from all hosts. Restarting services that need the driver, will automatically copy the new jar to the hosts.JDBC driver was successfully initialized.Ambari Server 'setup' completed successfully. 一路点击完成下一步即可]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>HDP</tag>
      </tags>
  </entry>
</search>
